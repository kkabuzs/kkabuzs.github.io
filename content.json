{"meta":{"title":"kkabuzs博客屋","subtitle":"","description":"学习记录和整理","author":"Kkabuzs","url":"https://kkabuzs.github.io","root":"/"},"pages":[{"title":"分类","date":"2025-04-04T12:00:53.000Z","updated":"2025-04-04T12:00:53.000Z","comments":true,"path":"categories/index.html","permalink":"https://kkabuzs.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2025-04-12T07:49:01.000Z","updated":"2025-04-12T07:49:01.000Z","comments":true,"path":"tags/index.html","permalink":"https://kkabuzs.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"MongoDB-6.0.21离线分片集群部署","slug":"mongodb-6-0-21-lixian-fenpianjiqun","date":"2025-04-21T17:53:44.000Z","updated":"2025-04-21T17:53:44.000Z","comments":true,"path":"articles/2025/04/22/mongodb-6-0-21-lixian-fenpianjiqun/","permalink":"https://kkabuzs.github.io/articles/2025/04/22/mongodb-6-0-21-lixian-fenpianjiqun/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 MongoDB-6.0.21离线分片集群部署一 介绍 MongoDB 分片集群通过将数据分散存储在多个节点（分片）上，以支持处理大规模数据和高并发读写操作。它由多个组件组成，包括分片（Shard）、配置服务器（Config Server）和查询路由器（Query Router，也称为 mongos）。分片负责存储实际的数据，配置服务器存储集群的元数据，查询路由器则负责接收客户端的请求，并将其路由到正确的分片上进行处理。 1.1 核心组件分片（Shard） 功能： 分片是存储实际数据的服务器，一个分片集群可包含多个分片。每个分片能够是单个服务器，也可以是一个复制集，复制集的运用提升了数据的可用性与持久性。数据依据分片键被划分并存储于不同的分片上。 示例场景： 在一个电商数据库中，可按商品 ID 进行分片，不同范围的商品 ID 数据会存于不同的分片，如此能将数据均匀分散，减轻单个服务器的负载。 配置服务器（Config Server） 功能： 配置服务器存储了集群的元数据，涵盖分片的位置、数据块的分布等信息。查询路由器（mongos）会借助这些元数据来确定请求应被路由到哪个或哪些分片。自 MongoDB 3.4 版本起，建议使用复制集形式的配置服务器，以增强其高可用性。 示例场景： 当客户端发起查询商品的请求时，配置服务器会告知查询路由器该商品的数据存于哪个分片。 查询路由器（Query Router，mongos） 功能： 作为客户端与分片集群间的接口，客户端的所有请求都要先经过 mongos。mongos 会依据配置服务器提供的元数据，把请求路由到合适的分片上进行处理，然后将各分片的处理结果汇总并返回给客户端。 示例场景： 客户端向 mongos 发送一个查询商品列表的请求，mongos 根据商品 ID 的范围确定涉及的分片，向这些分片发送查询请求，最后把各分片返回的结果整合后返回给客户端。 监控与管理工具 功能： 除了上述核心组件，还可借助 MongoDB 自带的监控与管理工具（如 MongoDB Cloud Manager、MongoDB Ops Manager）来监控集群的性能、状态，管理集群的配置与备份等。 示例场景： 通过 MongoDB Ops Manager 可以实时查看每个分片的磁盘使用情况、读写性能等指标，及时发现并处理潜在的问题。 架构示例 12345678910111213141516171819202122232425262728293031323334+-----------------+| 客户端应用 |+-----------------+ | v+-----------------+| mongos |+-----------------+ | | 查询请求 v+-----------------+| 配置服务器集群 |+-----------------+ | | 元数据 v+-----------------+| 分片 1 (复制集) |+-----------------+ | | 数据存储 v+-----------------+| 分片 2 (复制集) |+-----------------+ | | 数据存储 v+-----------------+| 分片 3 (复制集) |+-----------------+# 这个架构图清晰展示了客户端应用通过 mongos 与分片集群交互，mongos 依据配置服务器的元数据将请求路由到相应分片的过程。 1.2 特点 数据自动分布： MongoDB 会根据数据的分片键自动将数据分散到不同的分片上，实现数据的均衡分布。 支持多种分片策略： 包括基于范围、哈希等分片方式，可以根据数据的特点和业务需求选择合适的分片策略，以优化数据分布和查询性能。 动态调整： 可以在集群运行过程中动态地添加或删除分片，以及调整分片的配置，无需停机维护，实现对集群的动态扩展和优化。 与 MongoDB 原生功能集成： 分片集群与 MongoDB 的其他功能（如复制集、索引等）紧密集成，能够充分利用 MongoDB 的各种特性，提供强大的数据库服务。 1.3 优点 可扩展性： 能够水平扩展，通过添加更多的分片节点来处理不断增长的数据量和并发请求，轻松应对大规模数据存储和高流量访问。 高可用性： 数据在多个分片上复制，即使部分节点出现故障，也可以通过其他正常的分片继续提供服务，保证系统的连续性。 负载均衡： 自动将数据和请求均匀分布到各个分片上，避免单个节点负载过高，提高整个集群的性能和资源利用率。 灵活性： 可以根据业务需求灵活地调整集群的规模和架构，例如添加或删除分片、调整数据分布等。 1.4 缺点 复杂性： 部署和管理相对复杂，需要对 MongoDB 的分片机制有深入的了解，同时涉及到多个组件的配置和协调，增加了运维的难度和成本。 数据一致性挑战： 在分布式环境下，保证数据的一致性需要一定的机制和策略，可能会涉及到更多的性能开销和潜在的一致性问题，尤其是在跨分片的事务处理中。 查询性能的不确定性： 虽然分片集群可以提高整体的读写性能，但对于一些复杂的查询，尤其是涉及到多个分片的关联查询，可能会因为数据分布和查询路由的复杂性而导致性能不稳定或下降。 二 准备工作2.1 服务器准备 最小生产配置（6节点）：配置服务器：3节点（cfg1, cfg2, cfg3）分片1：3节点（shard1a, shard1b, shard1c）mongos路由：2节点（mongos1, mongos2） 说明：本例子中采用了在三台服务器装配置服务器，一个分片，每台服务器都装了mongos路由 IP 系统 配置 说明 192.168.79.138 Centos7.6 2C2G 1 192.168.79.139 Centos7.6 2C2G 2 192.168.79.140 Centos7.6 2C2G 3 192.168.79.143 Centos7.6 2C2G 验证添加删除节点 2.2 环境初始化（所有节点）1234567891011121314# 检查系统版本cat /etc/redhat-release# 检查SELinux状态getenforce # 建议设置为Disabled# 检查防火墙systemctl status firewalld # 建议关闭或配置规则# 创建mongodb用户和目录groupadd mongodbuseradd -r -g mongodb -s /bin/false mongodbmkdir -p /data/&#123;configdb,shard,logs&#125;chown -R mongodb:mongodb /data 2.3 系统优化配置（所有节点）123456789101112# 禁用THP（Transparent Huge Pages）echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defrag# 添加到/etc/rc.local保证重启生效echo -e &quot;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled\\necho never &gt; /sys/kernel/mm/transparent_hugepage/defrag&quot; &gt;&gt; /etc/rc.localchmod +x /etc/rc.local# 调整系统限制echo &quot;mongodb soft nofile 64000&quot; &gt;&gt; /etc/security/limits.confecho &quot;mongodb hard nofile 64000&quot; &gt;&gt; /etc/security/limits.confecho &quot;mongodb soft nproc 64000&quot; &gt;&gt; /etc/security/limits.conf 三 分片集群部署3.1 架构规划（最小配置） 配置服务器：3节点（cfg1:27019, cfg2:27019, cfg3:27019） 分片1：3节点副本集（shard1a:27018, shard1b:27018, shard1c:27018） 分片2：3节点副本集（shard2a:27018, shard2b:27018, shard2c:27018）# 本示例只创建一个分片，不同名字可以一直延展 mongos路由：3节点（mongos1:27017, mongos2:27017, mongos3:27017） #路由两节点也行 3.2 分步安装部署3.2.1 安装MongoDB二进制文件（所有节点）12345678910111213# 解压tar xf mongodb-linux-x86_64-rhel70-6.0.21.tgz -C /usr/local/tar xf mongosh-1.10.6-linux-x64.tgz -C /usr/local/# 改名cd /usr/local/mv mongodb-linux-x86_64-rhel70-6.0.21/ mongodbmv mongosh-1.10.6-linux-x64 mongosh# 创建配置文件目录cd mongodb/mkdir confcd conf 3.2.2 部署配置服务器副本集配置文件 (/usr/local/mongodb/conf/mongod-config.conf) 12345678910111213141516171819202122232425262728293031323334# 创建配置文件systemLog: destination: file path: /data/logs/configsvr.log logAppend: true logRotate: reopenstorage: dbPath: /data/configdb journal: enabled: true wiredTiger: engineConfig: cacheSizeGB: 1net: bindIp: 0.0.0.0 port: 27019replication: replSetName: configReplsharding: clusterRole: configsvrprocessManagement: fork: true pidFilePath: /var/run/mongod-config.pidsecurity:# keyFile: /data/mongodb-keyfile authorization: disabled# 说明：初始化之前，不开启认证 初始化命令: 1234567891011121314# 启动所有配置服务器（所有节点）/usr/local/mongodb/bin/mongod -f /usr/local/mongodb/conf/mongod-config.conf# 初始化副本集/usr/local/mongosh/bin/mongosh --host 192.168.79.138 --port 27019&gt; rs.initiate(&#123; _id: &quot;configRepl&quot;, configsvr: true, members: [ &#123; _id: 0, host: &quot;192.168.79.138:27019&quot; &#125;, &#123; _id: 1, host: &quot;192.168.79.139:27019&quot; &#125;, &#123; _id: 2, host: &quot;192.168.79.140:27019&quot; &#125; ] &#125;) 3.2.3 部署分片副本集配置文件 (/usr/local/mongodb/conf/mongod-shard1.conf) 12345678910111213141516171819202122232425262728293031# 创建配置文件systemLog: destination: file path: /data/logs/shard1.log logAppend: truestorage: dbPath: /data/shard journal: enabled: true wiredTiger: engineConfig: cacheSizeGB: 1 # 根据内存调整net: bindIp: 0.0.0.0 port: 27018replication: replSetName: shard1sharding: clusterRole: shardsvrprocessManagement: fork: true pidFilePath: /var/run/mongod-shard1.pidsecurity:# keyFile: /data/mongodb-keyfile authorization: disabled 初始化分片: 123456789101112131415# 启动分片节点（所有节点）/usr/local/mongodb/bin/mongod -f /usr/local/mongodb/conf/mongod-shard1.conf# 初始化副本集/usr/local/mongosh/bin/mongosh --host 192.168.79.138 --port 27018&gt; rs.initiate(&#123; _id: &quot;shard1&quot;, members: [ &#123; _id: 0, host: &quot;192.168.79.138:27018&quot;, priority: 2 &#125;, &#123; _id: 1, host: &quot;192.168.79.139:27018&quot;, priority: 1 &#125;, &#123; _id: 2, host: &quot;192.168.79.140:27018&quot;, arbiterOnly: true &#125; ] &#125;)# 分片集群可以不要仲裁节点，本示例部署了仲裁节点 3.2.4 部署mongos路由配置文件 (/usr/local/mongodb/conf/mongos.conf) 1234567891011121314151617181920# 创建配置文件systemLog: destination: file path: /data/logs/mongos.log logAppend: truenet: bindIp: 0.0.0.0 port: 27017sharding: configDB: configRepl/192.168.79.138:27019,192.168.79.139:27019,192.168.79.140:27019processManagement: fork: true pidFilePath: /var/run/mongos.pid#security:# keyFile: /data/mongodb-keyfile 启动mongos 12# 本示例三台机器都部署了mongos/usr/local/mongodb/bin/mongos -f /usr/local/mongodb/conf/mongos.conf 四 集群初始化4.1 添加分片到集群12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 连接mongos/usr/local/mongosh/bin/mongosh --host 192.168.79.138 --port 27017# 创建分片&gt; sh.addShard(&quot;shard1/192.168.79.138:27018,192.168.79.139:27018&quot;)报错：MongoServerError: Cannot add shard1/192.168.79.138:27018,192.168.79.139:27018 as a shard since the implicit default write concern on this shard is set to &#123;w : 1&#125;, because number of arbiters in the shard&#x27;s configuration caused the number of writable voting members not to be strictly more than the voting majority. Change the shard configuration or set the cluster-wide write concern using the setDefaultRWConcern command and try again.原因分析：在 MongoDB 的副本集中，写关注（write concern）用于指定写操作在被确认之前需要传播到多少个副本集成员。&#123;w : 1&#125; 表示写操作只需要在主节点上成功写入即可被确认。而在将分片添加到集群时，MongoDB 会检查分片的配置，确保其写关注设置符合集群的要求。当副本集中存在仲裁者时，仲裁者不参与数据的写入，只参与选举投票。如果仲裁者的数量导致可写投票成员的数量没有严格超过投票多数，就会出现这个错误。解决办法：# 连接mongos服务器/usr/local/mongosh/bin/mongosh --host 192.168.79.138 --port 27017&gt; db.adminCommand(&#123; setDefaultRWConcern: 1, defaultWriteConcern: &#123; w: &quot;majority&quot; &#125;, writeConcern: &#123; w: &quot;majority&quot; &#125;&#125;)输出：&#123; defaultReadConcern: &#123; level: &#x27;local&#x27; &#125;, defaultWriteConcern: &#123; w: &#x27;majority&#x27;, wtimeout: 0 &#125;, updateOpTime: Timestamp(&#123; t: 1744186820, i: 2 &#125;), updateWallClockTime: ISODate(&quot;2025-04-09T08:20:21.412Z&quot;), defaultWriteConcernSource: &#x27;global&#x27;, defaultReadConcernSource: &#x27;implicit&#x27;, localUpdateWallClockTime: ISODate(&quot;2025-04-09T08:20:21.415Z&quot;), ok: 1, &#x27;$clusterTime&#x27;: &#123; clusterTime: Timestamp(&#123; t: 1744186821, i: 2 &#125;), signature: &#123; hash: Binary(Buffer.from(&quot;0000000000000000000000000000000000000000&quot;, &quot;hex&quot;), 0), keyId: Long(&quot;0&quot;) &#125; &#125;, operationTime: Timestamp(&#123; t: 1744186821, i: 2 &#125;)&#125;# 此刻重新加入分片就不报错了&gt; sh.addShard(&quot;shard1/192.168.79.138:27018,192.168.79.139:27018&quot;)输出：&#123; shardAdded: &#x27;shard1&#x27;, ok: 1, &#x27;$clusterTime&#x27;: &#123; clusterTime: Timestamp(&#123; t: 1744186861, i: 10 &#125;), signature: &#123; hash: Binary(Buffer.from(&quot;0000000000000000000000000000000000000000&quot;, &quot;hex&quot;), 0), keyId: Long(&quot;0&quot;) &#125; &#125;, operationTime: Timestamp(&#123; t: 1744186861, i: 10 &#125;)&#125; 五 添加安全配置5.1 创建keyfile（所有节点）12345678# 在主节点生成keyfileopenssl rand -base64 756 &gt; /data/mongodb-keyfilechmod 400 /data/mongodb-keyfilechown mongodb:mongodb /data/mongodb-keyfile# 分发到其他节点scp /data/mongodb-keyfile root@192.168.79.139:/data/scp /data/mongodb-keyfile root@192.168.79.140:/data/ 5.2 创建管理员用户1234567891011121314151617181920212223242526272829# 连接mongos/usr/local/mongosh/bin/mongosh --host 192.168.79.138 --port 27017use admin # 切换为admin# 创建超级管理员db.createUser(&#123; user: &quot;admin&quot;, # 名字自己起，随意 pwd: &quot;yourStrongPassword&quot;, # 替换为实际密码 # zs@@123 roles: [ &#123; role: &quot;root&quot;, db: &quot;admin&quot; &#125;, &#123; role: &quot;clusterAdmin&quot;, db: &quot;admin&quot; &#125;, &#123; role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; &#125;, &#123; role: &quot;dbAdminAnyDatabase&quot;, db: &quot;admin&quot; &#125;, &#123; role: &quot;readWriteAnyDatabase&quot;, db: &quot;admin&quot; &#125; ]&#125;)# 创建操作集群，数据库的管理员db.createUser(&#123; user: &quot;clusteradmin&quot;, pwd: &quot;yourStrongPassword&quot;, # zs@@123 roles: [ &#123; role: &quot;clusterAdmin&quot;, db: &quot;admin&quot; &#125;, &#123; role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; &#125; ]&#125;)# 查询用户信息命令db.system.users.find(&#123; user: &quot;admin&quot; &#125;).pretty() 1234567891011121314151617181920212223242526272829// 输出：[ &#123; _id: &#x27;admin.admin&#x27;, userId: new UUID(&quot;0bbdb4e6-ff72-4475-aa86-0bb7d23de00c&quot;), user: &#x27;admin&#x27;, db: &#x27;admin&#x27;, credentials: &#123; &#x27;SCRAM-SHA-1&#x27;: &#123; iterationCount: 10000, salt: &#x27;OnIcTUkRinQ3rf+PWQhnmA==&#x27;, storedKey: &#x27;E1NyqSqeeEoxjbIu4lpFWXVZPxY=&#x27;, serverKey: &#x27;jz7e6o2fxMhNqhnp+wpLEbA5Lp0=&#x27; &#125;, &#x27;SCRAM-SHA-256&#x27;: &#123; iterationCount: 15000, salt: &#x27;pObhLJQbLp7ilNRthmxd5hkW/T1KuqxPVW/E+g==&#x27;, storedKey: &#x27;APlFIKQUnGsyrB44h1De7eGkGcGta5PNMJUiit2jdyo=&#x27;, serverKey: &#x27;pYkayEP1ii4WiNWrq3D8y9WmebZ8dhgDQ07E4/rU1zI=&#x27; &#125; &#125;, roles: [ &#123; role: &#x27;root&#x27;, db: &#x27;admin&#x27; &#125;, &#123; role: &#x27;clusterAdmin&#x27;, db: &#x27;admin&#x27; &#125; ] &#125;] 特别注意!!!! 在 MongoDB 分片集群中，通过 mongos 创建的用户默认不会自动同步到各个分片服务器上，这是因为： 用户数据仅存储在配置服务器(config server)的 admin 数据库中 分片服务器(shard)需要独立认证用户凭据 这是 MongoDB 分片集群的设计行为，不是系统缺陷 1234567891011// 若需要操作分片，则需要同步用户信息到分片集群// 在分片集群的主节点创建用户// 可以和配置集群保持一致db.createUser(&#123; user: &quot;admin&quot;, # 名字自己起，随意 pwd: &quot;yourStrongPassword&quot;, # 替换为实际密码 roles: [ &#123; role: &quot;root&quot;, db: &quot;admin&quot; &#125;, &#123; role: &quot;clusterAdmin&quot;, db: &quot;admin&quot; &#125; ]&#125;) 为用户新增权限的正确方法 通过 mongos 为集群用户添加权限（推荐方式） 123456789// 连接到mongosuse admindb.grantRolesToUser( &quot;username&quot;, [ &#123; role: &quot;readWrite&quot;, db: &quot;targetDatabase&quot; &#125;, // 添加库级读写权限 &#123; role: &quot;dbAdmin&quot;, db: &quot;admin&quot; &#125; // 添加管理权限 ]) 验证分片服务器的用户列表use admindb.system.users.find() 12345678910111213141516171819202122232425262728shard1 [direct: primary] admin&gt; db.system.users.find()[ &#123; _id: &#x27;admin.root&#x27;, userId: new UUID(&quot;126d5130-998f-4f82-96a8-dd754246e81f&quot;), user: &#x27;root&#x27;, db: &#x27;admin&#x27;, credentials: &#123; &#x27;SCRAM-SHA-1&#x27;: &#123; iterationCount: 10000, salt: &#x27;QfqvYBECkEnNlz+x3+hEXw==&#x27;, storedKey: &#x27;pymFSYJE+5HanpJ+xDLMlBz38gg=&#x27;, serverKey: &#x27;INjIgudppIPLqIV1JWlNs5MKoDY=&#x27; &#125;, &#x27;SCRAM-SHA-256&#x27;: &#123; iterationCount: 15000, salt: &#x27;RyjTmqfM/gGvHhDOMAtiWhvBC6Eta72bvYqfdg==&#x27;, storedKey: &#x27;JVHPwxNx4roiqc7ZT66cRXWPaoF0ogd+HrbNovWcVtk=&#x27;, serverKey: &#x27;HaHtkotXaXzth5Wpv3bNBiicRy5us4P4NSPTX9DJg0Y=&#x27; &#125; &#125;, roles: [ &#123; role: &#x27;clusterAdmin&#x27;, db: &#x27;admin&#x27; &#125;, &#123; role: &#x27;root&#x27;, db: &#x27;admin&#x27; &#125; ] &#125;] 5.3 开启安全配置# 停止分片服务（所有节点） [root@localhost conf]# ps aux | grep mongod root 11352 2.1 6.0 3165824 113296 ? SLl 15:24 4:24 /usr/local/mongodb/bin/mongod -f /usr/local/mongodb/conf/mongod-config.conf root 11964 1.5 5.4 3204592 100824 ? Sl 15:32 2:56 ./bin/mongod -f conf/mongod-shard1.conf [root@localhost conf]# kill 11964 # 再停止配置服务（所有节点） [root@localhost conf]# kill 11352 # 停止mongos服务（所有节点） [root@localhost conf]# ps aux | grep mongos root 13651 0.5 1.5 2587960 29676 ? Sl 15:58 0:58 ./bin/mongos -f conf/mongos.conf [root@localhost conf]# kill 13651 # 需要把所有节点配置文件的keyfile全开启 security: keyFile: /data/mongodb-keyfile authorization: enabled # 取消注释改为开启（security.authorization只适用mongod） # mongos配置文件 security: keyFile: /data/mongodb-keyfile # 取消注释 5.4 重新启动服务12345678# 启动配置服务（所有节点）/usr/local/mongodb/bin/mongod -f /usr/local/mongodb/conf/mongod-config.conf# 启动分片服务（所有节点）/usr/local/mongodb/bin/mongod -f /usr/local/mongodb/conf/mongod-shard1.conf# 启动mongos路由服务（所有节点）/usr/local/mongodb/bin/mongos -f /usr/local/mongodb/conf/mongos.conf 5.5 验证登录1234567891011121314151617# 先测试超级管理员登录/usr/local/mongosh/bin/mongosh --host 192.168.79.138 --port 27017 -u admin -p zs@@123 --authenticationDatabase admin---Current Mongosh Log ID: 67f72ad9c2dd92b63ae118e2Connecting to: mongodb://&lt;credentials&gt;@192.168.79.138:27017/?directConnection=true&amp;authSource=admin&amp;appName=mongosh+1.10.6Using MongoDB: 6.0.21Using Mongosh: 1.10.6mongosh 2.5.0 is available for download: https://www.mongodb.com/try/download/shellFor mongosh info see: https://docs.mongodb.com/mongodb-shell/------ The server generated these startup warnings when booting 2025-04-09T19:15:57.166-07:00: You are running this process as the root user, which is not recommended------[direct: mongos] test&gt; sh.status() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115# 输出shardingVersion&#123; _id: 1, minCompatibleVersion: 5, currentVersion: 6, clusterId: ObjectId(&quot;67f61e12f30a8acb66fef8cd&quot;)&#125;---shards[ &#123; _id: &#x27;shard1&#x27;, host: &#x27;shard1/192.168.79.138:27018,192.168.79.139:27018&#x27;, state: 1, topologyTime: Timestamp(&#123; t: 1744186861, i: 7 &#125;) &#125;]---active mongoses[ &#123; &#x27;6.0.21&#x27;: 1 &#125; ]---autosplit&#123; &#x27;Currently enabled&#x27;: &#x27;yes&#x27; &#125;---balancer&#123; &#x27;Currently enabled&#x27;: &#x27;yes&#x27;, &#x27;Failed balancer rounds in last 5 attempts&#x27;: 0, &#x27;Currently running&#x27;: &#x27;no&#x27;, &#x27;Migration Results for the last 24 hours&#x27;: &#x27;No recent migrations&#x27;&#125;---databases[ &#123; database: &#123; _id: &#x27;config&#x27;, primary: &#x27;config&#x27;, partitioned: true &#125;, collections: &#123; &#x27;config.system.sessions&#x27;: &#123; shardKey: &#123; _id: 1 &#125;, unique: false, balancing: true, chunkMetadata: [ &#123; shard: &#x27;shard1&#x27;, nChunks: 1024 &#125; ], chunks: [ &#x27;too many chunks to print, use verbose if you want to force print&#x27; ], tags: [] &#125; &#125; &#125;, &#123; database: &#123; _id: &#x27;testdb&#x27;, primary: &#x27;shard1&#x27;, partitioned: false, version: &#123; uuid: new UUID(&quot;68f6362d-a05e-40a8-a20e-63ed47241131&quot;), timestamp: Timestamp(&#123; t: 1744186927, i: 1 &#125;), lastMod: 1 &#125; &#125;, collections: &#123; &#x27;testdb.users&#x27;: &#123; shardKey: &#123; userId: &#x27;hashed&#x27; &#125;, unique: false, balancing: true, chunkMetadata: [ &#123; shard: &#x27;shard1&#x27;, nChunks: 2 &#125; ], chunks: [ &#123; min: &#123; userId: MinKey() &#125;, max: &#123; userId: Long(&quot;0&quot;) &#125;, &#x27;on shard&#x27;: &#x27;shard1&#x27;, &#x27;last modified&#x27;: Timestamp(&#123; t: 1, i: 0 &#125;) &#125;, &#123; min: &#123; userId: Long(&quot;0&quot;) &#125;, max: &#123; userId: MaxKey() &#125;, &#x27;on shard&#x27;: &#x27;shard1&#x27;, &#x27;last modified&#x27;: Timestamp(&#123; t: 1, i: 1 &#125;) &#125; ], tags: [] &#125; &#125; &#125;][direct: mongos] test&gt; db.stats()&#123; raw: &#123; &#x27;shard1/192.168.79.138:27018,192.168.79.139:27018&#x27;: &#123; db: &#x27;test&#x27;, collections: 0, views: 0, objects: 0, avgObjSize: 0, dataSize: 0, storageSize: 0, indexes: 0, indexSize: 0, totalSize: 0, scaleFactor: 1, fsUsedSize: 0, fsTotalSize: 0, ok: 1 &#125; &#125;, objects: 0, avgObjSize: 0, dataSize: 0, storageSize: 0, totalSize: 0, indexes: 0, indexSize: 0, scaleFactor: 1, fileSize: 0, ok: 1, &#x27;$clusterTime&#x27;: &#123; clusterTime: Timestamp(&#123; t: 1744251631, i: 1 &#125;), signature: &#123; hash: Binary(Buffer.from(&quot;e1fffd79d10bfdfa8b9d3323b855d294b67bf8c3&quot;, &quot;hex&quot;), 0), keyId: Long(&quot;7491208092835643417&quot;) &#125; &#125;, operationTime: Timestamp(&#123; t: 1744251631, i: 1 &#125;)&#125; 123456789101112131415161718192021222324[direct: mongos] test&gt; use testdbswitched to db testdb[direct: mongos] testdb&gt; db.users.getShardDistribution()Shard shard1 at shard1/192.168.79.138:27018,192.168.79.139:27018&#123; data: &#x27;50KiB&#x27;, docs: 1000, chunks: 2, &#x27;estimated data per chunk&#x27;: &#x27;25KiB&#x27;, &#x27;estimated docs per chunk&#x27;: 500&#125;---Totals&#123; data: &#x27;50KiB&#x27;, docs: 1000, chunks: 2, &#x27;Shard shard1&#x27;: [ &#x27;100 % data&#x27;, &#x27;100 % docs in cluster&#x27;, &#x27;51B avg obj size on shard&#x27; ]&#125; sh.status() &#x2F;&#x2F; 查看分片状态db.stats() &#x2F;&#x2F; 查看数据库统计 测试clusteradmin用户登录 123456789101112131415161718192021/usr/local/mongosh/bin/mongosh --host 192.168.79.138 --port 27017 -u clusteradmin -p zs@@123 --authenticationDatabase admin&gt; sh.status() // 查看分片状态# 添加了安全认证之后，重新测试分片功能// 启用数据库分片sh.enableSharding(&quot;testdb1&quot;)// 创建分片集合db.adminCommand(&#123; shardCollection: &quot;testdb1.users&quot;, key: &#123; userId: &quot;hashed&quot; &#125;&#125;)// 插入测试数据use testdb1for (i=1; i&lt;=12341; i++) &#123; db.users.insert(&#123;userId: i, data: &quot;test&quot;+i&#125;)&#125; 1234567891011121314# 出现报错MongoBulkWriteError: not authorized on testdb1 to execute command &#123; insert: &quot;users&quot;, documents: [ &#123; userId: 1, data: &quot;test1&quot;, _id: ObjectId(&#x27;67f72d3583a7485597775119&#x27;) &#125; ], ordered: true, lsid: &#123; id: UUID(&quot;2438810d-f498-4135-86b4-be3e25e7dd88&quot;) &#125;, txnNumber: 1, $clusterTime: &#123; clusterTime: Timestamp(1744252191, 1), signature: &#123; hash: BinData(0, BCE7688B25A7D590830F4270723EC46B6EA80569), keyId: 7491208092835643417 &#125; &#125;, $db: &quot;testdb1&quot; &#125;Result: BulkWriteResult &#123; insertedCount: 0, matchedCount: 0, modifiedCount: 0, deletedCount: 0, upsertedCount: 0, upsertedIds: &#123;&#125;, insertedIds: &#123; &#x27;0&#x27;: ObjectId(&quot;67f72d3583a7485597775119&quot;) &#125;&#125;Write Errors: []# 报错说明没有权限，因为这是个集群用户，只有管理集群权限，没有操作testdb1的权限，此时我们可以赋权，或者创建专门操作testdb1库的用户，这里选择赋权。 处理报错问题 12345678910111213141516171819202122# 退出，选择admin用户登录/usr/local/mongosh/bin/mongosh --host 192.168.79.138 --port 27017 -u admin -p zs@@123 --authenticationDatabase admin&gt; use admin&gt; db.grantRolesToUser(&quot;clusteradmin&quot;, [ &#123; role: &quot;readWrite&quot;, db: &quot;testdb1&quot; &#125;])# 查看clusteradmin用户的权限&gt; db.getUser(&quot;clusteradmin&quot;)&#123; _id: &#x27;admin.clusteradmin&#x27;, userId: new UUID(&quot;4236ad1c-bcac-45c8-a593-5c36ea563dbc&quot;), user: &#x27;clusteradmin&#x27;, db: &#x27;admin&#x27;, roles: [ &#123; role: &#x27;clusterAdmin&#x27;, db: &#x27;admin&#x27; &#125;, &#123; role: &#x27;userAdminAnyDatabase&#x27;, db: &#x27;admin&#x27; &#125;, &#123; role: &#x27;readWrite&#x27;, db: &#x27;testdb1&#x27; &#125; ], mechanisms: [ &#x27;SCRAM-SHA-1&#x27;, &#x27;SCRAM-SHA-256&#x27; ]&#125; 重新测试 12345678910# 重新使用clusteradmin用户登录进行测试/usr/local/mongosh/bin/mongosh --host 192.168.79.138 --port 27017 -u clusteradmin -p zs@@123 --authenticationDatabase admin&gt; use testdb1&gt; for (i = 1; i &lt;= 12341; i++) &#123; db.users.insert(&#123; userId: i, data: &quot;test&quot; + i &#125;); &#125;# 此时无报错了// 验证数据分布db.users.getShardDistribution() 输出 1234567891011121314151617181920Shard shard1 at shard1/192.168.79.138:27018,192.168.79.139:27018&#123; data: &#x27;639KiB&#x27;, docs: 12341, chunks: 2, &#x27;estimated data per chunk&#x27;: &#x27;319KiB&#x27;, &#x27;estimated docs per chunk&#x27;: 6170&#125;---Totals&#123; data: &#x27;639KiB&#x27;, docs: 12341, chunks: 2, &#x27;Shard shard1&#x27;: [ &#x27;100 % data&#x27;, &#x27;100 % docs in cluster&#x27;, &#x27;53B avg obj size on shard&#x27; ]&#125; 以下是对信息的解析： 分片基本信息 Shard shard1 at shard1/192.168.79.138:27018,192.168.79.139:27018：表示名为 shard1 的分片，它由位于 192.168.79.138 主机的 27018 端口和 192.168.79.139 主机的 27018 端口上的节点组成。 分片数据统计信息 data: &#39;639KiB&#39;：说明该分片上存储的数据总量为 639KiB。 docs: 12341：表示该分片上存储的文档数量为 12341 个。 chunks: 2：指该分片被划分为 2 个数据块。 &#39;estimated data per chunk&#39;: &#39;319KiB&#39;：是对每个数据块平均存储数据量的估计值，约为 319KiB。 &#39;estimated docs per chunk&#39;: 6170：是对每个数据块平均存储文档数量的估计值，约为 6170 个。 集群总体统计信息 data: &#39;639KiB&#39; 和 docs: 12341：再次强调整个集群中数据总量和文档总数与 shard1 中的相同，说明当前集群只有这一个分片存储了数据。 chunks: 2：表示整个集群的数据块数量为 2 个。 &#39;Shard shard1&#39;: [&#39;100 % data&#39;, &#39;100 % docs in cluster&#39;, &#39;53B avg obj size on shard&#39;]：指出 shard1 存储了集群中 100% 的数据和文档，并且该分片上对象的平均大小为 53B。 六 维护6.1 查看分片数据分布1234use configdb.chunks.aggregate([ &#123; $group: &#123; _id: &quot;$shard&quot;, count: &#123; $sum: 1 &#125; &#125; &#125;]) 1234# 输出：[ &#123; _id: &#x27;shard1&#x27;, count: 1028 &#125; ]# 目前只有一个分片 6.2 平衡器管理12345// 检查平衡器是否正在运行sh.isBalancerRunning()// 检查平衡器是否已启用sh.getBalancerState() 启用或禁用平衡器 要是你需要启用或者禁用平衡器，可以使用 sh.setBalancerState() 命令。 12345// 启用平衡器sh.setBalancerState(true)// 禁用平衡器sh.setBalancerState(false) 配置平衡器运行时间 123456use configdb.settings.update( &#123; _id: &quot;balancer&quot; &#125;, &#123; $set: &#123; activeWindow: &#123; start: &quot;23:00&quot;, stop: &quot;05:00&quot; &#125; &#125; &#125;, &#123; upsert: true &#125;) 上述代码将平衡器的运行时间设置为每天晚上 23:00 到次日凌晨 05:00 6.3 资源监控 tools下载地址：https://fastdl.mongodb.org/tools/db/mongodb-database-tools-rhel70-x86_64-100.12.0.tgz 1234# 工具安装tar xf mongodb-database-tools-rhel70-x86_64-100.12.0.tgz -C /usr/local/cd /usr/local/mv mongodb-database-tools-rhel70-x86_64-100.12.0 mongotools 资源监控 1/usr/local/mongotools/bin/mongostat --host 192.168.79.138 --port 27017 -u clusteradmin -p zs@@123 --authenticationDatabase admin 输出： 123456789101112132025-04-10T02:12:59.188-0700 WARNING: On some systems, a password provided directly using --password may be visible to system status programs such as `ps` that may be invoked by other users. Consider omitting the password to provide it via stdin, or using the --config option to specify a configuration file with the password.insert query update delete getmore command flushes mapped vsize res faults qrw arw net_in net_out conn time *0 *0 *0 *0 0 1|0 0 0B 2.46G 31.0M 0 0|0 0|0 215b 23.2k 3 Apr 10 02:13:01.212 *0 *0 *0 *0 0 0|0 0 0B 2.46G 31.0M 0 0|0 0|0 213b 23.0k 3 Apr 10 02:13:02.212 *0 *0 *0 *0 0 1|0 0 0B 2.46G 32.0M 0 0|0 0|0 214b 23.1k 3 Apr 10 02:13:03.210 *0 *0 *0 *0 0 1|0 0 0B 2.46G 32.0M 0 0|0 0|0 214b 23.0k 3 Apr 10 02:13:04.210 *0 *0 *0 *0 0 0|0 0 0B 2.46G 32.0M 0 0|0 0|0 213b 22.9k 3 Apr 10 02:13:05.212 *0 *0 *0 *0 0 1|0 0 0B 2.46G 32.0M 0 0|0 0|0 214b 23.1k 3 Apr 10 02:13:06.209 *0 *0 *0 *0 0 1|0 0 0B 2.46G 32.0M 0 0|0 0|0 214b 23.1k 3 Apr 10 02:13:07.205 *0 *0 *0 *0 0 0|0 0 0B 2.46G 32.0M 0 0|0 0|0 213b 22.9k 3 Apr 10 02:13:08.209 *0 *0 *0 *0 0 2|0 0 0B 2.46G 32.0M 0 0|0 0|0 378b 23.5k 3 Apr 10 02:13:09.208 *0 *0 *0 *0 0 1|0 0 0B 2.46G 32.0M 0 0|0 0|0 361b 23.4k 3 Apr 10 02:13:10.210 主要字段含义： insert/query/update/delete: 每秒操作次数getmore: 游标批量获取次数command: 命令执行次数dirty/used: WiredTiger 缓存使用率vsize/res: 虚拟内存&#x2F;常驻内存使用量net_in/net_out: 网络流量 6.4 添加、删除节点6.4.1 添加配置节点 前期准备 12345678910// 准备一台新服务器，保证mongodb，mongosh的版本，部署目录，权限全部保持一致// 初始化操作（省略）// 拷贝keyfile到新的服务器，启动配置进程/usr/local/mongodb/bin/mongod -f /usr/local/mongodb/conf/mongod-config.conf// 在配置集群主节点操作rs.add(&quot;hostip:port&quot;) # 示例 rs.add(&quot;192.168.79.143:27019&quot;)// 查看是否添加成功rs.status() 输出： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141&#123; set: &#x27;configRepl&#x27;, date: ISODate(&quot;2025-04-18T08:04:34.902Z&quot;), myState: 1, term: Long(&quot;10&quot;), syncSourceHost: &#x27;&#x27;, syncSourceId: -1, configsvr: true, heartbeatIntervalMillis: Long(&quot;2000&quot;), majorityVoteCount: 2, writeMajorityCount: 2, votingMembersCount: 3, writableVotingMembersCount: 3, optimes: &#123; lastCommittedOpTime: &#123; ts: Timestamp(&#123; t: 1744963473, i: 2 &#125;), t: Long(&quot;10&quot;) &#125;, lastCommittedWallTime: ISODate(&quot;2025-04-18T08:04:33.704Z&quot;), readConcernMajorityOpTime: &#123; ts: Timestamp(&#123; t: 1744963473, i: 2 &#125;), t: Long(&quot;10&quot;) &#125;, appliedOpTime: &#123; ts: Timestamp(&#123; t: 1744963473, i: 2 &#125;), t: Long(&quot;10&quot;) &#125;, durableOpTime: &#123; ts: Timestamp(&#123; t: 1744963473, i: 2 &#125;), t: Long(&quot;10&quot;) &#125;, lastAppliedWallTime: ISODate(&quot;2025-04-18T08:04:33.704Z&quot;), lastDurableWallTime: ISODate(&quot;2025-04-18T08:04:33.704Z&quot;) &#125;, lastStableRecoveryTimestamp: Timestamp(&#123; t: 1744963443, i: 1 &#125;), electionCandidateMetrics: &#123; lastElectionReason: &#x27;electionTimeout&#x27;, lastElectionDate: ISODate(&quot;2025-04-18T08:00:17.849Z&quot;), electionTerm: Long(&quot;10&quot;), lastCommittedOpTimeAtElection: &#123; ts: Timestamp(&#123; t: 0, i: 0 &#125;), t: Long(&quot;-1&quot;) &#125;, lastSeenOpTimeAtElection: &#123; ts: Timestamp(&#123; t: 1744963018, i: 2 &#125;), t: Long(&quot;8&quot;) &#125;, numVotesNeeded: 2, priorityAtElection: 1, electionTimeoutMillis: Long(&quot;10000&quot;), numCatchUpOps: Long(&quot;0&quot;), newTermStartDate: ISODate(&quot;2025-04-18T08:00:17.859Z&quot;), wMajorityWriteAvailabilityDate: ISODate(&quot;2025-04-18T08:00:18.306Z&quot;) &#125;, members: [ &#123; _id: 0, name: &#x27;192.168.79.138:27019&#x27;, health: 1, state: 1, stateStr: &#x27;PRIMARY&#x27;, uptime: 271, optime: &#123; ts: Timestamp(&#123; t: 1744963473, i: 2 &#125;), t: Long(&quot;10&quot;) &#125;, optimeDate: ISODate(&quot;2025-04-18T08:04:33.000Z&quot;), lastAppliedWallTime: ISODate(&quot;2025-04-18T08:04:33.704Z&quot;), lastDurableWallTime: ISODate(&quot;2025-04-18T08:04:33.704Z&quot;), syncSourceHost: &#x27;&#x27;, syncSourceId: -1, infoMessage: &#x27;&#x27;, electionTime: Timestamp(&#123; t: 1744963217, i: 1 &#125;), electionDate: ISODate(&quot;2025-04-18T08:00:17.000Z&quot;), configVersion: 2, configTerm: 10, self: true, lastHeartbeatMessage: &#x27;&#x27; &#125;, &#123; _id: 1, name: &#x27;192.168.79.139:27019&#x27;, health: 1, state: 2, stateStr: &#x27;SECONDARY&#x27;, uptime: 269, optime: &#123; ts: Timestamp(&#123; t: 1744963472, i: 1 &#125;), t: Long(&quot;10&quot;) &#125;, optimeDurable: &#123; ts: Timestamp(&#123; t: 1744963472, i: 1 &#125;), t: Long(&quot;10&quot;) &#125;, optimeDate: ISODate(&quot;2025-04-18T08:04:32.000Z&quot;), optimeDurableDate: ISODate(&quot;2025-04-18T08:04:32.000Z&quot;), lastAppliedWallTime: ISODate(&quot;2025-04-18T08:04:33.704Z&quot;), lastDurableWallTime: ISODate(&quot;2025-04-18T08:04:33.704Z&quot;), lastHeartbeat: ISODate(&quot;2025-04-18T08:04:32.942Z&quot;), lastHeartbeatRecv: ISODate(&quot;2025-04-18T08:04:32.940Z&quot;), pingMs: Long(&quot;0&quot;), lastHeartbeatMessage: &#x27;&#x27;, syncSourceHost: &#x27;192.168.79.138:27019&#x27;, syncSourceId: 0, infoMessage: &#x27;&#x27;, configVersion: 2, configTerm: 10 &#125;, &#123; _id: 2, name: &#x27;192.168.79.140:27019&#x27;, health: 1, state: 2, stateStr: &#x27;SECONDARY&#x27;, uptime: 267, optime: &#123; ts: Timestamp(&#123; t: 1744963472, i: 1 &#125;), t: Long(&quot;10&quot;) &#125;, optimeDurable: &#123; ts: Timestamp(&#123; t: 1744963472, i: 1 &#125;), t: Long(&quot;10&quot;) &#125;, optimeDate: ISODate(&quot;2025-04-18T08:04:32.000Z&quot;), optimeDurableDate: ISODate(&quot;2025-04-18T08:04:32.000Z&quot;), lastAppliedWallTime: ISODate(&quot;2025-04-18T08:04:33.704Z&quot;), lastDurableWallTime: ISODate(&quot;2025-04-18T08:04:33.704Z&quot;), lastHeartbeat: ISODate(&quot;2025-04-18T08:04:32.942Z&quot;), lastHeartbeatRecv: ISODate(&quot;2025-04-18T08:04:32.949Z&quot;), pingMs: Long(&quot;0&quot;), lastHeartbeatMessage: &#x27;&#x27;, syncSourceHost: &#x27;192.168.79.139:27019&#x27;, syncSourceId: 1, infoMessage: &#x27;&#x27;, configVersion: 2, configTerm: 10 &#125;, &#123; _id: 3, name: &#x27;192.168.79.143:27019&#x27;, health: 0, state: 6, stateStr: &#x27;(not reachable/healthy)&#x27;, uptime: 0, optime: &#123; ts: Timestamp(&#123; t: 0, i: 0 &#125;), t: Long(&quot;-1&quot;) &#125;, optimeDurable: &#123; ts: Timestamp(&#123; t: 0, i: 0 &#125;), t: Long(&quot;-1&quot;) &#125;, optimeDate: ISODate(&quot;1970-01-01T00:00:00.000Z&quot;), optimeDurableDate: ISODate(&quot;1970-01-01T00:00:00.000Z&quot;), lastAppliedWallTime: ISODate(&quot;1970-01-01T00:00:00.000Z&quot;), lastDurableWallTime: ISODate(&quot;1970-01-01T00:00:00.000Z&quot;), lastHeartbeat: ISODate(&quot;2025-04-18T08:04:32.947Z&quot;), lastHeartbeatRecv: ISODate(&quot;1970-01-01T00:00:00.000Z&quot;), pingMs: Long(&quot;0&quot;), lastHeartbeatMessage: &#x27;&#x27;, authenticated: false, syncSourceHost: &#x27;&#x27;, syncSourceId: -1, infoMessage: &#x27;&#x27;, configVersion: -1, configTerm: -1 &#125; ], ok: 1, lastCommittedOpTime: Timestamp(&#123; t: 1744963473, i: 2 &#125;), &#x27;$clusterTime&#x27;: &#123; clusterTime: Timestamp(&#123; t: 1744963473, i: 2 &#125;), signature: &#123; hash: Binary(Buffer.from(&quot;bb24c5d713e2289c327d05e9887f7d9efd7eb11d&quot;, &quot;hex&quot;), 0), keyId: Long(&quot;7491208092835643417&quot;) &#125; &#125;, operationTime: Timestamp(&#123; t: 1744963473, i: 2 &#125;)&#125; 已经看到节点添加成功了（生产环境使用主机名，不建议ip）修改mongos配置文件，把新的配置节点信息添加，重启所有节点mongos服务 6.4.2 添加分片副本集节点12345678910111213141516// 在主节点连接分片集群（新节点服务已经运行，配置，数据目录等信息保持一致）root@localhost conf]# mongosh --port 27018Current Mongosh Log ID: 68020801ac653b7dba78a0e8Connecting to: mongodb://127.0.0.1:27018/?directConnection=true&amp;serverSelectionTimeoutMS=2000&amp;appName=mongosh+1.10.6Using MongoDB: 6.0.21Using Mongosh: 1.10.6mongosh 2.5.0 is available for download: https://www.mongodb.com/try/download/shellFor mongosh info see: https://docs.mongodb.com/mongodb-shell/&gt; use admin switched to db admin&gt; db.auth(&quot;admin&quot;,&quot;zs@@123&quot;)&#123; ok: 1 &#125; rs.add(&quot;new-node:27018&quot;) 123456789101112131415&gt; rs.add(&quot;192.168.79.143:27018&quot;)// 输出：&#123; ok: 1, lastCommittedOpTime: Timestamp(&#123; t: 1744963782, i: 1 &#125;), &#x27;$clusterTime&#x27;: &#123; clusterTime: Timestamp(&#123; t: 1744963783, i: 1 &#125;), signature: &#123; hash: Binary(Buffer.from(&quot;5b05512ecf6e13784b4f4dbef9c102f13af06e16&quot;, &quot;hex&quot;), 0), keyId: Long(&quot;7491208092835643417&quot;) &#125; &#125;, operationTime: Timestamp(&#123; t: 1744963782, i: 1 &#125;)&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136&gt; rs.status() // 检查新节点的stateStr应为&quot;SECONDARY&quot;// 输出：&#123; set: &#x27;shard1&#x27;, date: ISODate(&quot;2025-04-18T08:10:02.797Z&quot;), myState: 1, term: Long(&quot;12&quot;), syncSourceHost: &#x27;&#x27;, syncSourceId: -1, heartbeatIntervalMillis: Long(&quot;2000&quot;), majorityVoteCount: 3, writeMajorityCount: 3, votingMembersCount: 4, writableVotingMembersCount: 3, optimes: &#123; lastCommittedOpTime: &#123; ts: Timestamp(&#123; t: 1744963799, i: 1 &#125;), t: Long(&quot;12&quot;) &#125;, lastCommittedWallTime: ISODate(&quot;2025-04-18T08:09:59.141Z&quot;), readConcernMajorityOpTime: &#123; ts: Timestamp(&#123; t: 1744963799, i: 1 &#125;), t: Long(&quot;12&quot;) &#125;, appliedOpTime: &#123; ts: Timestamp(&#123; t: 1744963799, i: 1 &#125;), t: Long(&quot;12&quot;) &#125;, durableOpTime: &#123; ts: Timestamp(&#123; t: 1744963799, i: 1 &#125;), t: Long(&quot;12&quot;) &#125;, lastAppliedWallTime: ISODate(&quot;2025-04-18T08:09:59.141Z&quot;), lastDurableWallTime: ISODate(&quot;2025-04-18T08:09:59.141Z&quot;) &#125;, lastStableRecoveryTimestamp: Timestamp(&#123; t: 1744963749, i: 1 &#125;), electionCandidateMetrics: &#123; lastElectionReason: &#x27;electionTimeout&#x27;, lastElectionDate: ISODate(&quot;2025-04-18T08:00:29.004Z&quot;), electionTerm: Long(&quot;12&quot;), lastCommittedOpTimeAtElection: &#123; ts: Timestamp(&#123; t: 0, i: 0 &#125;), t: Long(&quot;-1&quot;) &#125;, lastSeenOpTimeAtElection: &#123; ts: Timestamp(&#123; t: 1744963001, i: 1 &#125;), t: Long(&quot;10&quot;) &#125;, numVotesNeeded: 2, priorityAtElection: 2, electionTimeoutMillis: Long(&quot;10000&quot;), numCatchUpOps: Long(&quot;0&quot;), newTermStartDate: ISODate(&quot;2025-04-18T08:00:29.008Z&quot;), wMajorityWriteAvailabilityDate: ISODate(&quot;2025-04-18T08:00:29.375Z&quot;) &#125;, members: [ &#123; _id: 0, name: &#x27;192.168.79.138:27018&#x27;, health: 1, state: 1, stateStr: &#x27;PRIMARY&#x27;, uptime: 588, optime: &#123; ts: Timestamp(&#123; t: 1744963799, i: 1 &#125;), t: Long(&quot;12&quot;) &#125;, optimeDate: ISODate(&quot;2025-04-18T08:09:59.000Z&quot;), lastAppliedWallTime: ISODate(&quot;2025-04-18T08:09:59.141Z&quot;), lastDurableWallTime: ISODate(&quot;2025-04-18T08:09:59.141Z&quot;), syncSourceHost: &#x27;&#x27;, syncSourceId: -1, infoMessage: &#x27;&#x27;, electionTime: Timestamp(&#123; t: 1744963229, i: 1 &#125;), electionDate: ISODate(&quot;2025-04-18T08:00:29.000Z&quot;), configVersion: 3, configTerm: 12, self: true, lastHeartbeatMessage: &#x27;&#x27; &#125;, &#123; _id: 1, name: &#x27;192.168.79.139:27018&#x27;, health: 1, state: 2, stateStr: &#x27;SECONDARY&#x27;, uptime: 583, optime: &#123; ts: Timestamp(&#123; t: 1744963799, i: 1 &#125;), t: Long(&quot;12&quot;) &#125;, optimeDurable: &#123; ts: Timestamp(&#123; t: 1744963799, i: 1 &#125;), t: Long(&quot;12&quot;) &#125;, optimeDate: ISODate(&quot;2025-04-18T08:09:59.000Z&quot;), optimeDurableDate: ISODate(&quot;2025-04-18T08:09:59.000Z&quot;), lastAppliedWallTime: ISODate(&quot;2025-04-18T08:09:59.141Z&quot;), lastDurableWallTime: ISODate(&quot;2025-04-18T08:09:59.141Z&quot;), lastHeartbeat: ISODate(&quot;2025-04-18T08:10:02.335Z&quot;), lastHeartbeatRecv: ISODate(&quot;2025-04-18T08:10:02.335Z&quot;), pingMs: Long(&quot;0&quot;), lastHeartbeatMessage: &#x27;&#x27;, syncSourceHost: &#x27;192.168.79.138:27018&#x27;, syncSourceId: 0, infoMessage: &#x27;&#x27;, configVersion: 3, configTerm: 12 &#125;, &#123; _id: 2, name: &#x27;192.168.79.140:27018&#x27;, health: 1, state: 7, stateStr: &#x27;ARBITER&#x27;, uptime: 584, lastHeartbeat: ISODate(&quot;2025-04-18T08:10:02.335Z&quot;), lastHeartbeatRecv: ISODate(&quot;2025-04-18T08:10:02.334Z&quot;), pingMs: Long(&quot;0&quot;), lastHeartbeatMessage: &#x27;&#x27;, syncSourceHost: &#x27;&#x27;, syncSourceId: -1, infoMessage: &#x27;&#x27;, configVersion: 3, configTerm: 12 &#125;, &#123; _id: 3, name: &#x27;192.168.79.143:27018&#x27;, health: 1, state: 2, stateStr: &#x27;SECONDARY&#x27;, # 这里已经改变 uptime: 20, optime: &#123; ts: Timestamp(&#123; t: 1744963799, i: 1 &#125;), t: Long(&quot;12&quot;) &#125;, optimeDurable: &#123; ts: Timestamp(&#123; t: 1744963799, i: 1 &#125;), t: Long(&quot;12&quot;) &#125;, optimeDate: ISODate(&quot;2025-04-18T08:09:59.000Z&quot;), optimeDurableDate: ISODate(&quot;2025-04-18T08:09:59.000Z&quot;), lastAppliedWallTime: ISODate(&quot;2025-04-18T08:09:59.141Z&quot;), lastDurableWallTime: ISODate(&quot;2025-04-18T08:09:59.141Z&quot;), lastHeartbeat: ISODate(&quot;2025-04-18T08:10:02.335Z&quot;), lastHeartbeatRecv: ISODate(&quot;2025-04-18T08:10:00.852Z&quot;), pingMs: Long(&quot;0&quot;), lastHeartbeatMessage: &#x27;&#x27;, syncSourceHost: &#x27;192.168.79.139:27018&#x27;, syncSourceId: 1, infoMessage: &#x27;&#x27;, configVersion: 3, configTerm: 12 &#125; ], ok: 1, lastCommittedOpTime: Timestamp(&#123; t: 1744963799, i: 1 &#125;), &#x27;$clusterTime&#x27;: &#123; clusterTime: Timestamp(&#123; t: 1744963799, i: 2 &#125;), signature: &#123; hash: Binary(Buffer.from(&quot;a72400ff211096017f0f38e351826bc16c3b150a&quot;, &quot;hex&quot;), 0), keyId: Long(&quot;7491208092835643417&quot;) &#125; &#125;, operationTime: Timestamp(&#123; t: 1744963799, i: 1 &#125;)&#125; 分片节点添加成功 验证操作 // 检查分片状态，mongos操作 &gt; use admin &gt; sh.status() // 查看集群分片列表 &gt; db.adminCommand(&#123; listShards: 1 &#125;) // 查看数据平衡 &gt; use config &gt; db.chunks.find().sort(&#123;shard:1&#125;) // 查看数据分布 6.4.3 移除配置节点删除配置节点前的准备工作 检查当前配置节点状态 12345// 连接到任意mongos或配置服务器use configdb.shards.find() // 查看当前分片状态db.mongos.find() // 查看mongos实例rs.status() // 查看配置副本集状态 确保集群健康 12sh.status() // 检查整体集群状态db.adminCommand(&#123; listShards: 1 &#125;) // 列出所有分片 从配置副本集中删除节点 连接到配置副本集的主节点 mongosh &quot;mongodb://admin:password@config-server-primary:27019/admin?authSource=admin&quot; 123456789101112[root@localhost conf]# mongosh --port 27019Current Mongosh Log ID: 68020cc772ba9bbe89b3f522Connecting to: mongodb://127.0.0.1:27019/?directConnection=true&amp;serverSelectionTimeoutMS=2000&amp;appName=mongosh+1.10.6Using MongoDB: 6.0.21Using Mongosh: 1.10.6mongosh 2.5.0 is available for download: https://www.mongodb.com/try/download/shellFor mongosh info see: https://docs.mongodb.com/mongodb-shell/&gt; use admin&gt; db.auth(&quot;admin&quot;,&quot;zs@@123&quot;) 查看当前副本集成员 12rs.conf() // 查看详细配置rs.status() // 查看节点状态 删除指定配置节点 rs.remove(&quot;config-server-to-remove:27019&quot;) // 替换为要删除的节点地址 1&gt; rs.remove(&quot;192.168.79.143:27019&quot;) 验证 1rs.status() // 确认节点已移除 更新所有mongos配置 修改mongos配置文件 123# mongos.confsharding: configDB: configReplSet/remaining-config1:27019,remaining-config2:27019 # 移除被删除的节点 重启所有节点mongos服务 监控调整 123// 检查配置服务器状态use admindb.adminCommand(&#123; replSetGetStatus: 1 &#125;) 6.4.4 删除分片副本集节点安全删除节点 连接到主节点 mongosh &quot;mongodb://admin:password@primary-node:27018/admin?authSource=admin&quot; 1234567891011121314[root@localhost conf]# mongosh --port 27018Current Mongosh Log ID: 680212ff368d20209c829b99Connecting to: mongodb://127.0.0.1:27018/?directConnection=true&amp;serverSelectionTimeoutMS=2000&amp;appName=mongosh+1.10.6Using MongoDB: 6.0.21Using Mongosh: 1.10.6mongosh 2.5.0 is available for download: https://www.mongodb.com/try/download/shellFor mongosh info see: https://docs.mongodb.com/mongodb-shell/shard1 [direct: primary] test&gt; use adminswitched to db adminshard1 [direct: primary] admin&gt; db.auth(&quot;admin&quot;,&quot;zs@@123&quot;)&#123; ok: 1 &#125; 降级主节点（如果要删除的是主节点） 12rs.stepDown() // 强制主节点降级rs.status() // 确认新主节点已选举产生 正式删除节点 rs.remove(&quot;node-to-remove:27018&quot;) // 替换为要删除的节点地址 1&gt; rs.remove(&quot;192.168.79.143:27018&quot;) 12345678910111213// 输出：&#123; ok: 1, lastCommittedOpTime: Timestamp(&#123; t: 1744966680, i: 1 &#125;), &#x27;$clusterTime&#x27;: &#123; clusterTime: Timestamp(&#123; t: 1744966680, i: 1 &#125;), signature: &#123; hash: Binary(Buffer.from(&quot;f40ce72c59baa0b342b2cf6cf93c8769d9a0e3e5&quot;, &quot;hex&quot;), 0), keyId: Long(&quot;7491208092835643417&quot;) &#125; &#125;, operationTime: Timestamp(&#123; t: 1744966680, i: 1 &#125;)&#125; 验证删除结果 12rs.conf() // 检查成员列表rs.status() // 查看节点状态","categories":[{"name":"架构基础","slug":"架构基础","permalink":"https://kkabuzs.github.io/categories/%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://kkabuzs.github.io/tags/MongoDB/"}]},{"title":"MongoDB-6.0.21副本集离线部署手册-（Replica Set）副本集模式","slug":"mongodb-6-0-21-lixian-fubenjibushu","date":"2025-04-21T17:51:21.000Z","updated":"2025-04-21T17:51:21.000Z","comments":true,"path":"articles/2025/04/22/mongodb-6-0-21-lixian-fubenjibushu/","permalink":"https://kkabuzs.github.io/articles/2025/04/22/mongodb-6-0-21-lixian-fubenjibushu/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 MongoDB-6.0.21副本集离线部署手册-（Replica Set）副本集模式一 介绍 MongoDB 副本集是一组维护相同数据集的 MongoDB 实例，由一个主节点（Primary）和多个从节点（Secondary）组成。客户端的写操作只能在主节点上执行，而读操作可以在主节点或从节点上执行。副本集通过心跳机制来监控成员状态，当主节点出现故障时，会自动进行选举，从从节点中选出一个新的主节点，以保证系统的高可用性。 1.1 架构组成 主节点（Primary）： 唯一可以处理写操作的节点。当客户端发起写请求时，主节点会将写操作应用到自己的数据集上，并将这些操作记录到操作日志（oplog）中。 负责将 oplog 中的操作同步到从节点。 从节点（Secondary）： 从主节点复制 oplog，并将其中的操作应用到自己的数据集上，从而与主节点保持数据一致。 可以处理读请求，通过将读操作分发到从节点，可以减轻主节点的负载，提高系统的读性能。 仲裁节点（Arbiter）： 仲裁节点不存储数据，只参与主节点的选举过程。当副本集中的节点数量为偶数时，仲裁节点可以打破平局，确保选举能够顺利进行。 1.2 工作流程 写操作： 客户端将写请求发送到主节点，主节点将写操作应用到自己的数据集上，并将操作记录到 oplog 中。然后，主节点将 oplog 同步到从节点。 读操作： 客户端可以选择将读请求发送到主节点或从节点。如果发送到主节点，将获得最新的数据；如果发送到从节点，可能会获得稍微旧一点的数据。 故障转移： 当主节点出现故障时，从节点会检测到主节点不可用，并发起选举过程。选举过程中，各个从节点会投票选出一个新的主节点。仲裁节点在选举过程中起到关键作用，确保选举能够顺利进行。 1.3 优点高可用性 自动故障转移： 当主节点出现故障时，副本集可以自动进行选举，选出一个新的主节点，从而保证系统的高可用性。客户端无需手动干预，即可继续进行读写操作。 数据冗余： 副本集中的每个从节点都存储了一份完整的数据副本，即使某个节点出现故障，其他节点仍然可以提供服务。 数据一致性 操作日志同步： 主节点将写操作记录到 oplog 中，并将 oplog 同步到从节点。从节点通过应用 oplog 中的操作，与主节点保持数据一致。 多数确认： MongoDB 支持多数确认（majority write concern），即写操作必须在大多数节点上成功执行后，才会向客户端返回成功响应。这样可以确保数据的一致性。 读写分离 读扩展： 可以将读请求分发到从节点，从而减轻主节点的负载，提高系统的读性能。特别是在读密集型应用中，读写分离可以显著提高系统的吞吐量。 灵活配置： 客户端可以根据业务需求，灵活选择将读请求发送到主节点或从节点。例如，对于对数据实时性要求较高的读请求，可以发送到主节点；对于对数据实时性要求较低的读请求，可以发送到从节点。 1.4 缺点复杂性增加 部署和管理复杂： 副本集需要部署多个节点，并且需要进行配置和管理。例如，需要配置节点之间的通信、选举参数等。 故障诊断困难： 当副本集出现故障时，由于涉及多个节点，故障诊断和排查会比较困难。需要对每个节点的状态进行检查，找出故障原因。 成本增加 硬件成本： 副本集需要部署多个节点，每个节点都需要占用一定的硬件资源，如 CPU、内存、磁盘等。这会增加硬件成本。 维护成本： 需要对多个节点进行维护和管理，如监控节点状态、备份数据等。这会增加维护成本。 数据延迟 从节点同步延迟： 从节点从主节点复制 oplog 并应用操作需要一定的时间，因此从节点上的数据可能会稍微落后于主节点。在对数据实时性要求较高的应用中，可能会出现数据不一致的问题。 选举延迟： 当主节点出现故障时，选举新的主节点需要一定的时间。在选举过程中，副本集可能无法处理写操作，会导致一定的服务中断。 二 准备工作2.1 准备环境 所有节点使用相同操作系统版本（推荐RHEL&#x2F;CentOS 7+或Ubuntu 18.04+） IP 系统 配置 说明 192.168.79.130 Centos7.6 1C1G mongodb主节点 192.168.79.131 Centos7.6 1C1G mongodb从节点 192.168.79.132 Centos7.6 1C1G mongodb仲裁节点 确保节点间时间同步（NTP服务） 禁用THP（Transparent Huge Pages） 2.2 服务器初始化12345678910111213141516171819202122# 三个节点统一设置，这里以 主节点 为例[root@localhost ~]# cat /etc/redhat-release CentOS Linux release 7.6.1810 (Core) [root@localhost ~]# uname -r3.10.0-957.el7.x86_64sed -i.bak &#x27;7s/enforcing/disabled/&#x27; /etc/selinux/configsetenforce 0 systemctl stop firewalldsystemctl disable firewalld# 创建用户和目录groupadd mongodbuseradd -r -g mongodb -s /bin/false mongodbmkdir -p /data/mongodb/&#123;db,logs&#125;chown -R mongodb:mongodb /data/mongodb# 禁用THP（添加到/etc/rc.local保证重启生效）echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defrag 2.3 离线安装MongoDB准备安装包 mongodb-linux-x86_64-rhel70-6.0.21.tgz 主体文件包下载地址：https://www.mongodb.com/try/download/community mongosh-1.10.6-linux-x64.tgz 启动命令包（因为mongodb6.0要用mongosh命令连接，故此需要额外下载工具包）下载地址：https://www.mongodb.com/try/download/shell 2.3.1 手动安装（所有节点）123456# 上传安装包到服务器[root@localhost ~]# tar xf mongodb-linux-x86_64-rhel70-6.0.21.tgz -C /usr/local[root@localhost ~]# tar xf mongosh-1.10.6-linux-x64.tgz[root@localhost ~]# mv mongosh-1.10.6-linux-x64 mongosh[root@localhost ~]# cd /usr/local[root@localhost local]# mv mongodb-linux-x86_64-rhel70-6.0.21 mongodb 2.3.2 创建配置文件（&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;conf&#x2F;mongodb.yml）1234567891011121314151617181920212223242526272829303132[root@localhost ~]# mkdir /usr/local/mongodb/conf[root@localhost ~]# vim /usr/local/mongodb/conf/mongodb.ymlstorage: dbPath: /data/mongodb/db journal: enabled: true wiredTiger: engineConfig: cacheSizeGB: 1 # 根据内存调整，建议不超过可用内存的50%systemLog: destination: file path: /data/mongodb/logs/mongod.log logAppend: truenet: port: 27017 bindIp: 0.0.0.0replication: replSetName: rs0 # 副本集名称processManagement: fork: true pidFilePath: /var/run/mongod.pidsecurity: authorization: disabled # 禁用认证# authorization: enabled # 启用认证# keyFile: /data/mongodb/keyfile # 副本集认证文件# 注意这里先禁用认证，因为直接开启 集群初始化可能会报错，初始化集群之后重新开启认证即可。 2.3.3 初始化副本集12345678910111213141516# 所有节点都启动mongodb服务[root@localhost ~]# /usr/local/mongodb/bin/mongod -f /usr/local/mongodb/conf/mongodb.yml# 在注解点连接集群[root@localhost ~]# cd mongosh/bin/[root@localhost bin]# ./mongosh --host 192.168.79.130rs.initiate(&#123; _id: &quot;rs0&quot;, version: 1, members: [ &#123; _id: 0, host: &quot;192.168.79.130:27017&quot; &#125;, &#123; _id: 1, host: &quot;192.168.79.131:27017&quot; &#125;, &#123; _id: 2, host: &quot;192.168.79.132:27017&quot;, arbiterOnly: true &#125; # 仲裁节点 ] &#125;) 2.3.4 创建管理用户12345678910use admin # 切换为admindb.createUser(&#123; user: &quot;admin&quot;, pwd: &quot;yourStrongPassword&quot;, # 替换为实际密码 roles: [ &#123; role: &quot;root&quot;, db: &quot;admin&quot; &#125;, &#123; role: &quot;clusterAdmin&quot;, db: &quot;admin&quot; &#125; ]&#125;) 2.3.5 开启认证123456789101112131415161718192021222324252627282930313233343536373839404142434445# 停止服务（所有节点）pkill mongodopenssl rand -base64 756 &gt; /data/mongodb/keyfilechmod 400 /data/mongodb/keyfilechown mongodb:mongodb /data/mongodb/keyfile# 修改配置文件[root@localhost ~]# vim /usr/local/mongodb/conf/mongodb.ymlstorage: dbPath: /data/mongodb/db journal: enabled: true wiredTiger: engineConfig: cacheSizeGB: 1 # 根据内存调整，建议不超过可用内存的50%systemLog: destination: file path: /data/mongodb/logs/mongod.log logAppend: truenet: port: 27017 bindIp: 0.0.0.0replication: replSetName: rs0 # 副本集名称processManagement: fork: true pidFilePath: /var/run/mongod.pidsecurity:# authorization: disabled # 禁用认证 authorization: enabled # 启用认证 keyFile: /data/mongodb/keyfile # 副本集认证文件 # 重新启动服务/usr/local/mongodb/bin/mongod -f /usr/local/mongodb/conf/mongodb.yml 2.3.6 测试连接123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113[root@localhost ~]# lsanaconda-ks.cfg mongodb-linux-x86_64-rhel70-6.0.21.tgz mongosh mongosh-1.10.6-linux-x64.tgz original-ks.cfg[root@localhost ~]# ./mongosh/bin/mongosh --host 192.168.79.130 -u admin -p zs@@123 --authenticationDatabase admin···Current Mongosh Log ID: 67f4bc26d05252519cba53cdConnecting to: mongodb://&lt;credentials&gt;@192.168.79.130:27017/?directConnection=true&amp;authSource=admin&amp;appName=mongosh+1.10.6Using MongoDB: 6.0.21Using Mongosh: 1.10.6mongosh 2.4.2 is available for download: https://www.mongodb.com/try/download/shellFor mongosh info see: https://docs.mongodb.com/mongodb-shell/------ The server generated these startup warnings when booting 2025-04-07T22:45:49.608-07:00: The configured WiredTiger cache size is more than 80% of available RAM. See http://dochub.mongodb.org/core/faq-memory-diagnostics-wt 2025-04-07T22:45:50.251-07:00: You are running this process as the root user, which is not recommended------rs0 [direct: other] test&gt; # 验证副本集状态rs0 [direct: other] test&gt; rs.status()···&#123; set: &#x27;rs0&#x27;, date: ISODate(&quot;2025-04-08T06:04:21.933Z&quot;), myState: 3, term: Long(&quot;1&quot;), syncSourceHost: &#x27;&#x27;, syncSourceId: -1, heartbeatIntervalMillis: Long(&quot;2000&quot;), majorityVoteCount: 2, writeMajorityCount: 2, votingMembersCount: 3, writableVotingMembersCount: 2, optimes: &#123; lastCommittedOpTime: &#123; ts: Timestamp(&#123; t: 0, i: 0 &#125;), t: Long(&quot;-1&quot;) &#125;, lastCommittedWallTime: ISODate(&quot;1970-01-01T00:00:00.000Z&quot;), appliedOpTime: &#123; ts: Timestamp(&#123; t: 1744077485, i: 1 &#125;), t: Long(&quot;1&quot;) &#125;, durableOpTime: &#123; ts: Timestamp(&#123; t: 1744077485, i: 1 &#125;), t: Long(&quot;1&quot;) &#125;, lastAppliedWallTime: ISODate(&quot;2025-04-08T01:58:05.695Z&quot;), lastDurableWallTime: ISODate(&quot;2025-04-08T01:58:05.695Z&quot;) &#125;, lastStableRecoveryTimestamp: Timestamp(&#123; t: 1744077485, i: 1 &#125;), members: [ &#123; _id: 0, name: &#x27;192.168.79.130:27017&#x27;, health: 1, state: 3, stateStr: &#x27;RECOVERING&#x27;, uptime: 1112, optime: &#123; ts: Timestamp(&#123; t: 1744077485, i: 1 &#125;), t: Long(&quot;1&quot;) &#125;, optimeDate: ISODate(&quot;2025-04-08T01:58:05.000Z&quot;), lastAppliedWallTime: ISODate(&quot;2025-04-08T01:58:05.695Z&quot;), lastDurableWallTime: ISODate(&quot;2025-04-08T01:58:05.695Z&quot;), syncSourceHost: &#x27;&#x27;, syncSourceId: -1, infoMessage: &#x27;&#x27;, configVersion: 1, configTerm: 1, self: true, lastHeartbeatMessage: &#x27;&#x27; &#125;, &#123; _id: 1, name: &#x27;192.168.79.131:27017&#x27;, health: 0, state: 6, stateStr: &#x27;(not reachable/healthy)&#x27;, uptime: 0, optime: &#123; ts: Timestamp(&#123; t: 0, i: 0 &#125;), t: Long(&quot;-1&quot;) &#125;, optimeDurable: &#123; ts: Timestamp(&#123; t: 0, i: 0 &#125;), t: Long(&quot;-1&quot;) &#125;, optimeDate: ISODate(&quot;1970-01-01T00:00:00.000Z&quot;), optimeDurableDate: ISODate(&quot;1970-01-01T00:00:00.000Z&quot;), lastAppliedWallTime: ISODate(&quot;1970-01-01T00:00:00.000Z&quot;), lastDurableWallTime: ISODate(&quot;1970-01-01T00:00:00.000Z&quot;), lastHeartbeat: ISODate(&quot;2025-04-08T06:04:21.450Z&quot;), lastHeartbeatRecv: ISODate(&quot;1970-01-01T00:00:00.000Z&quot;), pingMs: Long(&quot;0&quot;), lastHeartbeatMessage: &#x27;&#x27;, authenticated: false, syncSourceHost: &#x27;&#x27;, syncSourceId: -1, infoMessage: &#x27;&#x27;, configVersion: -1, configTerm: -1 &#125;, &#123; _id: 2, name: &#x27;192.168.79.132:27017&#x27;, health: 0, state: 6, stateStr: &#x27;(not reachable/healthy)&#x27;, uptime: 0, lastHeartbeat: ISODate(&quot;2025-04-08T06:04:21.798Z&quot;), lastHeartbeatRecv: ISODate(&quot;1970-01-01T00:00:00.000Z&quot;), pingMs: Long(&quot;0&quot;), lastHeartbeatMessage: &#x27;&#x27;, authenticated: false, syncSourceHost: &#x27;&#x27;, syncSourceId: -1, infoMessage: &#x27;&#x27;, configVersion: -1, configTerm: -1 &#125; ], ok: 1&#125; 2.4 日常维护命令2.4.1 查看副本集状态1234rs.status()rs.conf()rs.printReplicationInfo()rs.printSecondaryReplicationInfo() 2.4.2 主从切换12// 在primary上执行rs.stepDown(300) // 300秒内不参与选举 2.4.3 添加&#x2F;移除节点12345// 添加新节点rs.add(&quot;newnode:27017&quot;)// 移除节点rs.remove(&quot;oldnode:27017&quot;) 2.5 注意事项 版本一致性： 确保所有节点MongoDB版本完全相同 防火墙配置： 开放27017端口及节点间所有必要端口 磁盘空间： 监控数据目录空间使用情况 备份策略： 定期执行mongodump或使用文件系统快照 日志轮转： 配置logrotate防止日志文件过大","categories":[{"name":"架构基础","slug":"架构基础","permalink":"https://kkabuzs.github.io/categories/%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://kkabuzs.github.io/tags/MongoDB/"}]},{"title":"MongoDB-6.0.21离线单节点部署","slug":"mongodb-6-0-21-lixian-danjiedianbushu","date":"2025-04-21T15:55:06.000Z","updated":"2025-04-21T15:55:06.000Z","comments":true,"path":"articles/2025/04/21/mongodb-6-0-21-lixian-danjiedianbushu/","permalink":"https://kkabuzs.github.io/articles/2025/04/21/mongodb-6-0-21-lixian-danjiedianbushu/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 MongoDB-6.0.21离线单节点部署一 准备工作 下载离线环境包mongodb-linux-x86_64-rhel70-6.0.21.tgz 主体文件包下载地址：https://www.mongodb.com/try/download/community mongosh-1.10.6-linux-x64.tgz 启动命令包（因为mongodb6.0要用mongosh命令连接，故此需要额外下载工具包）下载地址：https://www.mongodb.com/try/download/shell 1.1 环境初始化1234567891011121314151617181920212223[root@localhost ~]# cat /etc/redhat-release CentOS Linux release 7.6.1810 (Core) [root@localhost ~]# uname -r3.10.0-957.el7.x86_64sed -i.bak &#x27;7s/enforcing/disabled/&#x27; /etc/selinux/configsetenforce 0systemctl stop firewalldsystemctl disable firewalld# 创建用户和目录groupadd mongodbuseradd -r -g mongodb -s /bin/false mongodbmkdir -p /data/mongodb/&#123;db,logs&#125;chown -R mongodb:mongodb /data/mongodb# 禁用THP（添加到/etc/rc.local保证重启生效）echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho -e &quot;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled\\necho never &gt; /sys/kernel/mm/transparent_hugepage/defrag&quot; &gt;&gt; /etc/rc.localchmod +x /etc/rc.local 1.2 离线安装12345678910111213# 解压tar xf mongodb-linux-x86_64-rhel70-6.0.21.tgz -C /usr/local/tar xf mongosh-1.10.6-linux-x64.tgz -C /usr/local/# 改名cd /usr/local/mv mongodb-linux-x86_64-rhel70-6.0.21/ mongodbmv mongosh-1.10.6-linux-x64 mongosh# 创建配置文件目录cd mongodb/mkdir confcd conf 1.3 创建配置文件/usr/local/mongodb/conf/mongod.conf 1234567891011121314151617181920systemLog: destination: file path: /data/mongodb/logs/mongod.log logAppend: truestorage: dbPath: /data/mongodb/db journal: enabled: true wiredTiger: engineConfig: cacheSizeGB: 1 # 根据可用内存调整，建议不超过物理内存的50%net: bindIp: 0.0.0.0 # 生产环境建议绑定具体IP port: 27017processManagement: fork: true # 后台运行 pidFilePath: /var/run/mongod.pid 1.4 启动服务1/usr/local/mongodb/bin/mongod -f /usr/local/mongodb/conf/mongod.conf","categories":[{"name":"架构基础","slug":"架构基础","permalink":"https://kkabuzs.github.io/categories/%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://kkabuzs.github.io/tags/MongoDB/"}]},{"title":"继承与派生","slug":"jichengyupaisheng","date":"2023-05-10T05:52:52.000Z","updated":"2023-05-10T05:52:52.000Z","comments":true,"path":"articles/2023/05/10/jichengyupaisheng/","permalink":"https://kkabuzs.github.io/articles/2023/05/10/jichengyupaisheng/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 26、继承与派生特别鸣谢： — b站老男孩agen老师 一，什么是继承12345&#x27;&#x27;&#x27;继承是一种创建新类的方式，在Python中，新建的类可以继承一个或多个父类，新建的类可称为子类或派生类，父类又可称为基类或超类，子类会遗传父类的属性需要注意的是： 在python中，新建的类可以继承一个或多个父类&#x27;&#x27;&#x27; 12345678910111213141516171819202122232425class Parent1: #定义父类 passclass Parent2: #定义父类 passclass Sub1(Parent1): #单继承 passclass Sub2(Parent1,Parent2): #多继承 pass# 通过类的内置属性__bases__可以查看类继承的所有父类print(Sub1.__bases__) # (&lt;class &#x27;__main__.Parent1&#x27;&gt;,)print(Sub2.__bases__) # (&lt;class &#x27;__main__.Parent1&#x27;&gt;, &lt;class &#x27;__main__.Parent2&#x27;&gt;)# ps1: 在python2中有经典类和新式类之分# 新式类：继承了object类的子类，以及该子类的子类子子类。。。# 经典类：没有继承object类的子类，以及该子类的子类子子类。。。# ps2: 在python3中没有继承任何类，那么会默认继承object类，所以python3中所有的类都是新式类print(Parent1.__bases__) # (&lt;class &#x27;object&#x27;&gt;,)print(Parent2.__bases__) # (&lt;class &#x27;object&#x27;&gt;,) 二，继承与抽象1234567891011121314151617181920212223242526272829303132333435363738# python的多继承# 优点：子类可以同时遗传多个父类的属性，最大限度的重用代码# 缺点：违背人的思维习惯：继承表达的是一种&quot;是&quot;什么的关系。# 代码可读性会变差。# 不建议使用多继承，扩展性变差。# 如果真的涉及到一饿子类不可避免的要重用多个父类的属性，应该使用mixins# 2、为何要用继承：用类解决类与类代码冗余问题# 3、如何实现继承class People: school = &#x27;清华大学&#x27; def __init__(self,name,age,sax): self.name = name self.age = age self.sax = saxclass Teacher(People): def __init__(self,name,age,sax,salary,level): # 调用父类的init方法 People.__init__(self,name,age,sax) self.salary = salary self.level = level def teacher(self): print(&#x27;&#123;&#125; is teaching&#x27;.format(self.name))class Student(People): def student(self): print(&#x27;&#123;&#125; is student&#x27;.format(self.name))teach_obj = Teacher(&#x27;lisi&#x27;,30,&#x27;male&#x27;,5000,&#x27;三&#x27;)teach_obj.teacher() # lisi is teachingprint(teach_obj.name,teach_obj.age,teach_obj.sax,teach_obj.salary,teach_obj.level,teach_obj.school) # lisi 30 male 5000 三 清华大学obj = Student(&#x27;zhangsan&#x27;,18,&#x27;male&#x27;)obj.student()print(obj.school) 三，属性查找1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 单继承背景下的属性查找class Foo: def f1(self): print(&#x27;Foo.f1&#x27;) def f2(self): print(&#x27;Foo.f2&#x27;) self.f1() # obj.f1()class Bar(Foo): def f1(self): print(&#x27;Bar.f1&#x27;)obj = Bar()obj.f2()&#x27;&#x27;&#x27;输出：Foo.f2Bar.f1&#x27;&#x27;&#x27;# 如果就想访问Foo类下的f1函数，有两种实现方法# 示范一：class Foo: def f1(self): print(&#x27;Foo.f1&#x27;) def f2(self): print(&#x27;Foo.f2&#x27;) Foo.f1(self)class Bar(Foo): def f1(self): print(&#x27;Bar.f1&#x27;)obj = Bar()obj.f2()&#x27;&#x27;&#x27;输出：Foo.f2Foo.f1&#x27;&#x27;&#x27;# 示范二：class Foo: def __f1(self): print(&#x27;Foo.f1&#x27;) def f2(self): print(&#x27;Foo.f2&#x27;) self.__f1()class Bar(Foo): def f1(self): print(&#x27;Bar.f1&#x27;)obj = Bar()obj.f2()&#x27;&#x27;&#x27;输出：Foo.f2Foo.f1&#x27;&#x27;&#x27; 四，多继承带来的菱形问题4.1 菱形问题 大多数面向对象语言都不支持多继承，而在Python中，一个子类是可以同时继承多个父类的，这固然可以带来一个子类可以对多个不同父类加以重用的好处，但也有可能引发著名的 Diamond problem菱形问题(或称钻石问题，有时候也被称为“死亡钻石”)，菱形其实就是对下面这种继承结构的形象比喻 A类在顶部，B类和C类分别位于其下方，D类在底部将两者连接在一起形成菱形。 12345678910111213141516171819202122232425262728# 这种继承结构下导致的问题称之为菱形问题：如果A中有一个方法，B和/或C都重写了该方法，而D没有重写它，那么D继承的是哪个版本的方法：class A(object): def test(self): print(&#x27;from A&#x27;)class B(A): def test(self): print(&#x27;from B&#x27;)class C(A): def test(self): print(&#x27;from C&#x27;)class D(B,C): passobj = D()obj.test() # 结果为：from Bprint(D.mro())# [&lt;class &#x27;__main__.D&#x27;&gt;, &lt;class &#x27;__main__.B&#x27;&gt;, &lt;class &#x27;__main__.C&#x27;&gt;, &lt;class &#x27;__main__.A&#x27;&gt;, &lt;class &#x27;object&#x27;&gt;]# 查找顺序如上，简单概括就是，D（B，C）按顺序，先找D，再找B，在找C，如果C在前面，就是先找D，再找C，在找B 4.2 继承原理 python到底是如何实现继承的呢？ 对于你定义的每一个类，Python都会计算出一个方法解析顺序(MRO)列表，该MRO列表就是一个简单的所有基类的线性顺序列表，如下 12print(D.mro())# [&lt;class &#x27;__main__.D&#x27;&gt;, &lt;class &#x27;__main__.B&#x27;&gt;, &lt;class &#x27;__main__.C&#x27;&gt;, &lt;class &#x27;__main__.A&#x27;&gt;, &lt;class &#x27;object&#x27;&gt;] python会在MRO列表上从左到右开始查找基类,直到找到第一个匹配这个属性的类为止。 而这个MRO列表的构造是通过一个C3线性化算法来实现的。我们不去深究这个算法的数学原理,它实际上就是合并所有父类的MRO列表并遵循如下三条准则: 1.子类会先于父类被检查 2.多个父类会根据它们在列表中的顺序被检查 3.如果对下一个类存在两个合法的选择,选择第一个父类 ps: 1.由对象发起的属性查找，会从对象自身的属性里检索，没有则会按照对象的类.mro()规定的顺序依次找下去， 2.由类发起的属性查找，会按照当前类.mro()规定的顺序依次找下去， 4.3 深度优先和广度优先 参照下述代码，多继承结构为非菱形结构，此时，会按照先找B这一条分支，然后再找C这一条分支，最后找D这一条分支的顺序直到找到我们想要的属性 123456789101112131415161718192021222324252627282930313233343536373839class E: def test(self): print(&#x27;from E&#x27;)class F: def test(self): print(&#x27;from F&#x27;)class B(E): def test(self): print(&#x27;from B&#x27;)class C(F): def test(self): print(&#x27;from C&#x27;)class D: def test(self): print(&#x27;from D&#x27;)class A(B, C, D): # def test(self): # print(&#x27;from A&#x27;) passprint(A.mro())&#x27;&#x27;&#x27;[&lt;class &#x27;__main__.A&#x27;&gt;, &lt;class &#x27;__main__.B&#x27;&gt;, &lt;class &#x27;__main__.E&#x27;&gt;, &lt;class &#x27;__main__.C&#x27;&gt;, &lt;class &#x27;__main__.F&#x27;&gt;, &lt;class &#x27;__main__.D&#x27;&gt;, &lt;class &#x27;object&#x27;&gt;]&#x27;&#x27;&#x27;obj = A()obj.test() # 结果为：from B# 可依次注释上述类中的方法test来进行验证 python3都是新式类，都按照这种顺序进行查找 1234567891011121314151617181920212223242526272829303132class G(object): def test(self): print(&#x27;from G&#x27;)class E(G): def test(self): print(&#x27;from E&#x27;)class F(G): def test(self): print(&#x27;from F&#x27;)class B(E): def test(self): print(&#x27;from B&#x27;)class C(F): def test(self): print(&#x27;from C&#x27;)class D(G): def test(self): print(&#x27;from D&#x27;)class A(B,C,D): # def test(self): # print(&#x27;from A&#x27;) passobj = A()obj.test() # 如上图，查找顺序为:obj-&gt;A-&gt;B-&gt;E-&gt;C-&gt;F-&gt;D-&gt;G-&gt;object# 可依次注释上述类中的方法test来进行验证","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"封装","slug":"fengzhuang","date":"2023-05-09T01:37:55.000Z","updated":"2023-05-09T01:37:55.000Z","comments":true,"path":"articles/2023/05/09/fengzhuang/","permalink":"https://kkabuzs.github.io/articles/2023/05/09/fengzhuang/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 25、封装特别鸣谢： — b站老男孩agen老师 一，引入 面向对象编程有三大特性：封装、继承、多态，其中最重要的一个特性就是封装。封装指的就是把数据与功能都整合到一起，听起来是不是很熟悉，没错，我们之前所说的”整合“二字其实就是封装的通俗说法。除此之外，针对封装到对象或者类中的属性，我们还可以严格控制对它们的访问，分两步实现：隐藏与开放接口 二，隐藏属性12345678910111213141516171819202122# 如何隐藏：在属性名面前加__前缀，就会实现一个对外隐藏属性的效果# 说明：# I、在类外部无法直接访问双下滑线开头的属性，但知道了类名和属性名就可以拼出名字：_类名__属性，然后就可以访问了，如Foo._Foo__x，所以说这种操作并没有严格意义上地限制外部访问，仅仅只是一种语法意义上的变形。# II、在类内部是可以直接访问双下滑线开头的属性的，比如self.__f1()，因为在类定义阶段类内部双下滑线开头的属性统一发生了变形。class Foo(): __x = 1 # _Foo__x def __f1(self): # _Foo__f1 print(self) def f2(self): print(self.__x) print(self.__f1)print(Foo._Foo__x) # 1print(Foo._Foo__f1) # &lt;function Foo.__f1 at 0x10b26bd30&gt;# III、变形操作只在类定义阶段发生一次,在类定义之后的赋值操作，不会变形。Foo.__y = 3# print(Foo.__dict__)print(Foo.__y) # 3 三，开放接口 定义属性就是为了使用，所以隐藏并不是目的 3.1 隐藏数据属性 将数据隐藏起来就限制了类外部对数据的直接操作，然后类内应该提供相应的接口来允许类外部间接地操作数据，接口之上可以附加额外的逻辑来对数据的操作进行严格地控制 1234567891011121314151617class People: def __init__(self,name): self.__name = name def get_name(self): # 通过该接口就可以间接的访问到名字属性 print(self.__name) def set_name(self,val): if type(val) is not str: print(&#x27;必须为字符串类型&#x27;) self.__name = valobj = People(&#x27;zhangsan&#x27;)# print(obj.name) # 无法直接用名字属性obj.get_name() # zhangsanobj.set_name(1231) # 必须为字符串类型，封装了之后可以更加严格的判断处理 3.2 隐藏函数属性 目的的是为了隔离复杂度，例如ATM程序的取款功能,该功能有很多其他功能组成，比如插卡、身份认证、输入金额、打印小票、取钱等，而对使用者来说,只需要开发取款这个功能接口即可,其余功能我们都可以隐藏起来 1234567891011121314151617181920class ATM: def __card(self): #插卡 print(&#x27;插卡&#x27;) def __auth(self): #身份认证 print(&#x27;用户认证&#x27;) def __input(self): #输入金额 print(&#x27;输入取款金额&#x27;) def __print_bill(self): #打印小票 print(&#x27;打印账单&#x27;) def __take_money(self): #取钱 print(&#x27;取款&#x27;) def withdraw(self): #取款功能 self.__card() self.__auth() self.__input() self.__print_bill() self.__take_money()obj=ATM()obj.withdraw() 四，property1234# 装饰器：# 是在不修改被装饰对象源代码以及调用方式的前提下为被装饰对象添加新功能的可调用对象# property是一个装饰器，是用来将绑定给对象的方法为伪造成一个数据属性 案例1： 1234567891011121314151617181920212223242526272829303132333435&#x27;&#x27;&#x27;什么是特性propertyproperty是一种特殊的属性，访问它时会执行一段功能（函数）然后返回值例一：BMI指数（bmi是计算而来的，但很明显它听起来像是一个属性而非方法，如果我们将其做成一个属性，更便于理解）成人的BMI数值：过轻：低于18.5正常：18.5-23.9过重：24-27肥胖：28-32非常肥胖, 高于32 体质指数（BMI）=体重（kg）÷身高^2（m） EX：70kg÷（1.75×1.75）=22.86&#x27;&#x27;&#x27;class People: def __init__(self,name,weight,height): self.name = name self.weight = weight self.height = height # 定义函数的原因： # 1、从bmi的公式上看，bmi应该是触发功能计算得到的 # 2、bmi随着身高，体重的变化而动态变化的，不是一个固定的值 # 简单的说，每次都是需要临时计算得到的 # 但是，bmi听起来更像是一个数据属性，而不是功能 @property def bmi(self): return self.weight / (self.height ** 2)obj = People(&#x27;zhangsan&#x27;,72,1.73)print(obj.bmi) # 24.0569347455645 案例2： 12345678910111213141516171819class People2: def __init__(self,name): self.__name = name def get_name(self): return self.__name def set_name(self, name): if type(name) is not str: print(&#x27;必须为str类型&#x27;) self.__name = name def del_name(self): print(&#x27;不让删除&#x27;) # del self.__name name = property(get_name,set_name,del_name)obj1 = People2(&#x27;zhangsan&#x27;)print(obj1.name)obj1.name = &#x27;lisi&#x27;print(obj1.name)del obj1.name 案例3：(推荐使用的方式) 123456789101112131415161718192021222324class People3: def __init__(self,name): self.__name = name @property def name(self): # obj3.name return self.__name @name.setter def name(self, name): # obj3.name = &#x27;lisi&#x27; if type(name) is not str: print(&#x27;必须为str类型&#x27;) self.__name = name @name.deleter def name(self): # del obj3.name print(&#x27;不让删除&#x27;) # del self.__nameobj3 = People3(&#x27;zhangsan&#x27;)print(obj1.name)obj3.name = &#x27;lisi&#x27;print(obj1.name)del obj3.name","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"面向对象编程","slug":"mianxiangduixiangbiancheng","date":"2023-05-05T02:00:09.000Z","updated":"2023-05-05T02:00:09.000Z","comments":true,"path":"articles/2023/05/05/mianxiangduixiangbiancheng/","permalink":"https://kkabuzs.github.io/articles/2023/05/05/mianxiangduixiangbiancheng/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 24、面向对象编程特别鸣谢： — b站老男孩agen老师 一，对象的概念12345678910111213&#x27;&#x27;&#x27;面向过程： 核心是&quot;过程&quot;二字 过程的终极奥义是：将程序流程化 过程是&quot;流水线&quot;，用来分步骤解决问题面向对象： 核心是&quot;对象&quot;二字 对象的终极奥义就是：将程序&quot;整合&quot; 对象是&quot;容器&quot;，用来盛放数据与功能的&#x27;&#x27;&#x27; 所有的程序都是由”数据”与“功能“组成，因而编写程序的本质就是定义出一系列的数据，然后定义出一系列的功能来对数据进行操作。在学习”对象“之前，程序中的数据与功能是分离开的，如下: 12345678910# 程序 = 数据 + 功能# 学生的数据name = &#x27;zhangsan&#x27;age = 18stu_gender = &#x27;male&#x27;# 学生的功能def tell_info(): print(&#x27;name %s age %s 性别 %s&#x27; %(name,age,stu_gender)) 二，类与对象 类即类别&#x2F;种类，是面向对象分析和设计的基石，如果多个对象有相似的数据与功能，那么该多个对象就属于同一种类。有了类的好处是：我们可以把同一类对象相同的数据与功能存放到类里，而无需每个对象都重复存一份，这样每个对象里只需存自己独有的数据即可，极大地节省了空间。所以，如果说对象是用来存放数据与功能的容器，那么类则是用来存放多个对象相同的数据与功能的容器。 综上所述，虽然我们是先介绍对象后介绍类，但是需要强调的是：在程序中，必须要事先定义类，然后再调用类产生对象（调用类拿到的返回值就是对象）。产生对象的类与对象之间存在关联，这种关联指的是：对象可以访问到类中共有的数据与功能，所以类中的内容仍然是属于对象的，类只不过是一种节省空间、减少代码冗余的机制，面向对象编程最终的核心仍然是去使用对象。 三，面向对象编程3.1 类的定义与实例化12345678910111213141516171819202122232425# 先定义类# 类是对象相似数据与功能的集合体。所以类体中最常见的是变量与函数的定义，但是类体其实是可以包含任意代码的# 注意：类体代码在定义阶段就会立即执行，会产生类的名称空间class Student: # 1、变量的定义 stu_school = &#x27;oldboy&#x27; # 2、功能的定义 def tell_stu_info(stu_obj): print(&#x27;学生信息：名字：%s 年龄：%s 性别：%s&#x27; %( stu_obj[&#x27;stu_name&#x27;], stu_obj[&#x27;stu_age&#x27;], stu_obj[&#x27;stu_gender&#x27;] )) def set_info(stu_obj,x,y,z): stu_obj[&#x27;stu_name&#x27;] = x stu_obj[&#x27;stu_age&#x27;] = y stu_obj[&#x27;stu_gender&#x27;] = z print(&#x27;==========&gt;&#x27;)print(Student.__dict__)print(Student.__dict__[&#x27;stu_school&#x27;]) # oldboyprint(Student.__dict__[&#x27;set_info&#x27;]) # &lt;function Student.set_info at 0x100ee3e50&gt; 3.2 属性访问3.2.1 类属性与对象属性123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# 属性访问的语法# 1、访问数据属性print(Student.stu_school) # print(Student.__dict__[&#x27;stu_school&#x27;]) # oldboy# 2、访问函数属性print(Student.set_info) # print(Student.__dict__[&#x27;set_info&#x27;]) # &lt;function Student.set_info at 0x100ee3e50&gt;# 再调用类产生对象stu1_obj = Student() # 实例化对象stu2_obj = Student()stu3_obj = Student()print(stu1_obj.__dict__) # &#123;&#125;print(stu2_obj.__dict__) # &#123;&#125;print(stu3_obj.__dict__) # &#123;&#125;# 为对象定制自己的独有属性# 问题1：代码重复# 问题2：属性的查找顺序stu1_obj.stu_name = &#x27;zhangsan&#x27; # stu1_obj.__dict__[&#x27;stu_name&#x27;] = &#x27;zhangsan&#x27;stu1_obj.stu_age = 18 # stu1_obj.__dict__[&#x27;stu_age&#x27;] = 18stu1_obj.stu_gender = &#x27;male&#x27; # stu1_obj.__dict__[&#x27;stu_gender&#x27;] = &#x27;male&#x27;print(stu1_obj.__dict__) # &#123;&#x27;stu_name&#x27;: &#x27;zhangsan&#x27;, &#x27;stu_age&#x27;: 18, &#x27;stu_gender&#x27;: &#x27;male&#x27;&#125;# stu2_obj.stu_name = &#x27;lisi&#x27;# stu2_obj.stu_age = 38# stu2_obj.stu_gender = &#x27;male&#x27;# print(stu2_obj.__dict__)## stu3_obj.stu_name = &#x27;wangwu&#x27;# stu3_obj.stu_age = 20# stu3_obj.stu_gender = &#x27;female&#x27;# print(stu3_obj.__dict__)# 解决问题1：# 解决方案1：def init(obj,x,y,z): obj.stu_name = x obj.stu_age = y obj.stu_gender = zinit(stu1_obj,&#x27;zhangsan&#x27;,18,&#x27;male&#x27;)init(stu2_obj,&#x27;lisi&#x27;,38,&#x27;male&#x27;)init(stu3_obj,&#x27;wangwu&#x27;,20,&#x27;female&#x27;)# 解决方案2：# 一，先定义类class Student: # 1、变量的定义 stu_school = &#x27;oldboy&#x27; # 空对象,&#x27;zhangsan&#x27;,18,&#x27;male&#x27; def __init__(obj, x, y, z): obj.stu_name = x # 空对象.stu_name = &#x27;zhangsan&#x27; obj.stu_age = y # 空对象.stu_age = 18 obj.stu_gender = z # 空对象.stu_gender = &#x27;male&#x27; # 2、功能的定义 def tell_stu_info(stu_obj): print(&#x27;学生信息：名字：%s 年龄：%s 性别：%s&#x27; %( stu_obj[&#x27;stu_name&#x27;], stu_obj[&#x27;stu_age&#x27;], stu_obj[&#x27;stu_gender&#x27;] )) def set_info(stu_obj,x,y,z): stu_obj[&#x27;stu_name&#x27;] = x stu_obj[&#x27;stu_age&#x27;] = y stu_obj[&#x27;stu_gender&#x27;] = z# 二，再调用类产生对象# 调用类的过程又称之为实例化，发生了三件事# 1、先产生一个空对象# 2、python会自动调用类中的__init__方法，然后将空对象以及调用类时括号内传入的参数一同传给__init__方法# 3、返回初始化好的对象stu1_obj = Student(&#x27;zhangsan&#x27;,18,&#x27;male&#x27;) # Student.__init__(空对象,&#x27;zhangsan&#x27;,18,&#x27;male&#x27;)stu2_obj = Student(&#x27;lisi&#x27;,38,&#x27;male&#x27;)stu3_obj = Student(&#x27;wangwu&#x27;,20,&#x27;female&#x27;)print(stu1_obj.__dict__) # &#123;&#x27;stu_name&#x27;: &#x27;zhangsan&#x27;, &#x27;stu_age&#x27;: 18, &#x27;stu_gender&#x27;: &#x27;male&#x27;&#125;# 总结__init__方法# 1、会在调用类时自动触发执行，用来为对象初始化自己独有的数据。# 2、__init__内应该存放的是为对象初始化属性的功能（共有属性），但是是可以存放任意其他代码，想要在类调用时就立刻执行的代码都可以放到该方法内。# 3、__init__方法必须返回None 3.2.2 属性查找顺序与绑定方法对象的名称空间里只存放着对象独有的属性，而对象们相似的属性是存放于类中的。对象在访问属性时，会优先从对象本身的__dict__中查找，未找到，则去类的__dict__中查找 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081class Student: # 1、变量的定义 stu_school = &#x27;oldboy&#x27; count = 0 # 空对象,&#x27;zhangsan&#x27;,18,&#x27;male&#x27; def __init__(self, x, y, z): Student.count += 1 self.stu_name = x # 空对象.stu_name = &#x27;zhangsan&#x27; self.stu_age = y # 空对象.stu_age = 18 self.stu_gender = z # 空对象.stu_gender = &#x27;male&#x27; # 2、功能的定义 def tell_stu_info(self): print(&#x27;学生信息：名字：%s 年龄：%s 性别：%s&#x27; %( self.stu_name, self.stu_age, self.stu_gender )) def set_info(self,x,y,z): self.stu_name = x self.stu_age = y self.stu_gender = zstu1_obj = Student(&#x27;zhangsan&#x27;,18,&#x27;male&#x27;) # Student.__init__(空对象,&#x27;zhangsan&#x27;,18,&#x27;male&#x27;)stu2_obj = Student(&#x27;lisi&#x27;,38,&#x27;male&#x27;)stu3_obj = Student(&#x27;wangwu&#x27;,20,&#x27;female&#x27;)print(stu1_obj.count) # 3print(stu2_obj.count) # 3print(stu3_obj.count) # 3# 类中存放的是对象共有的数据与功能# 一、类可以访问：# 1、类的数据属性print(Student.stu_school)# 2、类的函数属性print(Student.tell_stu_info)print(Student.set_info)# 二、但其实类中的东西是给对象使用的# 1、类的数据属性是共享给所有对象用的，大家访问的地址都一样print(id(Student.stu_school)) # 4447792176print(id(stu1_obj.stu_school)) # 4447792176print(id(stu2_obj.stu_school)) # 4447792176print(id(stu3_obj.stu_school)) # 4447792176# Student.stu_school = &#x27;OLDBOY&#x27; # 类修改，影响所有对象# print(Student.stu_school) # OLDBOY# print(stu1_obj.stu_school) # OLDBOY# print(stu2_obj.stu_school) # OLDBOY# print(stu3_obj.stu_school) # OLDBOYstu1_obj.stu_school = &#x27;OLDBOY&#x27; # 对象修改，只影响对象本身print(Student.stu_school) # oldboyprint(stu1_obj.stu_school) # OLDBOYprint(stu2_obj.stu_school) # oldboyprint(stu3_obj.stu_school) # oldboy# 2、类中定义的函数主要是给对象使用的，而且是绑定给对象的，虽然所有对象指向的都是相同的功能，但是绑定到不同的对象就是不同的绑定方法，内存地址各不相同# 类调用自己的函数属性必须严格按照函数的用法来Student.tell_stu_info(stu1_obj) # 学生信息：名字：zhangsan 年龄：18 性别：maleStudent.tell_stu_info(stu2_obj) # 学生信息：名字：lisi 年龄：38 性别：maleStudent.tell_stu_info(stu3_obj) # 学生信息：名字：wangwu 年龄：20 性别：femaleStudent.set_info(stu1_obj,&#x27;ZHANGSAN&#x27;,19,&#x27;MALE&#x27;)Student.tell_stu_info(stu1_obj) # 学生信息：名字：ZHANGSAN 年龄：19 性别：MALE# 绑定到对象的方法特殊之处在于，绑定给谁就应该由谁来调用，谁来调用，就会将’谁’本身当做第一个参数自动传入（方法__init__也是一样的道理）print(Student.tell_stu_info) # &lt;function Student.tell_stu_info at 0x106297e50&gt;print(stu1_obj.tell_stu_info) # &lt;bound method Student.tell_stu_info of &lt;__main__.Student object at 0x1086edfd0&gt;&gt;print(stu2_obj.tell_stu_info) # &lt;bound method Student.tell_stu_info of &lt;__main__.Student object at 0x1027c4f70&gt;&gt;print(stu3_obj.tell_stu_info) # &lt;bound method Student.tell_stu_info of &lt;__main__.Student object at 0x10421fee0&gt;&gt;stu1_obj.tell_stu_info() # 等同于 Student.tell_stu_info(stu1_obj) 3.3.3 小结在上述介绍类与对象的使用过程中，我们更多的是站在底层原理的角度去介绍类与对象之间的关联关系，如果只是站在使用的角度，我们无需考虑语法“对象.属性&quot;中”属性“到底源自于哪里，只需要知道是通过对象获取到的就可以了，所以说，对象是一个高度整合的产物，有了对象，我们只需要使用”对象.xxx“的语法就可以得到跟这个对象相关的所有数据与功能，十分方便且解耦合程度极高。","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"常用模块","slug":"changyongmokuai","date":"2023-04-23T06:22:29.000Z","updated":"2023-04-23T06:22:29.000Z","comments":true,"path":"articles/2023/04/23/changyongmokuai/","permalink":"https://kkabuzs.github.io/articles/2023/04/23/changyongmokuai/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 23、常用模块特别鸣谢： — b站老男孩agen老师 一，时间模块time123456789101112131415161718192021# 一：timeimport time# 时间分为三种格式：# 1、时间戳：从1970年到现在经过的秒数# 作用：用于时间间隔的计算print(time.time())# 2、按照某种格式显示的时间：2023-04-19 17:36:33# 作用：用于展示时间# 年 月 日 时 分 秒 上下午print(time.strftime(&#x27;%Y-%m-%d %H:%M:%S %p&#x27;))print(time.strftime(&#x27;%Y-%m-%d %X&#x27;)) # X：直接表示分时秒的时间# 3、结构化时间(struct_time)# 作用：用于单独获取时间的某一部分res = time.localtime()print(res)print(res.tm_year) # 获取年份print(res.tm_mon) # 获取月份 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 二：datetimeimport datetimeprint(datetime.datetime.now()) # 2023-04-19 17:48:20.419771# 时间，计算三天后的时间print(datetime.datetime.now() + datetime.timedelta(days=3)) # 2023-04-22 17:48:20.419771# 时间，计算三天前的时间print(datetime.datetime.now() + datetime.timedelta(days=-3))# 时间模块需要掌握的操作# 1、时间格式的转换import time# 结构化时间转成时间戳s_time = time.localtime()print(time.mktime(s_time))# 时间戳转成结构化时间tp_time = time.time()print(time.localtime(tp_time))# 补充：世界标准时间与本地时间：区别只有小时的时间print(time.localtime()) # 东八区时间print(time.gmtime()) # 世界标准时间，和我们有八个小时时差，需要加8才是我们的小时# 结构化时间转成格式化字符串形式的时间s_time = time.localtime()print(time.strftime(&#x27;%Y-%m-%d %H:%M:%S&#x27;,s_time))print(time.strptime(&#x27;2023-04-19 18:02:49&#x27;,&#x27;%Y-%m-%d %H:%M:%S&#x27;))# 真正需要掌握的只有一条：format string(格式化字符串形式的时间) &lt;-----&gt; timestamp(时间戳)# &#x27;2023-04-19 18:02:49&#x27;+7# format string(格式化字符串形式的时间） ---&gt; struct_time(结构化时间) ---&gt; timestamp(时间戳)struct_time = time.strptime(&#x27;2023-04-19 18:02:49&#x27;,&#x27;%Y-%m-%d %H:%M:%S&#x27;)timestamp=time.mktime(struct_time)+7*86400print(timestamp)# format string(格式化字符串形式的时间） &lt;--- struct_time(结构化时间) &lt;--- timestamp(时间戳)format_time = time.strftime(&#x27;%Y-%m-%d %X&#x27;,time.localtime(timestamp))print(format_time) # 2023-04-26 18:02:49time.sleep(3) # 睡眠三秒# 了解知识print(time.asctime()) # Thu Apr 20 10:48:09 2023import datetimeprint(datetime.datetime.now()) #返回 2023-04-20 10:50:10.456470print(datetime.date.fromtimestamp(time.time()) ) # 时间戳直接转成日期格式 2023-04-20print(datetime.datetime.now() )print(datetime.datetime.now() + datetime.timedelta(3)) #当前时间+3天print(datetime.datetime.now() + datetime.timedelta(-3)) #当前时间-3天print(datetime.datetime.now() + datetime.timedelta(hours=3)) #当前时间+3小时print(datetime.datetime.now() + datetime.timedelta(minutes=30)) #当前时间+30分 二，random模块 随机模块，取随机值 1234567891011121314151617181920212223242526import randomprint(random.random()) # (0,1)----float 大于0且小于1之间的小数print(random.randint(1,3)) # [1,3] 大于等于1且小于等于3之间的整数print(random.randrange(1,3)) # [1,3) 大于等于1且小于3之间的整数print(random.choice([1,&#x27;23&#x27;,[4,5]])) # 1或者23或者[4,5]print(random.sample([1,&#x27;23&#x27;,[4,5]],2)) # 列表元素任意2个组合print(random.uniform(1,3)) # 大于1小于3的小数，如1.927109612082716item=[1,3,5,7,9]random.shuffle(item) # 打乱item的顺序,相当于&quot;洗牌&quot;print(item)# 随机验证码import randomdef make_code(size=4): res = &#x27;&#x27; for i in range(size): zs_letter = chr(random.randint(65,90)) # chr方法是将数字转成字符编码表里面对应的字符，反之使用ord方法 zs_digit = str(random.randint(0,9)) zs_character = random.choice([zs_letter,zs_digit]) res += zs_character return resprint(make_code(4)) 三，os模块 os模块是与操作系统交互的一个接口 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import os# os.getcwd() # 获取当前工作目录，即当前python脚本工作的目录路径# os.chdir(&quot;dirname&quot;) # 改变当前脚本工作目录；相当于shell下cd# os.curdir # 返回当前目录: (&#x27;.&#x27;)# os.pardir # 获取当前目录的父目录字符串名：(&#x27;..&#x27;)# os.makedirs(&#x27;dirname1/dirname2&#x27;) # 可生成多层递归目录# os.removedirs(&#x27;dirname1&#x27;) # 若目录为空，则删除，并递归到上一级目录，如若也为空，则删除，依此类推# os.mkdir(&#x27;dirname&#x27;) # 生成单级目录；相当于shell中mkdir dirname# os.rmdir(&#x27;dirname&#x27;) # 删除单级空目录，若目录不为空则无法删除，报错；相当于shell中rmdir dirname# os.listdir(&#x27;dirname&#x27;) # 列出指定目录下的所有文件和子目录，包括隐藏文件，并以列表方式打印# os.remove() # 删除一个文件# os.rename(&quot;oldname&quot;,&quot;newname&quot;) # 重命名文件/目录# os.stat(&#x27;path/filename&#x27;) # 获取文件/目录信息# os.sep # 输出操作系统特定的路径分隔符，win下为&quot;\\\\&quot;,Linux下为&quot;/&quot;# os.linesep # 输出当前平台使用的行终止符，win下为&quot;\\r\\n&quot;,Linux下为&quot;\\n&quot;# os.pathsep # 输出用于分割文件路径的字符串 win下为;,Linux下为:# os.name # 输出字符串指示当前使用平台。win-&gt;&#x27;nt&#x27;; Linux-&gt;&#x27;posix&#x27;# os.system(&quot;bash command&quot;) # 运行shell命令，直接显示# os.environ # 获取系统环境变量# os.path.abspath(path) # 返回path规范化的绝对路径# os.path.split(path) # 将path分割成目录和文件名二元组返回# os.path.dirname(path) # 返回path的目录。其实就是os.path.split(path)的第一个元素# os.path.basename(path) # 返回path最后的文件名。如何path以／或\\结尾，那么就会返回空值。即os.path.split(path)的第二个元素# os.path.exists(path) # 如果path存在，返回True；如果path不存在，返回False# os.path.isabs(path) # 如果path是绝对路径，返回True# os.path.isfile(path) # 如果path是一个存在的文件，返回True。否则返回False# os.path.isdir(path) # 如果path是一个存在的目录，则返回True。否则返回False# os.path.join(path1[, path2[, ...]]) # 将多个路径组合后返回，第一个绝对路径之前的参数将被忽略# os.path.getatime(path) # 返回path所指向的文件或者目录的最后存取时间# os.path.getmtime(path) # 返回path所指向的文件或者目录的最后修改时间# os.path.getsize(path) # 返回path的大小# 获取某一个文件夹下所有的子文件及子文件夹的名字import osres=os.listdir(&#x27;/Users/zhaoshuo/PycharmProjects/pythonProject/Python全栈_b站_老男孩/day22&#x27;)print(res) # [&#x27;03os模块.py&#x27;, &#x27;02random模块.py&#x27;, &#x27;01时间模块.py&#x27;, &#x27;day22笔记.md&#x27;]# 统计文件大小size = os.path.getsize(&#x27;/Users/zhaoshuo/PycharmProjects/pythonProject/Python全栈_b站_老男孩/day22/03os模块.py&#x27;) # 显示的是字节print(size)# os.environ # 获取系统环境变量# 规定：key与value必须都为字符串os.environ[&#x27;aaaaaaaaa&#x27;]=&#x27;1111&#x27; # 添加一个环境变量print(os.environ)# os.path.abspath(path) # 返回path规范化的绝对路径print(os.path.abspath(__file__))# os.path.split(path) # 将path分割成目录和文件名二元组返回print(os.path.split(r&#x27;/a/b/c/d.txt&#x27;)) # (&#x27;/a/b/c&#x27;, &#x27;d.txt&#x27;)# os.path.dirname(path) # 返回path的目录。其实就是os.path.split(path)的第一个元素print(os.path.dirname(r&#x27;/a/b/c/d.txt&#x27;)) # /a/b/c# os.path.basename(path) # 返回path最后的文件名。如何path以／或\\结尾，那么就会返回空值。即os.path.split(path)的第二个元素print(os.path.basename(r&#x27;/a/b/c/d.txt&#x27;)) # d.txt# os.path.join(path1[, path2[, ...]]) # 将多个路径组合后返回，第一个绝对路径之前的参数将被忽略print(os.path.join(&#x27;a&#x27;,&#x27;/&#x27;,&#x27;b&#x27;,&#x27;c&#x27;)) # /b/c# 取上上一级目录import osBASE_DIR=os.path.dirname(os.path.dirname(__file__))print(BASE_DIR) # /Users/zhaoshuo/PycharmProjects/pythonProject/Python全栈_b站_老男孩# 在python3.5之后，推出了一个新的模块pathlibfrom pathlib import Pathres = Path(__file__).parent.parent # parent代表取上一级，两个就代表取两层print(res) # /Users/zhaoshuo/PycharmProjects/pythonProject/Python全栈_b站_老男孩res1 = Path(&#x27;/a/b/c&#x27;) / &#x27;d/f.txt&#x27;print(res1) # /a/b/c/d/f.txt 四，sys模块1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# sys模块import sys# sys.argv # 命令行参数List，第一个元素是程序本身路径# sys.exit(n) # 退出程序，正常退出时exit(0)# sys.version # 获取Python解释程序的版本信息# sys.maxint # 最大的Int值# sys.path # 返回模块的搜索路径，初始化时使用PYTHONPATH环境变量的值# sys.platform # 返回操作系统平台名称# python3 04sys模块.py 1 2 3# sys.argv获取的是解释器后的参数值print(sys.argv) #[&#x27;04sys模块.py&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;]# 拷贝文件程序优化：实现linux上cp命令的效果src_file = sys.argv[1]dst_file = sys.argv[2]# 可以加判断，继续完善with open(r&#x27;%s&#x27; %src_file,mode=&#x27;rb&#x27;) as src_f, \\ open(r&#x27;%s&#x27; %dst_file,mode=&#x27;wb&#x27;) as dst_f: for line in src_f: dst_f.write(line)# 打印进度条案例&quot;&quot;&quot;知识储备：#=========知识储备==========#进度条的效果[# ][## ][### ][#### ]#指定宽度print(&#x27;[%-50s]&#x27; %&#x27;#&#x27;)print(&#x27;[%-50s]&#x27; %&#x27;##&#x27;)print(&#x27;[%-50s]&#x27; %&#x27;###&#x27;)print(&#x27;[%-50s]&#x27; %&#x27;####&#x27;)# 解析：[%-15s]：%s代表格式化字符串，50代表总宽度，-代表左对齐# 打印import timeres = &#x27;&#x27;for i in range(50): res += &#x27;#&#x27; time.sleep(0.5) print(&#x27;\\r[%-50s]&#x27; % res,end=&#x27;&#x27;) # end=&#x27;&#x27;代表不换行输出，\\r代表都输出在左侧，下一次输出覆盖上一次结果，实现进度条效果&quot;&quot;&quot;# 模拟下载，打印进度条import timedef progress(percent): if percent &gt;= 1: # 判断的目的是，如果大于一，返回percent=1，不加判断就会出现 101% 超出百分百的情况 percent = 1 res = int(50 * percent) * &#x27;#&#x27; print(&#x27;\\r[%-50s] %d%%&#x27; % (res,int(100*percent)), end=&#x27;&#x27;) # %d%%代表的是 %d 传个整数，%%代表百分号，两个%代表转义，取消特殊意思，就显示%recv_size = 0total_size = 333354while recv_size &lt; total_size: time.sleep(0.01) recv_size+=1024 # 打印进度条 percent = recv_size / total_size progress(percent)# 宽度写活示例import sysimport timedef progress(percent,width=50): if percent &gt;= 1: percent=1 show_str=(&#x27;[%%-%ds]&#x27; %width) %(int(width*percent)*&#x27;#&#x27;) print(&#x27;\\r%s %d%%&#x27; %(show_str,int(100*percent)),end=&#x27;&#x27;)data_size=10250recv_size=0while recv_size &lt; data_size: time.sleep(0.1) #模拟数据的传输延迟 recv_size+=1024 #每次收1024 percent=recv_size/data_size #接收的比例 progress(percent,width=30) #进度条的宽度70 五，shutil模块 高级的 文件、文件夹、压缩包 处理模块 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485import shutil# 将文件内容拷贝到另一个文件中# shutil.copyfileobj(fsrc, fdst[, length])shutil.copyfileobj(open(&#x27;old.xml&#x27;,&#x27;r&#x27;), open(&#x27;new.xml&#x27;, &#x27;w&#x27;))# 拷贝文件# shutil.copyfile(src, dst)shutil.copyfile(&#x27;f1.log&#x27;, &#x27;f2.log&#x27;) #目标文件无需存在# 仅拷贝权限。内容、组、用户均不变# shutil.copymode(src, dst)shutil.copymode(&#x27;f1.log&#x27;, &#x27;f2.log&#x27;) #目标文件必须存在# 仅拷贝状态的信息，包括：mode bits, atime, mtime, flags# shutil.copystat(src, dst)shutil.copystat(&#x27;f1.log&#x27;, &#x27;f2.log&#x27;) #目标文件必须存在# 拷贝文件和权限（使用比较多）# shutil.copy(src, dst)shutil.copy(&#x27;f1.log&#x27;, &#x27;f2.log&#x27;)# 拷贝文件和状态信息# shutil.copy2(src, dst)shutil.copy2(&#x27;f1.log&#x27;, &#x27;f2.log&#x27;)# 递归的去拷贝文件夹# shutil.ignore_patterns(*patterns)# shutil.copytree(src, dst, symlinks=False, ignore=None)shutil.copytree(&#x27;folder1&#x27;, &#x27;folder2&#x27;, ignore=shutil.ignore_patterns(&#x27;*.pyc&#x27;, &#x27;tmp*&#x27;)) #目标目录不能存在，注意对folder2目录父级目录要有可写权限，ignore的意思是排除# 拷贝软连接：通常的拷贝都把软连接拷贝成硬链接，即对待软连接来说，创建新的文件shutil.copytree(&#x27;f1&#x27;, &#x27;f2&#x27;, symlinks=True, ignore=shutil.ignore_patterns(&#x27;*.pyc&#x27;, &#x27;tmp*&#x27;))# 递归的去删除文件# shutil.rmtree(path[, ignore_errors[, onerror]])shutil.rmtree(&#x27;folder1&#x27;)# 递归的去移动文件，它类似mv命令，其实就是重命名# shutil.move(src, dst)shutil.move(&#x27;folder1&#x27;, &#x27;folder3&#x27;)&#x27;&#x27;&#x27;shutil.make_archive(base_name, format,...)创建压缩包并返回文件路径，例如：zip、tar创建压缩包并返回文件路径，例如：zip、tar base_name： 压缩包的文件名，也可以是压缩包的路径。只是文件名时，则保存至当前目录，否则保存至指定路径， 如 data_bak =&gt;保存至当前路径 如：/tmp/data_bak =&gt;保存至/tmp/ format： 压缩包种类，“zip”, “tar”, “bztar”，“gztar” root_dir： 要压缩的文件夹路径（默认当前目录） owner： 用户，默认当前用户 group： 组，默认当前组 logger： 用于记录日志，通常是logging.Logger对象&#x27;&#x27;&#x27;#将 /data 下的文件打包放置当前程序目录import shutilret = shutil.make_archive(&quot;data_bak&quot;, &#x27;gztar&#x27;, root_dir=&#x27;/data&#x27;)#将 /data下的文件打包放置 /tmp/目录import shutilret1 = shutil.make_archive(&quot;/tmp/data_bak&quot;, &#x27;gztar&#x27;, root_dir=&#x27;/data&#x27;)# zipfile压缩解压缩import zipfile# 压缩z = zipfile.ZipFile(&#x27;laxi.zip&#x27;, &#x27;w&#x27;)z.write(&#x27;a.log&#x27;)z.write(&#x27;data.data&#x27;)z.close()# 解压z = zipfile.ZipFile(&#x27;laxi.zip&#x27;, &#x27;r&#x27;)z.extractall(path=&#x27;.&#x27;)z.close() 六，json与pickle模块6.1什么是序列化与反序列化 指的是把内存的数据类型转换成一个特定格式的内容 内存中的数据类型 ----&gt; 序列化 ----&gt; 特定的格式（json格式或者pickle格式） 内存中的数据类型 &lt;---- 反序列化 &lt;---- 特定的格式（json格式或者pickle格式） 6.2 为何要序列化 序列化得到结果&#x3D;&gt;特定格式的内容（有两种用途） 1、可用于存储 =&gt; 用于存档 2、传输给其他平台使用 =&gt; 跨平台数据交互 python java 列表 特定的格式 数组 强调: 针对用途1的特定格式：可以是一种专用的格式 &#x3D;&gt; pickle（只有python可以识别） 针对用途2的特定格式：应该是一种通用，能够被所有语言识别的格式 &#x3D;&gt; json 6.3 如何序列化与反序列化12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# 示范1：import json# 序列化json_res = json.dumps([1,&#x27;aaaa&#x27;,False,True])print(json_res,type(json_res)) # [1, &quot;aaaa&quot;, false, true] &lt;class &#x27;str&#x27;&gt;# 反序列化l = json.loads(json_res)print(l,type(l)) # [1, &#x27;aaaa&#x27;, False, True] &lt;class &#x27;list&#x27;&gt;# 示范2：import json# 序列化test_json = json.dumps([1,&#x27;aaaa&#x27;,False,True])with open(r&#x27;test.json&#x27;,mode=&#x27;wt&#x27;,encoding=&#x27;utf-8&#x27;) as f: f.write(test_json)# 将序列化的结果写入文件的简单方法with open(r&#x27;test.json&#x27;,mode=&#x27;wt&#x27;,encoding=&#x27;utf-8&#x27;) as f: json.dump([1,&#x27;aaaa&#x27;,False,True],f)# 反序列化with open(r&#x27;test.json&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f1: test_json_load = f1.read() l = json.loads(test_json_load) print(l,type(l))# 反序列化的简单方法with open(r&#x27;test.json&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f1: l = json.load(f1) print(l,type(l))# json验证：json格式兼容的是所有语言通用的数据类型，不能支持某一语言独有的数据类型# json.dumps(&#123;1,3,4,5&#125;) # 报错，集合不能被json序列化# json强调：一定要搞清楚json格式，json没有单引号，不要与python混淆l = json.loads(&#x27;[1, &quot;aaaa&quot;, false, true]&#x27;)# pickle模块(和json操作一摸一样)import pickle# 序列化pickle_res = pickle.dumps(&#123;1,2,3,4,5,6&#125;)print(pickle_res,type(pickle_res))# 反序列化p = pickle.loads(pickle_res)print(p,type(p)) 6.4 猴子补丁6.4.1 什么是猴子补丁 属性在运行时的动态替换，叫做猴子补丁（Monkey Patch）。 猴子补丁的核心就是用自己的代码替换所用模块的源代码，详细地如下 1， 这个词原来为Guerrilla Patch，杂牌军、游击队，说明这部分不是原装的，在英文里guerilla发音和gorllia(猩猩)相似，再后来就写了monkey(猴子)。 2， 还有一种解释是说由于这种方式将原来的代码弄乱了(messing with it)，在英文里叫monkeying about(顽皮的)，所以叫做Monkey Patch。 6.4.2 猴子补丁的功能(一切皆对象) 拥有在模块运行时替换的功能, 例如: 一个函数对象赋值给另外一个函数对象(把函数原本的执行的功能给替换了) 1234567891011121314151617181920212223242526&#x27;&#x27;&#x27;猴子补丁的应用场景：如果我们的程序中已经基于json模块编写了大量代码了，发现有一个模块ujson比它性能更高，但用法一样，我们肯定不会想所有的代码都换成ujson.dumps或者ujson.loads,那我们可能会想到这么做import ujson as json，但是这么做的需要每个文件都重新导入一下，维护成本依然很高此时我们就可以用到猴子补丁了只需要在入口处加上：&#x27;&#x27;&#x27;import jsonimport ujsondef monkey_patch_json(): json.__name__ = &#x27;ujson&#x27; json.dumps = ujson.dumps json.loads = ujson.loadsmonkey_patch_json() # 之所以在入口处加，是因为模块在导入一次后，后续的导入便直接引用第一次的成果&#x27;&#x27;&#x27;#其实这种场景也比较多, 比如我们引用团队通用库里的一个模块, 又想丰富模块的功能, 除了继承之外也可以考虑用MonkeyPatch.采用猴子补丁之后，如果发现ujson不符合预期，那也可以快速撤掉补丁。个人感觉MonkeyPatch带了便利的同时也有搞乱源代码的风险!&#x27;&#x27;&#x27; 七，shelve与xml模块(了解)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# shelve模块比pickle模块简单，只有一个open函数，返回类似字典的对象，可读可写;key必须为字符串，而值可以是python所支持的数据类型import shelvef=shelve.open(r&#x27;sheve.txt&#x27;)# f[&#x27;stu1_info&#x27;]=&#123;&#x27;name&#x27;:&#x27;egon&#x27;,&#x27;age&#x27;:18,&#x27;hobby&#x27;:[&#x27;piao&#x27;,&#x27;smoking&#x27;,&#x27;drinking&#x27;]&#125;# f[&#x27;stu2_info&#x27;]=&#123;&#x27;name&#x27;:&#x27;gangdan&#x27;,&#x27;age&#x27;:53&#125;# f[&#x27;school_info&#x27;]=&#123;&#x27;website&#x27;:&#x27;http://www.pypy.org&#x27;,&#x27;city&#x27;:&#x27;beijing&#x27;&#125;print(f[&#x27;stu1_info&#x27;][&#x27;hobby&#x27;])f.close()# xml是实现不同语言或程序之间进行数据交换的协议，跟json差不多，但json使用起来更简单，不过，古时候，在json还没诞生的黑暗年代，大家只能选择用xml呀，至今很多传统公司如金融行业的很多系统的接口还主要是xml。&#x27;&#x27;&#x27;xml的格式如下，就是通过&lt;&gt;节点来区别数据结构的:&lt;?xml version=&quot;1.0&quot;?&gt;&lt;data&gt; &lt;country name=&quot;Liechtenstein&quot;&gt; &lt;rank updated=&quot;yes&quot;&gt;2&lt;/rank&gt; &lt;year&gt;2008&lt;/year&gt; &lt;gdppc&gt;141100&lt;/gdppc&gt; &lt;neighbor name=&quot;Austria&quot; direction=&quot;E&quot;/&gt; &lt;neighbor name=&quot;Switzerland&quot; direction=&quot;W&quot;/&gt; &lt;/country&gt; &lt;country name=&quot;Singapore&quot;&gt; &lt;rank updated=&quot;yes&quot;&gt;5&lt;/rank&gt; &lt;year&gt;2011&lt;/year&gt; &lt;gdppc&gt;59900&lt;/gdppc&gt; &lt;neighbor name=&quot;Malaysia&quot; direction=&quot;N&quot;/&gt; &lt;/country&gt; &lt;country name=&quot;Panama&quot;&gt; &lt;rank updated=&quot;yes&quot;&gt;69&lt;/rank&gt; &lt;year&gt;2011&lt;/year&gt; &lt;gdppc&gt;13600&lt;/gdppc&gt; &lt;neighbor name=&quot;Costa Rica&quot; direction=&quot;W&quot;/&gt; &lt;neighbor name=&quot;Colombia&quot; direction=&quot;E&quot;/&gt; &lt;/country&gt;&lt;/data&gt;&#x27;&#x27;&#x27;import xml.etree.ElementTree as ETtree = ET.parse(&quot;xmltest.xml&quot;)root = tree.getroot()print(root.tag)# 遍历xml文档for child in root: print(&#x27;========&gt;&#x27;, child.tag, child.attrib, child.attrib[&#x27;name&#x27;]) for i in child: print(i.tag, i.attrib, i.text)# 只遍历year 节点for node in root.iter(&#x27;year&#x27;): print(node.tag, node.text)# ---------------------------------------import xml.etree.ElementTree as ETtree = ET.parse(&quot;xmltest.xml&quot;)root = tree.getroot()# 修改for node in root.iter(&#x27;year&#x27;): new_year = int(node.text) + 1 node.text = str(new_year) node.set(&#x27;updated&#x27;, &#x27;yes&#x27;) node.set(&#x27;version&#x27;, &#x27;1.0&#x27;)tree.write(&#x27;test.xml&#x27;)# 删除nodefor country in root.findall(&#x27;country&#x27;): rank = int(country.find(&#x27;rank&#x27;).text) if rank &gt; 50: root.remove(country)tree.write(&#x27;output.xml&#x27;) 八，configparser模块 配置文件模块 1234567891011121314151617181920212223242526272829303132333435363738import configparserconfig = configparser.ConfigParser()config.read(&#x27;conf.ini&#x27;)# 1、获取sectionsprint(config.sections()) # [&#x27;section1&#x27;, &#x27;section2&#x27;]# 2、获取某一section下的所有optionsprint(config.options(&#x27;section1&#x27;))&#x27;&#x27;&#x27;[&#x27;k1&#x27;, &#x27;k2&#x27;, &#x27;user&#x27;, &#x27;age&#x27;, &#x27;is_admin&#x27;, &#x27;salary&#x27;]&#x27;&#x27;&#x27;# 3、获取itemsprint(config.items(&#x27;section1&#x27;))&#x27;&#x27;&#x27;[(&#x27;k1&#x27;, &#x27;v1&#x27;), (&#x27;k2&#x27;, &#x27;v2&#x27;), (&#x27;user&#x27;, &#x27;zhangsan&#x27;), (&#x27;age&#x27;, &#x27;18&#x27;), (&#x27;is_admin&#x27;, &#x27;true&#x27;), (&#x27;salary&#x27;, &#x27;31&#x27;)]&#x27;&#x27;&#x27;# 查看标题section1下user的值=&gt;字符串格式res = config.get(&#x27;section1&#x27;,&#x27;user&#x27;)print(res) # zhangsan# 查看标题section1下age的值=&gt;整数格式res = config.getint(&#x27;section1&#x27;,&#x27;age&#x27;)print(res,type(res)) # 18 &lt;class &#x27;int&#x27;&gt;# 查看标题section1下is_admin的值=&gt;布尔值格式res = config.getboolean(&#x27;section1&#x27;,&#x27;is_admin&#x27;)print(res,type(res)) # True &lt;class &#x27;bool&#x27;&gt;# 查看标题section1下salary的值=&gt;浮点型格式res = config.getfloat(&#x27;section1&#x27;,&#x27;salary&#x27;)print(res,type(res)) # 31.0 &lt;class &#x27;float&#x27;&gt; 九，hashlib模块9.1 什么是哈希（hash） hash是一类算法，该算法接受传入的内容，经过运算得到一串hash值 hash算法就像一座工厂，工厂接收你送来的原材料（可以用m.update()为工厂运送原材料），经过加工返回的产品就是hash值 9.2 hash值的特点 只要传入的内容一样，得到的hash值必然一样 不能由hash值反解成内容 只要使用的hash算法不变，无论校验的内容有多大，得到的hash值长度是固定的 9.3 hash的用途 用于密码密文传输与验证 用于文件的完整性校验 9.4 如何使用hash1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import hashlibm = hashlib.md5()m.update(&#x27;hello&#x27;.encode(&#x27;utf-8&#x27;))res = m.hexdigest()print(res) # 5d41402abc4b2a76b9719d911017c592m1 = hashlib.md5(&#x27;he&#x27;.encode(&#x27;utf-8&#x27;))m1.update(&#x27;l&#x27;.encode(&#x27;utf-8&#x27;))m1.update(&#x27;lo&#x27;.encode(&#x27;utf-8&#x27;))res1 = m1.hexdigest()print(res1) # 5d41402abc4b2a76b9719d911017c592# 模拟撞库import hashlibpasswds=[ &#x27;zhangsan3714&#x27;, &#x27;zhangsan1313&#x27;, &#x27;zhangsan94139413&#x27;, &#x27;zhangsan123456&#x27;, &#x27;123456zhangsan&#x27;, &#x27;zhang123san&#x27;, ]def make_passwd_dic(passwds): dic=&#123;&#125; for p in passwds: m=hashlib.md5() m.update(p.encode(&#x27;utf-8&#x27;)) dic[p]=m.hexdigest() return dicdef break_code(cryptograph,passwd_dic): for k,v in passwd_dic.items(): if v == cryptograph: print(&#x27;密码是===&gt;\\033[46m%s\\033[0m&#x27; %k)cryptograph=&#x27;443932f461d4357ee5ed615f0eebad29&#x27;break_code(cryptograph,make_passwd_dic(passwds))# 提升撞库的成本 =&gt; 密码加盐import hashlibmm = hashlib.md5()mm.update(&#x27;bongqiaqia&#x27;.encode(&#x27;utf-8&#x27;)) # 加的盐mm.update(&#x27;zhangsanchiregou&#x27;.encode(&#x27;utf-8&#x27;)) # 真实密码mm.update(&#x27;jionghaha&#x27;.encode(&#x27;utf-8&#x27;)) # 加的盐print(mm.hexdigest()) 十，subprocess模块 执行命令模块 1234567891011import subprocessobj = subprocess.Popen(&#x27;ls / ; ls /root&#x27;,shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, )res = obj.stdout.read()print(res.decode(&#x27;utf-8&#x27;))err_res = obj.stderr.read()print(err_res.decode(&#x27;utf-8&#x27;)) 十一，logging模块11.1 日志级别123456# 日志级别critical =&gt; 50error =&gt; 40warning =&gt; 30 # 默认级别是警告info =&gt; 20debug =&gt; 10 11.2 默认级别为warning，默认打印到终端12345678910111213import logginglogging.debug(&#x27;调试debug&#x27;)logging.info(&#x27;消息info&#x27;)logging.warning(&#x27;警告warn&#x27;)logging.error(&#x27;错误error&#x27;)logging.critical(&#x27;严重critical&#x27;)&#x27;&#x27;&#x27;WARNING:root:警告warnERROR:root:错误errorCRITICAL:root:严重critical&#x27;&#x27;&#x27; 11.3 为logging模块指定全局配置，针对所有logger有效，控制打印到文件中1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#======介绍可在logging.basicConfig()函数中可通过具体参数来更改logging模块默认行为，可用参数有filename：用指定的文件名创建FiledHandler（后边会具体讲解handler的概念），这样日志会被存储在指定的文件中。filemode：文件打开方式，在指定了filename时使用这个参数，默认值为“a”还可指定为“w”。format：指定handler使用的日志显示格式。datefmt：指定日期时间格式。level：设置rootlogger（后边会讲解具体概念）的日志级别stream：用指定的stream创建StreamHandler。可以指定输出到sys.stderr,sys.stdout或者文件，默认为sys.stderr。若同时列出了filename和stream两个参数，则stream参数会被忽略。format参数中可能用到的格式化串：%(name)s Logger的名字%(levelno)s 数字形式的日志级别%(levelname)s 文本形式的日志级别%(pathname)s 调用日志输出函数的模块的完整路径名，可能没有%(filename)s 调用日志输出函数的模块的文件名%(module)s 调用日志输出函数的模块名%(funcName)s 调用日志输出函数的函数名%(lineno)d 调用日志输出函数的语句所在的代码行%(created)f 当前时间，用UNIX标准的表示时间的浮 点数表示%(relativeCreated)d 输出日志信息时的，自Logger创建以 来的毫秒数%(asctime)s 字符串形式的当前时间。默认格式是 “2003-07-08 16:49:45,896”。逗号后面的是毫秒%(thread)d 线程ID。可能没有%(threadName)s 线程名。可能没有%(process)d 进程ID。可能没有%(message)s用户输出的消息#========使用import logginglogging.basicConfig(filename=&#x27;access.log&#x27;, format=&#x27;%(asctime)s - %(name)s - %(levelname)s -%(module)s: %(message)s&#x27;, datefmt=&#x27;%Y-%m-%d %H:%M:%S %p&#x27;, level=10)logging.debug(&#x27;调试debug&#x27;)logging.info(&#x27;消息info&#x27;)logging.warning(&#x27;警告warn&#x27;)logging.error(&#x27;错误error&#x27;)logging.critical(&#x27;严重critical&#x27;)#========结果access.log内容:2017-07-28 20:32:17 PM - root - DEBUG -test: 调试debug2017-07-28 20:32:17 PM - root - INFO -test: 消息info2017-07-28 20:32:17 PM - root - WARNING -test: 警告warn2017-07-28 20:32:17 PM - root - ERROR -test: 错误error2017-07-28 20:32:17 PM - root - CRITICAL -test: 严重criticalpart2: 可以为logging模块指定模块级的配置,即所有logger的配置 11.4 logging模块的Formatter，Handler，Logger，Filter对象 原理图 12345678#logger：产生日志的对象#Filter：过滤日志的对象#Handler：接收日志然后控制打印到不同的地方，FileHandler用来打印到文件中，StreamHandler用来打印到终端#Formatter对象：可以定制不同的日志格式对象，然后绑定给不同的Handler对象使用，以此来控制不同的Handler的日志格式 11.5 日志级别与配置1234567891011121314151617181920212223242526272829303132333435import logging# 1、日志基本配置logging.basicConfig( # 1、日志输出位置：1、终端 2、文件 # filename=&#x27;access.log&#x27;, # 不指定，默认打印到终端 # 2、日志格式 format=&#x27;%(asctime)s - %(name)s - %(levelname)s -%(module)s: %(message)s&#x27;, # 3、时间格式 datefmt=&#x27;%Y-%m-%d %H:%M:%S %p&#x27;, # 4、日志级别 # critical =&gt; 50 # error =&gt; 40 # warning =&gt; 30 # 默认级别是警告 # info =&gt; 20 # debug =&gt; 10 level=30,)# 输出日志logging.debug(&#x27;调试debug&#x27;)logging.info(&#x27;消息info&#x27;)logging.warning(&#x27;警告warn&#x27;)logging.error(&#x27;错误error&#x27;)logging.critical(&#x27;严重critical&#x27;)&#x27;&#x27;&#x27;# 注意下面的root是默认的日志名字WARNING:root:警告warnERROR:root:错误errorCRITICAL:root:严重critical&#x27;&#x27;&#x27; 11.6 日志字典123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111&quot;&quot;&quot;logging配置&quot;&quot;&quot;# 1、定义三种日志输出格式，日志中可能用到的格式化串如下# %(name)s Logger的名字# %(levelno)s 数字形式的日志级别# %(levelname)s 文本形式的日志级别# %(pathname)s 调用日志输出函数的模块的完整路径名，可能没有# %(filename)s 调用日志输出函数的模块的文件名# %(module)s 调用日志输出函数的模块名# %(funcName)s 调用日志输出函数的函数名# %(lineno)d 调用日志输出函数的语句所在的代码行# %(created)f 当前时间，用UNIX标准的表示时间的浮 点数表示# %(relativeCreated)d 输出日志信息时的，自Logger创建以 来的毫秒数# %(asctime)s 字符串形式的当前时间。默认格式是 “2003-07-08 16:49:45,896”。逗号后面的是毫秒# %(thread)d 线程ID。可能没有# %(threadName)s 线程名。可能没有# %(process)d 进程ID。可能没有# %(message)s用户输出的消息import os# 2、强调：其中的%(name)s为getlogger时指定的名字standard_format = &#x27;[%(asctime)s][%(threadName)s:%(thread)d][task_id:%(name)s][%(filename)s:%(lineno)d]&#x27; \\ &#x27;[%(levelname)s][%(message)s]&#x27;simple_format = &#x27;[%(levelname)s][%(asctime)s][%(filename)s:%(lineno)d]%(message)s&#x27;test_format = &#x27;[%(asctime)s] %(message)s&#x27;# 3、日志配置字典LOGGING_DIC = &#123; &#x27;version&#x27;: 1, &#x27;disable_existing_loggers&#x27;: False, &#x27;formatters&#x27;: &#123; &#x27;standard&#x27;: &#123; &#x27;format&#x27;: standard_format &#125;, &#x27;simple&#x27;: &#123; &#x27;format&#x27;: simple_format &#125;, &#x27;test&#x27;: &#123; &#x27;format&#x27;: test_format &#125;, &#125;, &#x27;filters&#x27;: &#123;&#125;, # handlers是日志的接收者，不同的handler会将日志输出到不同位置 &#x27;handlers&#x27;: &#123; #打印到终端的日志 &#x27;console&#x27;: &#123; &#x27;level&#x27;: &#x27;DEBUG&#x27;, &#x27;class&#x27;: &#x27;logging.StreamHandler&#x27;, # 打印到屏幕 &#x27;formatter&#x27;: &#x27;simple&#x27; &#125;, #打印到文件的日志,收集info及以上的日志 &#x27;default&#x27;: &#123; &#x27;level&#x27;: &#x27;DEBUG&#x27;, &#x27;class&#x27;: &#x27;logging.handlers.RotatingFileHandler&#x27;, # 保存到文件,日志轮转 &#x27;formatter&#x27;: &#x27;standard&#x27;, # 可以定制日志文件路径 # BASE_DIR = os.path.dirname(os.path.dirname(__file__)) # log文件的目录 # LOG_PATH = os.path.join(BASE_DIR,&#x27;a1.log&#x27;) &#x27;filename&#x27;: &#x27;a1.log&#x27;, # 日志文件 &#x27;maxBytes&#x27;: 1024*1024*5, # 日志大小 5M &#x27;backupCount&#x27;: 5, # 保留几份，也就是几个文件 &#x27;encoding&#x27;: &#x27;utf-8&#x27;, # 日志文件的编码，再也不用担心中文log乱码了 &#125;, &#x27;other&#x27;: &#123; &#x27;level&#x27;: &#x27;DEBUG&#x27;, &#x27;class&#x27;: &#x27;logging.FileHandler&#x27;, # 保存到文件 &#x27;formatter&#x27;: &#x27;test&#x27;, &#x27;filename&#x27;: &#x27;a2.log&#x27;, &#x27;encoding&#x27;: &#x27;utf-8&#x27;, &#125;, &#x27;default_time&#x27;: &#123; &#x27;level&#x27;: &#x27;DEBUG&#x27;, &#x27;class&#x27;: &#x27;logging.handlers.TimedRotatingFileHandler&#x27;, # 保存到文件,日志轮转 &#x27;formatter&#x27;: &#x27;standard&#x27;, # 可以定制日志文件路径 # BASE_DIR = os.path.dirname(os.path.dirname(__file__)) # log文件的目录 # LOG_PATH = os.path.join(BASE_DIR,&#x27;a1.log&#x27;) &#x27;filename&#x27;: &#x27;a1_time.log&#x27;, # 日志文件 &#x27;when&#x27;: &#x27;midnight&#x27;, # 指定午夜0点切割 &#x27;interval&#x27;: 1, # 每天轮转 &#x27;backupCount&#x27;: 5, # 保留几份，也就是几个文件 &#x27;encoding&#x27;: &#x27;utf-8&#x27;, # 日志文件的编码，再也不用担心中文log乱码了 &#125;, &#125;, # loggers是日志的产生者，产生的日志会传递给handler然后控制输出 &#x27;loggers&#x27;: &#123; # logging.getLogger(__name__)拿到的logger配置 # 不写名字，就代表是空key，getlogger找不到key名，就会走空key，然后日志名处显示的是getlogger的名字 &#x27;&#x27;: &#123; &#x27;handlers&#x27;: [&#x27;default&#x27;, &#x27;console&#x27;], # 这里把上面定义的两个handler都加上，即log数据既写入文件又打印到屏幕 &#x27;level&#x27;: &#x27;DEBUG&#x27;, # loggers(第一层日志级别关卡限制)---&gt;handlers(第二层日志级别关卡限制) &#x27;propagate&#x27;: False, # 默认为True，向上（更高level的logger）传递，通常设置为False即可，否则会一份日志向上层层传递 &#125;, &#x27;base_log&#x27;: &#123; &#x27;handlers&#x27;: [&#x27;default&#x27;, &#x27;console&#x27;], # 这里把上面定义的两个handler都加上，即log数据既写入文件又打印到屏幕 &#x27;level&#x27;: &#x27;DEBUG&#x27;, # loggers(第一层日志级别关卡限制)---&gt;handlers(第二层日志级别关卡限制) &#x27;propagate&#x27;: False, # 默认为True，向上（更高level的logger）传递，通常设置为False即可，否则会一份日志向上层层传递 &#125;, &#x27;专门的采集&#x27;: &#123; &#x27;handlers&#x27;: [&#x27;other&#x27;,], &#x27;level&#x27;: &#x27;DEBUG&#x27;, &#x27;propagate&#x27;: False, &#125;, &#125;,&#125; 11.7 使用12345678910111213141516171819import settings# !!!强调!!!# 1、logging是一个包，需要使用其下的config、getLogger，可以如下导入# from logging import config# from logging import getLoggerfrom logging import config,getLoggerconfig.dictConfig(settings.LOGGING_DIC) # 加载配置字典logging1 = getLogger(&#x27;base_log&#x27;)logging1.info(&#x27;这是一条base_log日志&#x27;)logging2 = getLogger(&#x27;专门的采集&#x27;)logging2.info(&#x27;这是专门的采集日志&#x27;)# 补充知识# 1、日志名的命名：日志名是区别业务归属的一种非常重要的标识# 2、日志轮转：日志记录着程序运行过程中的关键信息 十二，re模块12.1 什么是正则 正则就是用一些具有特殊含义的符号组合到一起（称为正则表达式）来描述字符或者字符串的方法。或者说：正则就是用来描述一类事物的规则。（在Python中）它内嵌在Python中，并通过 re 模块实现。正则表达式模式被编译成一系列的字节码，然后由用 C 编写的匹配引擎执行。 12.2 常用匹配模式(元字符) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899import re# \\w与\\W# \\w：匹配数字字母下划线，\\W取反print(re.findall(&#x27;\\w&#x27;,&#x27;abc123_*()-= &#x27;)) # [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;_&#x27;]print(re.findall(&#x27;\\W&#x27;,&#x27;abc123_*()-= &#x27;)) # [&#x27;*&#x27;, &#x27;(&#x27;, &#x27;)&#x27;, &#x27;-&#x27;, &#x27;=&#x27;, &#x27; &#x27;]# \\s与\\S# \\s：匹配任意空白字符，等价于[\\t\\n\\r\\f]，\\S取反print(re.findall(&#x27;\\s&#x27;,&#x27;abc 123 456&#x27;)) # [&#x27; &#x27;, &#x27; &#x27;]print(re.findall(&#x27;\\S&#x27;,&#x27;abc 123 456&#x27;)) # [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;, &#x27;5&#x27;, &#x27;6&#x27;]# \\t \\n \\r \\f 都是空，都可以被\\s匹配print(re.findall(&#x27;\\s&#x27;,&#x27;ab\\r\\t 1\\n23 45\\f6&#x27;)) # [&#x27;\\r&#x27;, &#x27;\\t&#x27;, &#x27; &#x27;, &#x27;\\n&#x27;, &#x27; &#x27;, &#x27;\\x0c&#x27;]# \\d与\\D# \\d：匹配任意数字，等价于[0-9]，\\D取反print(re.findall(&#x27;\\d&#x27;,&#x27;abc123_*()-= \\t\\n\\r&#x27;)) # [&#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;]print(re.findall(&#x27;\\D&#x27;,&#x27;abc123_*()-= \\t\\n\\r&#x27;)) # [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;_&#x27;, &#x27;*&#x27;, &#x27;(&#x27;, &#x27;)&#x27;, &#x27;-&#x27;, &#x27;=&#x27;, &#x27; &#x27;, &#x27;\\t&#x27;, &#x27;\\n&#x27;, &#x27;\\r&#x27;]# \\A：匹配字符串的开始（了解）print(re.findall(&#x27;\\Ahe&#x27;,&#x27;hello zhangsan 123 -=()&#x27;)) # [&#x27;he&#x27;],\\A==&gt;^# \\Z：匹配字符串结束，如果存在换行，只匹配到换行前的结束字符串（了解）print(re.findall(&#x27;123\\Z&#x27;,&#x27;hello zhangsan -=()123&#x27;)) # [&#x27;123&#x27;],\\Z==&gt;$&#x27;&#x27;&#x27;^ 指定匹配必须出现在字符串的开头或行的开头。\\A 指定匹配必须出现在字符串的开头（忽略 Multiline 选项）。$ 指定匹配必须出现在以下位置：字符串结尾、字符串结尾的 \\n 之前或行的结尾。\\Z 指定匹配必须出现在字符串的结尾或字符串结尾的 \\n 之前（忽略 Multiline 选项）。&#x27;&#x27;&#x27;# ^与$print(re.findall(&#x27;^a&#x27;,&#x27;abc 123 456&#x27;)) # [&#x27;a&#x27;]print(re.findall(&#x27;6$&#x27;,&#x27;abc 123 456&#x27;)) # [&#x27;6&#x27;]# 重复匹配：| . | * | ? | .* | .*? | + | &#123;n,m&#125; |# .：匹配除了\\n任意一个字符print(re.findall(&#x27;a.b&#x27;,&#x27;a1b&#x27;)) # [&#x27;a1b&#x27;]print(re.findall(&#x27;a.b&#x27;,&#x27;a1b a*b a b aaab&#x27;)) # [&#x27;a1b&#x27;, &#x27;a*b&#x27;, &#x27;a b&#x27;, &#x27;aab&#x27;]print(re.findall(&#x27;a.b&#x27;,&#x27;a\\nb&#x27;)) # []print(re.findall(&#x27;a.b&#x27;,&#x27;a\\nb&#x27;,re.S)) #[&#x27;a\\nb&#x27;]print(re.findall(&#x27;a.b&#x27;,&#x27;a\\nb&#x27;,re.DOTALL)) #[&#x27;a\\nb&#x27;]同上一条意思一样# *：左侧字符重复0次或无穷次，贪婪print(re.findall(&#x27;ab*&#x27;,&#x27;bbbbbbb&#x27;)) #[]print(re.findall(&#x27;ab*&#x27;,&#x27;a&#x27;)) #[&#x27;a&#x27;]print(re.findall(&#x27;ab*&#x27;,&#x27;abbbb&#x27;)) #[&#x27;abbbb&#x27;]# +：左侧字符重1次或无穷次，贪婪print(re.findall(&#x27;ab+&#x27;,&#x27;a&#x27;)) #[]print(re.findall(&#x27;ab+&#x27;,&#x27;abbb&#x27;)) #[&#x27;abbb&#x27;]# ?：左侧字符重0次或1次，贪婪print(re.findall(&#x27;ab?&#x27;,&#x27;a&#x27;)) #[&#x27;a&#x27;]print(re.findall(&#x27;ab?&#x27;,&#x27;abbb&#x27;)) #[&#x27;ab&#x27;]#匹配所有包含小数在内的数字print(re.findall(&#x27;\\d+\\.?\\d*&#x27;,&quot;asdfasdf123as1.13dfa12adsf1asdf3&quot;)) #[&#x27;123&#x27;, &#x27;1.13&#x27;, &#x27;12&#x27;, &#x27;1&#x27;, &#x27;3&#x27;]print(re.findall(&#x27;[0-9]+\\.?[0-9]*&#x27;,&quot;asdfasdf123as1.13dfa12adsf1asdf3&quot;)) # [&#x27;123&#x27;, &#x27;1.13&#x27;, &#x27;12&#x27;, &#x27;1&#x27;, &#x27;3&#x27;]# &#123;n,m&#125;：左侧字符重复n次到m次，性格贪婪# &#123;0,&#125; --&gt; *# &#123;1,&#125; --&gt; +# &#123;0,1&#125; --&gt; ?# &#123;n&#125; --&gt; 代表出现n次print(re.findall(&#x27;ab&#123;2&#125;&#x27;,&#x27;abbb&#x27;)) # [&#x27;abb&#x27;]print(re.findall(&#x27;ab&#123;2,4&#125;&#x27;,&#x27;abbb&#x27;)) # [&#x27;abbb&#x27;]print(re.findall(&#x27;ab&#123;1,&#125;&#x27;,&#x27;abbb&#x27;)) # [&#x27;abbb&#x27;] &#x27;ab&#123;1,&#125;&#x27; ===&gt; &#x27;ab+&#x27;print(re.findall(&#x27;ab&#123;0,&#125;&#x27;,&#x27;abbb&#x27;)) # [&#x27;abbb&#x27;] &#x27;ab&#123;0,&#125;&#x27; ===&gt; &#x27;ab*&#x27;# []：匹配指定字符的一个print(re.findall(&#x27;a[1*-]b&#x27;,&#x27;a1b a*b a-b&#x27;)) # [&#x27;a1b&#x27;, &#x27;a*b&#x27;, &#x27;a-b&#x27;] []内的都为普通字符了，且如果-没有被转意的话，应该放到[]的开头或结尾print(re.findall(&#x27;a[^1*-]b&#x27;,&#x27;a1b a*b a-b a=b&#x27;)) # [&#x27;a=b&#x27;] []内的^代表的意思是取反print(re.findall(&#x27;a[0-9]b&#x27;,&#x27;a1b a*b a-b a=b&#x27;)) # [&#x27;a1b&#x27;]print(re.findall(&#x27;a[a-z]b&#x27;,&#x27;a1b a*b a-b a=b aeb&#x27;)) # [&#x27;aeb&#x27;]print(re.findall(&#x27;a[a-zA-Z]b&#x27;,&#x27;a1b a*b a-b a=b aeb aEb&#x27;)) # [&#x27;aeb&#x27;, &#x27;aEb&#x27;]# \\：print(re.findall(&#x27;a\\\\c&#x27;,&#x27;a\\c&#x27;)) #对于正则来说a\\\\c确实可以匹配到a\\c,但是在python解释器读取a\\\\c时，会发生转义，然后交给re去执行，所以抛出异常print(re.findall(r&#x27;a\\\\c&#x27;,&#x27;a\\c&#x27;)) #r代表告诉解释器使用rawstring，即原生字符串，把我们正则内的所有符号都当普通字符处理，不要转义print(re.findall(&#x27;a\\\\\\\\c&#x27;,&#x27;a\\c&#x27;)) #同上面的意思一样，和上面的结果一样都是[&#x27;a\\\\c&#x27;]# ()：分组print(re.findall(&#x27;ab+&#x27;,&#x27;ababab123&#x27;)) #[&#x27;ab&#x27;, &#x27;ab&#x27;, &#x27;ab&#x27;]print(re.findall(&#x27;(ab)+123&#x27;,&#x27;ababab123&#x27;)) #[&#x27;ab&#x27;]，匹配到末尾的ab123中的abprint(re.findall(&#x27;(?:ab)+123&#x27;,&#x27;ababab123&#x27;)) #findall的结果不是匹配的全部内容，而是组内的内容,?:可以让结果为匹配的全部内容print(re.findall(&#x27;href=&quot;(.*?)&quot;&#x27;,&#x27;&lt;a href=&quot;http://www.baidu.com&quot;&gt;点击&lt;/a&gt;&#x27;))#[&#x27;http://www.baidu.com&#x27;]print(re.findall(&#x27;href=&quot;(?:.*?)&quot;&#x27;,&#x27;&lt;a href=&quot;http://www.baidu.com&quot;&gt;点击&lt;/a&gt;&#x27;))#[&#x27;href=&quot;http://www.baidu.com&quot;&#x27;]# |：或的意思print(re.findall(&#x27;compan(?:y|ies)&#x27;,&#x27;Too many companies have gone bankrupt, and the next one is my company&#x27;))","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"软件开发的目录规范","slug":"ruanjiankaifamuluguifan","date":"2023-04-19T06:44:43.000Z","updated":"2023-04-19T06:44:43.000Z","comments":true,"path":"articles/2023/04/19/ruanjiankaifamuluguifan/","permalink":"https://kkabuzs.github.io/articles/2023/04/19/ruanjiankaifamuluguifan/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 22、软件开发的目录规范特别鸣谢： — b站老男孩agen老师 一，软件开发目录规范 为了提高程序的可读性与可维护性，我们应该为软件设计良好的目录结构，这与规范的编码风格同等重要。软件的目录规范并无硬性标准，只要清晰可读即可，假设你的软件名为ATM，推荐目录结构如下 1234567891011121314151617181920212223ATM/|-- src/| |-- src.py||-- api/| |-- api.py||-- db/| |-- db_handle.py||-- lib/| |-- common.py||-- bin/| |-- start.py||-- conf/| |-- settings.py||-- run.py|-- setup.py|-- requirements.txt|-- README 123456789101112131415161718192021简要解释一下:• src/: 存放业务逻辑相关代码• api/: 存放接口文件，接口主要用于为业务逻辑提供数据操作。• db/: 存放操作数据库相关文件，主要用于与数据库交互• lib/: 存放程序中常用的自定义模块• bin/: 存放程序中常用的脚本文件• conf/: 存放配置文件• run.py: 程序的启动文件，一般放在项目的根目录下，因为在运行时会默认将运行文件所在的文件夹作为sys.path的第一个路径，这样就省去了处理环境变量的步骤• setup.py: 安装、部署、打包的脚本。• requirements.txt: 存放软件依赖的外部Python包列表。• README: 项目说明文件。 12345678910111213除此之外，有一些方案给出了更加多的内容，比如LICENSE.txt,ChangeLog.txt文件等，主要是在项目需要开源时才会用到，请读者自行查阅。关于README的内容，这个应该是每个项目都应该有的一个文件，目的是能简要描述该项目的信息，让读者快速了解这个项目。它需要说明以下几个事项:1、软件定位，软件的基本功能；2、运行代码的方法: 安装环境、启动命令等；3、简要的使用说明；4、代码目录结构说明，更详细点可以说明软件的基本原理；5、常见问题说明。 关于setup.py和requirements.txt： 12345一般来说，用setup.py来管理代码的打包、安装、部署问题。业界标准的写法是用Python流行的打包工具setuptools来管理这些事情，这种方式普遍应用于开源项目中。不过这里的核心思想不是用标准化的工具来解决这些问题，而是说，一个项目一定要有一个安装部署工具，能快速便捷的在一台新机器上将环境装好、代码部署好和将程序运行起来。requirements.txt文件的存在是为了方便开发者维护软件的依赖库。我们需要将开发过程中依赖库的信息添加进该文件中，避免在 setup.py安装依赖时漏掉软件包，同时也方便了使用者明确项目引用了哪些Python包。这个文件的格式是每一行包含一个包依赖的说明，通常是flask&gt;=0.10这种格式，要求是这个格式能被pip识别，这样就可以简单的通过 pip install -r requirements.txt来把所有Python依赖库都装好了，具体格式参照https://pip.readthedocs.io/en/1.1/requirements.html 1234567891011# 获取路径的优化方案import osimport sysBASE_DIR=os.path.dirname(os.path.dirname(__file__))sys.path.append(BASE_DIR)from src import srcif __name__ == &#x27;__main__&#x27;: src.run()","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"包","slug":"bao","date":"2023-04-18T06:15:39.000Z","updated":"2023-04-18T06:15:39.000Z","comments":true,"path":"articles/2023/04/18/bao/","permalink":"https://kkabuzs.github.io/articles/2023/04/18/bao/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 21、包特别鸣谢： — b站老男孩agen老师 一，包介绍 随着模块数目的增多，把所有模块不加区分地放到一起也是极不合理的，于是Python为我们提供了一种把模块组织到一起的方法，即创建一个包。包就是一个含有_init_.py文件的文件夹，文件夹内可以组织子模块或子包 包就是一个包含有__init__.py文件的文件夹,包的本质是模块的一种形式，包用来被当作模块导入 123456# 示例：mmm/├── __init__.py ├── mm1.py├── mm2.py└── mm3.py #子模块 需要强调的是 123#1. 在python3中，即使包下没有__init__.py文件，import 包仍然不会报错，而在python2中，包下一定要有该文件，否则import 包报错#2. 创建包的目的不是为了运行，而是被导入使用，记住，包只是模块的一种形式而已，包的本质就是一种模 二,包的使用2.1 导入包与__init__.py123456789# __init__.pyprint(&#x27;running....&#x27;)x = 111y = 222def say(): print(&#x27;sayying.....&#x27;) 12345678# 1、产生一个名称空间# 2、运行包下的__init__.py文件，将运行过程中产生的名字都丢到1的名称空间中# 3、在当前执行文件的名称空间中拿到一个名字mmm，mmm执行1的名称空间import mmmprint(mmm.x)print(mmm.y)mmm.say() 2.2 绝对导入与相对导入12345foo/├── __init__.py ├── mm1.py├── mm2.py└── mm3.py #子模块 针对包内的模块之间互相导入，导入的方式有两种 12345678910111213141516# mm1.pydef f1(): print(&#x27;f1&#x27;)# mm2.pydef f2(): print(&#x27;f2&#x27;)# mm3.pydef f3(): print(&#x27;f3&#x27;) 2.2.1 绝对导入：以顶级包为起始1234567# __init__.py# 绝对导入，以包的文件夹作为起始来进行导入from foo.mm1 import f1 # 点的左侧必须为包，否则语法错误from foo.mm2 import f2from foo.mm3 import f3 2.2.2 相对导入 仅限于包内使用，不能跨出包（包内模块之间的导入，推荐使用相对导入） 12345678910# . 代表当前文件夹# .. 代表上一层文件夹# __init__.py from .mm1 import f1# 强调# 1、相对导入不能跨出包，所以相对导入仅限于包内模块彼此之间导入# 2、绝对导入是没有任何限制的，所以绝对导入是一种通用的导入 123456789101112131415调用包：在调用的时候，只需要执行方法，包内的文件规划，和目录层级是制作包该考虑的，调用包只需要包名就可。# 环境变量是以执行文件为准的，所有被导入的模块或者说后续的其他文件引用的sys.path都是参照执行文件的sys.pathimport foofoo.f1()foo.f2()foo.f3()# 强调# 1.关于包相关的导入语句也分为import和from ... import ...两种，但是无论哪种，无论在什么位置，在导入时都必须遵循一个原则：凡是在导入时带点的，点的左边都必须是一个包，否则非法。可以带有一连串的点，如import 顶级包.子包.子模块,但都必须遵循这个原则。但对于导入后，在使用时就没有这种限制了，点的左边可以是包,模块，函数，类(它们都可以用点的方式调用自己的属性)。# 2、包A和包B下有同名模块也不会冲突，如A.a与B.a来自俩个命名空间# 3、import导入文件时，产生名称空间中的名字来源于文件，import 包，产生的名称空间的名字同样来源于文件，即包下的__init__.py，导入包本质就是在导入该文件 总结： 总结包的使用需要牢记三点 1、导包就是在导包下__init__.py文件 2、包内部的导入应该使用相对导入，相对导入也只能在包内部使用，而且...取上一级不能出包 3、使用语句中的点代表的是访问属性 m.n.x ----&gt; 向m要n，向n要x 而导入语句中的点代表的是路径分隔符 import a.b.c --&gt; a/b/c，文件夹下a下有子文件夹b，文件夹b下有子文件或文件夹c 所以导入语句中点的左边必须是一个包","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"模块","slug":"mokuai","date":"2023-04-07T08:01:28.000Z","updated":"2023-04-07T08:01:28.000Z","comments":true,"path":"articles/2023/04/07/mokuai/","permalink":"https://kkabuzs.github.io/articles/2023/04/07/mokuai/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 20、模块特别鸣谢： — b站老男孩agen老师 一，模块介绍1.1 什么是模块12345678910模块就是一系列功能的集合体，分为三大类 I：内置模块 II：第三方的模块 III：自定义的模块 一个python文件本身就是一个模块，例如：文件名m.py，模块名m ps：模块分为四种形式 1、使用python编写的.py文件 2、已被编译为共享库或DLL的C或C++扩展 3、把一系列模块组知道一起的文件夹（注：文件夹下有一个__init__.py文件，该文件夹称之为包） 4、使用C编写并链接到python解释器的内置模块 1.2 为什么要有模块1234I：内置与第三方的模块拿来就用，无需定义，这种拿来主义，可以极大的提升自己的开发效率II：自定义的模块 可以将程序的各部分功能提取出来放到一个模块中共享使用 优点：减少了代码冗余，程序的组织结构更加清晰 二，模块的使用2.1 import语句123456789#文件名：foo.pyprint(&#x27;模块foo======&gt;&#x27;)x=1def get(): print(x)def change(): global x x=0 要想在另外一个py文件中引用foo.py中的功能，需要使用import foo 首次导入模块会发生3件事1、执行foo.py2、产生foo.py的名称空间，将foo.py运行过程中产生的名字都丢到foo的名称空间中3、在当前文件中产生的有一个名字foo，该名字指向2中产的的名称空间 **ps：**之后的导入，都是直接引用首次导入产生的foo.py的名称空间，不会重复执行代码 2.1.1 调用12345678910111213141516171819# 强调1：指名道姓的问某一个模块要名字对应的值，不会与当前名称空间的名字发生冲突print(foo.x)print(foo.get())print(foo.change())# 强调2：无论是查看还是修改，操作的都是模块本身，与调用位置无关import foox = 3333333foo.change()print(x) # 3333333print(foo.x) # 0foo.get() # 0# 强调#1、在Python中模块也属于第一类对象，可以进行赋值、以数据形式传递以及作为容器类型的元素等操作。#2、模块名应该遵循小写形式，标准库从python2过渡到python3做出了很多这类调整，比如ConfigParser、Queue、SocketServer全更新为纯小写形式。 2.1.2 import其他的知识和规范1234567891011121314151617181920212223# 三、可以以逗号为分隔符，一行导入多个模块# 建议使用这种方式import timeimport fooimport time,foo # 不建议一行导入多个模块# 四、导入模块的规范# I. python内置模块# II. 第三方模块# III. 程序员自定义模块# 五、起别名import time as t # t=timet.sleep(3)# 六、模块是第一类对象（函数也是第一类对象）# 七、自定义模块的命名应该采用纯小写+下划线的命名风格# 八、可以在函数内导入模块（区别就是作用域，函数内是局部） 2.2 from-import 语句 import导入模块在使用时必须加”模块.”：优点：肯定不会与当前名称空间的名字冲突缺点：加前缀显得麻烦 from … import … 导入也发生了三件事1、产生一个模块的名称空间2、运行foo.py将运行过程中产生的名字都丢到模块的名称空间中3、在当前名称空间拿到一个名字，该名字与模块名称空间中的某一个名字对应 1234from foo import x,get,change #将模块foo中的x和get导入到当前名称空间a=x #直接使用模块foo中的x赋值给aget() #直接执行foo中的get函数change() #即便是当前有重名的x，修改的仍然是源文件中的x from ... import ... 导入模块在使用时不需要加前缀 优点：代码更精简 缺点：容易与当前名称空间混淆 另外from语句支持from foo import 语法，代表将foo中所有的名字都导入到当前位置 123456# 一行导入多个名字（不推荐）# from foo import x,get,change# *: 导入模块中所有的名字# （不推荐使用此方法）# from foo import * 模块的编写者可以在自己的文件中定义__all__变量用来控制*代表的意思 12345678#foo.py__all__=[&#x27;x&#x27;,&#x27;get&#x27;] #该列表中所有的元素必须是字符串类型，每个元素对应foo.py中的一个名字x=1def get(): print(x)def change(): global x x=0 123456789# 在另外一个文件中使用*导入时，就只能导入__all__定义的名字了from foo import * #此时的*只代表x和getx #可用get() #可用change() #不可用Foo() #不可用 2.3 其他导入语法(as) 还可以在当前位置为导入的模块起一个别名 123import foo as f #为导入的模块foo在当前位置起别名f，以后再使用时就用这个别名ff.xf.get() 还可以为导入的一个名字起别名 12from foo import get as gg() 2.4 循环导入问题 循环导入问题指的是在一个模块加载&#x2F;导入的过程中导入另外一个模块，而在另外一个模块中又返回来导入第一个模块中的名字，由于第一个模块尚未加载完毕，所以引用失败、抛出异常，究其根源就是在python中，同一个模块只会在第一次导入时执行其内部代码，再次导入该模块时，即便是该模块尚未完全加载完毕也不会去重复执行内部代码 以下述文件为例，来详细分析循环&#x2F;嵌套导入出现异常的原因以及解决的方案 123456# m1.py 文件print(&#x27;正在导入m1&#x27;)from m2 import yx=&#x27;m1&#x27; 123456# m2.py 文件print(&#x27;正在导入m2&#x27;)from m1 import xy=&#x27;m2&#x27; 123# run.py 文件import m1 测试一 1234567891011121314151617#1、执行run.py会抛出异常Traceback (most recent call last): File &quot;/Users/zhaoshuo/PycharmProjects/pythonProject/Python全栈_b站_老男孩/day21/run.py&quot;, line 1, in &lt;module&gt; import m1 File &quot;/Users/zhaoshuo/PycharmProjects/pythonProject/Python全栈_b站_老男孩/day21/m1.py&quot;, line 2, in &lt;module&gt; from m2 import y File &quot;/Users/zhaoshuo/PycharmProjects/pythonProject/Python全栈_b站_老男孩/day21/m2.py&quot;, line 2, in &lt;module&gt; from m1 import xImportError: cannot import name &#x27;x&#x27; from partially initialized module &#x27;m1&#x27; (most likely due to a circular import) (/Users/zhaoshuo/PycharmProjects/pythonProject/Python全栈_b站_老男孩/day21/m1.py)正在导入m1正在导入m2#2、分析先执行run.py---&gt;执行import m1，开始导入m1并运行其内部代码---&gt;打印内容&quot;正在导入m1&quot;---&gt;执行from m2 import y 开始导入m2并运行其内部代码---&gt;打印内容“正在导入m2”---&gt;执行from m1 import x,由于m1已经被导入过了，所以不会重新导入，所以直接去m1中拿x，然而x此时并没有存在于m1中，所以报错 测试二 123456789101112131415161718#1、执行文件不等于导入文件，比如执行m1.py不等于导入了m1Traceback (most recent call last): File &quot;/Users/zhaoshuo/PycharmProjects/pythonProject/Python全栈_b站_老男孩/day21/m1.py&quot;, line 2, in &lt;module&gt; from m2 import y File &quot;/Users/zhaoshuo/PycharmProjects/pythonProject/Python全栈_b站_老男孩/day21/m2.py&quot;, line 2, in &lt;module&gt; from m1 import x File &quot;/Users/zhaoshuo/PycharmProjects/pythonProject/Python全栈_b站_老男孩/day21/m1.py&quot;, line 2, in &lt;module&gt; from m2 import yImportError: cannot import name &#x27;y&#x27; from partially initialized module &#x27;m2&#x27; (most likely due to a circular import) (/Users/zhaoshuo/PycharmProjects/pythonProject/Python全栈_b站_老男孩/day21/m2.py)正在导入m1正在导入m2正在导入m1#2、分析执行m1.py，打印“正在导入m1”，执行from m2 import y ，导入m2进而执行m2.py内部代码---&gt;打印&quot;正在导入m2&quot;，执行from m1 import x，此时m1是第一次被导入，执行m1.py并不等于导入了m1，于是开始导入m1并执行其内部代码---&gt;打印&quot;正在导入m1&quot;，执行from m1 import y，由于m1已经被导入过了，所以无需继续导入而直接问m2要y，然而y此时并没有存在于m2中所以报错 解决方案 123456789101112131415161718192021222324252627282930313233343536373839404142# 方案一：导入语句放到最后，保证在导入时，所有名字都已经加载过# 文件：m1.pyprint(&#x27;正在导入m1&#x27;)x=&#x27;m1&#x27;from m2 import y# 文件：m2.pyprint(&#x27;正在导入m2&#x27;)y=&#x27;m2&#x27;from m1 import x# 文件：run.py内容如下，执行该文件，可以正常使用import m1print(m1.x)print(m1.y)# 方案二：导入语句放到函数中，只有在调用函数时才会执行其内部代码# 文件：m1.pyprint(&#x27;正在导入m1&#x27;)def f1(): from m2 import y print(x,y)x = &#x27;m1&#x27;# 文件：m2.pyprint(&#x27;正在导入m2&#x27;)def f2(): from m1 import x print(x,y)y = &#x27;m2&#x27;# 文件：run.py内容如下，执行该文件，可以正常使用import m1m1.f1() 注意：循环导入问题大多数情况是因为程序设计失误导致，上述解决方案也只是在烂设计之上的无奈之举，在我们的程序中应该尽量避免出现循环&#x2F;嵌套导入，如果多个模块确实都需要共享某些数据，可以将共享的数据集中存放到某一个地方，然后进行导入 2.5 搜索模块的路径与优先级123456789101112131415161718# 无论是import还是from...import在导入模块时都涉及到查找问题# 优先级：# 1、内存（内置模块）# 2、硬盘：按照sys.path中存放的文件夹的顺序依次查找要导入的模块import sys# 值为一个列表，存放了一系列的文件夹# 其中第一个文件夹是当前执行文件所在的文件夹print(sys.path)[&#x27;/Users/zhaoshuo/PycharmProjects/pythonProject/Python全栈_b站_老男孩/day21&#x27;, &#x27;/Users/zhaoshuo/PycharmProjects/pythonProject&#x27;, &#x27;/Library/Frameworks/Python.framework/Versions/3.9/lib/python39.zip&#x27;, &#x27;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9&#x27;, &#x27;/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload&#x27;, &#x27;/Users/zhaoshuo/PycharmProjects/pythonProject/venv/lib/python3.9/site-packages&#x27;]# 列表中的每个元素其实都可以当作一个目录来看：在列表中会发现有.zip或.egg结尾的文件，二者是不同形式的压缩文件，事实上Python确实支持从一个压缩文件中导入模块，我们也只需要把它们都当成目录去看即可。# 了解：sys.modules查看已经加载到内存中的模块print(sys.modules) sys.path中的第一个路径通常为空，代表执行文件所在的路径，所以在被导入模块与执行文件在同一目录下时肯定是可以正常导入的 12345# sys.path的应用import sys# 找foo.py，就把foo.py的文件夹添加到环境变量中sys.path.append(r&#x27;/Users/zhaoshuo/PycharmProjects/pythonProject/Python全栈_b站_老男孩/day21/aa&#x27;) # 临时添加import foo # 就不会再报错了 2.6 区分py文件的两种用途 一个Python文件有两种用途，一种被当主程序&#x2F;脚本执行，另一种被当模块导入，为了区别同一个文件的不同用途，每个py文件都内置了_name__变量，该变量在py文件被当做脚本执行时赋值为“_main”,在py文件被当做模块导入时赋值为模块名 作为模块foo.py的开发者，可以在文件末尾基于__name__在不同应用场景下值的不同来控制文件执行不同的逻辑 123456789101112# 一个py文件有两种用途# 1、被当成程序运行# 2、被当作模块导入# 二者的区别是什么？# 假设有一文件：foo.py# 1、当foo.py被运行时，__name__的值为&#x27;__main__&#x27;# 2、当foo.py被当作模块导入时，__name__的值为&#x27;foo&#x27;if __name__ == &#x27;__main__&#x27;: print(&#x27;文件被执行&#x27;)else: print(&#x27;文件被导入&#x27;) 2.7 编写一个规范的模块 我们在编写py文件时，需要时刻提醒自己，该文件既是给自己用的，也有可能会被其他人使用，因而代码的可读性与易维护性显得十分重要，为此我们在编写一个模块时最好按照统一的规范去编写，如下 1234567891011121314151617181920# 模版#!/usr/bin/env python #通常只在类unix环境有效,作用是可以使用脚本名来执行，而无需直接调用解释器。&quot;The module is used to...&quot; #模块的文档描述import sys #导入模块x=1 #定义全局变量,如果非必须,则最好使用局部变量,这样可以提高代码的易维护性,并且可以节省内存提高性能class Foo: #定义类,并写好类的注释 &#x27;Class Foo is used to...&#x27; passdef test(): #定义函数,并写好函数的注释 &#x27;Function test is used to…&#x27; passif __name__ == &#x27;__main__&#x27;: #主程序 test() #在被当做脚本执行时,执行此处的代码 2.8 函数的类型提示1234567891011121314151617def register(name:str,age:int,hobbies:tuple)-&gt;int: # -&gt;int 是提示返回值的类型 print(name) print(age) print(hobbies) return 111register(&#x27;zhangsan&#x27;,18,(&#x27;play&#x27;,&#x27;sleep&#x27;))def register(name:&quot;必须传入名字&quot;,age:&quot;年龄是数字&quot;,hobbies:tuple)-&gt;int: # 冒号后面是提示信息，随便写 不会影响原函数运行 print(name) print(age) print(hobbies) return 111register(&#x27;zhangsan&#x27;,18,(&#x27;play&#x27;,&#x27;sleep&#x27;))print(register.__annotations__) # 查看函数的提示信息","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"面向过程与函数式","slug":"mianxiangguochengyuhanshushi","date":"2023-04-06T09:15:50.000Z","updated":"2023-04-06T09:15:50.000Z","comments":true,"path":"articles/2023/04/06/mianxiangguochengyuhanshushi/","permalink":"https://kkabuzs.github.io/articles/2023/04/06/mianxiangguochengyuhanshushi/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 19、面向过程与函数式特别鸣谢： — b站老男孩agen老师 一，二分法 二分法 —&gt; 就是算法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# 需求：有一个按照从小到大顺序排列的数字列表# 需要从该数字列表中找到我们想要的哪一个数字# 如何做更高效？？？nums = [-3,4,7,10,13,21,43,77,89]find_num = 13# 方案一：整体遍历，效率太低for num in nums: if num == find_num: print(&#x27;find it&#x27;) break# 方案二：二分法&#x27;&#x27;&#x27;伪代码示例：def 函数(find_num,列表): mid_val = 找到列表中间的值 if find_num &gt; mid_val: # 接下来查找的应该是在列表的右半部分 列表=列表切片的右半部分 函数(find_num,列表) elif find_num &lt; mid_val: # 接下来查找的应该是在列表的左半部分 列表=列表切片的左半部分 函数(find_num,列表) else: print(&#x27;find it&#x27;)&#x27;&#x27;&#x27;nums = [-3,4,7,10,13,21,43,77,89]nums=nums.sort() # 若是不是有序的，进行排序find_num = 9def binary_search(find_num,l): print(l) if len(l) == 0: print(&#x27;找的值不存在&#x27;) return mid_index=len(l)//2 mid_val = l[mid_index] if find_num &gt; mid_val: # 接下来查找的应该是在列表的右半部分 l=l[mid_index+1:] binary_search(find_num,l) elif find_num &lt; mid_val: # 接下来查找的应该是在列表的左半部分 l=l[:mid_index] binary_search(find_num,l) else: print(&#x27;find it&#x27;)binary_search(find_num,nums) 二，面向过程的编程思想 核心是”过程”二字，过程即流程，指的是做事的步骤：先什么、再什么、后干什么 基于该思想编写程序就好比在设计一条流水线 2.1 面向过程的优缺点优点： 复杂的问题流程化、进而简单化 缺点： 扩展性非常差 2.2 面向过程的应用场景 1、不是所有的软件都需要频繁更迭：比如编写脚本2、即便是一个软件需要频繁更迭，也并不代表这个软件所有的组成部分都需要一起更迭 三，函数式 函数式编程并非用函数编程这么简单，而是将计算机的运算视为数学意义上的运算，比起面向过程，函数式更加注重的是执行结果而非执行的过程，代表语言有：Haskell、Erlang。而python并不是一门函数式编程语言，但是仍为我们提供了很多函数式编程好的特性，如lambda，map，reduce，filter 3.1 匿名函数与lambda 对比使用def关键字创建的是有名字的函数，使用lambda关键字创建则是没有名字的函数，即匿名函数，语法如下 lambda 参数1,参数2,...: expression 123456# 定义lambda x,y,z:x+y+z#等同于def func(x,y,z): return x+y+z 12345678910# 1、def用于定义有名函数# func = 函数的内存地址def func(x,y): return x+yprint(func) # &lt;function func at 0x1004b1d30&gt;# 2、lambda定义匿名函数lambda x,y:x+y # 返回值x+y 不用写return，默认就有returnprint(lambda x,y:x+y) # &lt;function &lt;lambda&gt; at 0x10fc53dc0&gt; 12345678910# 3、调用匿名函数# 方式一：很少这样使用res = (lambda x,y:x+y)(1,2)print(res)# 方式二：基本不会这样使用res1 = lambda x,y:x+yprint(res1(1,2))# 4、匿名函数用于临时调用一次的场景：更多的是将匿名函数与其他函数配合使用 3.2 匿名函数的应用 匿名函数与有名函数有相同的作用域，但是匿名意味着引用计数为0，使用一次就释放，所以匿名函数用于临时使用一次的场景，匿名函数通常与其他函数配合使用 1234567891011121314151617181920212223242526272829303132333435salaries=&#123; &#x27;siry&#x27;:3000, &#x27;tom&#x27;:7000, &#x27;lili&#x27;:10000, &#x27;jack&#x27;:2000&#125;# 需求1：找出薪资最高的那个人# def func(k):# return salaries[k]# res = max(salaries,key=func) # 返回值=func(&#x27;siry&#x27;)# print(res)# 使用匿名函数# max() 取最大值res = max(salaries,key=lambda k:salaries[k])print(res) # lili# min()取最小值res = min(salaries,key=lambda k:salaries[k])print(res) # jack# 排序sorted()的应用salaries=&#123; &#x27;siry&#x27;:3000, &#x27;tom&#x27;:7000, &#x27;lili&#x27;:10000, &#x27;jack&#x27;:2000&#125;res = sorted(salaries,key=lambda k:salaries[k])print(res) # [&#x27;jack&#x27;, &#x27;siry&#x27;, &#x27;tom&#x27;, &#x27;lili&#x27;] 从小到大res = sorted(salaries,key=lambda k:salaries[k],reverse=True) # 倒序，默认为falseprint(res) # [&#x27;lili&#x27;, &#x27;tom&#x27;, &#x27;siry&#x27;, &#x27;jack&#x27;] 从大到小 3.3 map、reduce、filter12345678910111213141516171819202122232425262728293031323334353637383940414243# ============================ map的应用（了解）l = [&#x27;zhangsan&#x27;,&#x27;lisi&#x27;,&#x27;wangwu&#x27;,&#x27;zhouliu&#x27;]# 缺点：如果值太多，会过多消耗资源，采取生成器模式res = [i+&#x27;_dsb&#x27;for i in l]print(res) # [&#x27;zhangsan_dsb&#x27;, &#x27;lisi_dsb&#x27;, &#x27;wangwu_dsb&#x27;, &#x27;zhouliu_dsb&#x27;]# 使用生成器res = (i+&#x27;_dsb&#x27;for i in l)print(res) # &lt;generator object &lt;genexpr&gt; at 0x1056c3660&gt;print(list(res)) # [&#x27;zhangsan_dsb&#x27;, &#x27;lisi_dsb&#x27;, &#x27;wangwu_dsb&#x27;, &#x27;zhouliu_dsb&#x27;]# 使用map函数# 主要用于批量处理res = map(lambda i:i+&#x27;_dsb&#x27;,l)print(res) # &lt;map object at 0x106a49cd0&gt; # 就是生成器# ============================ filter的应用（了解）# 主要用于过滤操作l = [&#x27;zhangsan_sb&#x27;,&#x27;lisi_sb&#x27;,&#x27;wangwu&#x27;,&#x27;zhouliu&#x27;]# 同理使用生成器res = (i for i in l if i.endswith(&#x27;sb&#x27;))print(res) # &lt;generator object &lt;genexpr&gt; at 0x104d56660&gt;# 使用filter函数res = filter(lambda i:i.endswith(&#x27;sb&#x27;),l)print(res) # &lt;filter object at 0x10d301850&gt; # 也是生成器# ============================ reduce的应用（了解）# 主要作用于合并操作from functools import reduce# 执行逻辑：先取出两个值，10和1，进行x+y操作，得出11，再取出2，在进行相加操作，11+2，得出13...依次进行res = reduce(lambda x,y:x+y,[1,2,3],10)print(res) # 16# 也可以进行字符串相加，逻辑同上res = reduce(lambda x,y:x+y,[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;],&#x27;hello&#x27;)print(res) # helloabcres = reduce(lambda x,y:x+y,[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;])# 如果没有初始值，那么会取出第一个a作为初始值，再取出b进行相加","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"函数递归","slug":"hanshudigui","date":"2023-04-06T08:31:49.000Z","updated":"2023-04-06T08:31:49.000Z","comments":true,"path":"articles/2023/04/06/hanshudigui/","permalink":"https://kkabuzs.github.io/articles/2023/04/06/hanshudigui/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 18、函数递归特别鸣谢： — b站老男孩agen老师 一，函数递归调用介绍 函数的递归调用：是函数嵌套调用的一种特殊形式具体是指：在调用一个函数过程中又直接或间接的调用到本身 123456# 直接调用本身def f1(): print(&#x27;是我是我还是我&#x27;) f1()f1() 123456789101112# 间接调用本身def f1(): print(&#x27;====&gt; f1&#x27;) f2()def f2(): print(&#x27;====&gt; f2&#x27;) f1()f1()# 递归的本质就是循环 从上图可以看出，两种情况下的递归调用都是一个无限循环的过程，但在python对函数的递归调用的深度做了限制，因而并不会像大家所想的那样进入无限循环，会抛出异常，要避免出现这种情况，就必须让递归调用在满足某个特定条件下终止。 123456789# 强调：递归调用不应该无限的调用下去，必须在满足某种条件下结束递归调用def f1(n): if n == 10: return print(n) n+=1 f1(n)f1(0) 提示： 1234#1. 可以使用sys.getrecursionlimit()去查看递归深度，默认值为1000，虽然可以使用sys.setrecursionlimit()去设定该值，但仍受限于主机操作系统栈大小的限制#2. python不是一门函数式编程语言，无法对递归进行尾递归优化。 二，回溯与递推 下面我们用一个浅显的例子，为了让读者阐释递归的原理和使用： 某公司四个员工坐在一起，问第四个人薪水，他说比第三个人多1000，问第三个人薪水，第他说比第二个人多1000，问第二个人薪水，他说比第一个人多1000，最后第一人说自己每月5000，请问第四个人的薪水是多少？ 思路解析： 要知道第四个人的月薪，就必须知道第三个人的，第三个人的又取决于第二个人的，第二个人的又取决于第一个人的，而且每一个员工都比前一个多一千，数学表达式即： 1234567salary(4)=salary(3)+1000 salary(3)=salary(2)+1000 salary(2)=salary(1)+1000 salary(1)=5000总结为： salary(n)=salary(n-1)+1000 (n&gt;1) salary(1)=5000 (n=1) 很明显这是一个递归的过程，可以将该过程分为两个阶段：回溯和递推。 在回溯阶段，要求第n个员工的薪水，需要回溯得到(n-1)个员工的薪水，以此类推，直到得到第一个员工的薪水，此时，salary(1)已知，因而不必再向前回溯了。然后进入递推阶段：从第一个员工的薪水可以推算出第二个员工的薪水(6000)，从第二个员工的薪水可以推算出第三个员工的薪水(7000)，以此类推，一直推算出第第四个员工的薪水(8000)为止，递归结束。需要注意的一点是，递归一定要有一个结束条件，这里n&#x3D;1就是结束条件。 123456789# 代码实现：def salary(n): if n==1: return 5000 return salary(n-1)+1000s=salary(4)print(s) # 8000 程序分析： 在未满足n &#x3D;&#x3D; 1的条件时，一直进行递归调用，即一直回溯。而在满足n &#x3D;&#x3D; 1的条件时，终止递归调用，即结束回溯，从而进入递推阶段，依次推导直到得到最终的结果。 2.1 递归的应用 比如有一个嵌套多层的列表，要求打印出所有的元素，代码实现如下 1234567891011l = [1,2,[3,[4,[5,6,7,8]]]]def f1(list1): for x in list1: if type(x) is list: # 如果是列表，应该再循环，再判断，即重新运行本身的代码 f1(x) else: print(x)f1(l)","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"生成器","slug":"shengchengqi","date":"2023-04-05T08:02:17.000Z","updated":"2023-04-05T08:02:17.000Z","comments":true,"path":"articles/2023/04/05/shengchengqi/","permalink":"https://kkabuzs.github.io/articles/2023/04/05/shengchengqi/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 17、生成器特别鸣谢： — b站老男孩agen老师 一，生成器与yield 在函数内一旦存在yield关键字，调用函数并不会执行函数体代码，会返回一个生成器对象，生成器就是自定义的迭代器 12345678910111213141516171819202122232425262728293031323334353637383940414243def func(): print(&#x27;第一次&#x27;) yield 1 print(&#x27;第二次&#x27;) yield 2 print(&#x27;第三次&#x27;) yield 3 print(&#x27;第四次&#x27;)g = func()print(g) # &lt;generator object func at 0x1026c7660&gt; # generator就是生成器# 生成器就是迭代器# g.__iter__()# g.__next__()# 生成器的__next__方法才会触发函数体代码的运行，然后遇到yield停下来，将yield后的值当作本次调用的结果返回res1 = g.__next__()print(res1)&#x27;&#x27;&#x27;输出内容：第一次1&#x27;&#x27;&#x27;res2 = g.__next__()print(res2)&#x27;&#x27;&#x27;输出内容：第一次1第二次2&#x27;&#x27;&#x27;# 小知识补充print(&#x27;aaa&#x27;.__len__())len(&#x27;aaa&#x27;) # 本质就是 &#x27;aaa&#x27;.__len__()next(g) # 等同于 g.__next__()# iter(可迭代对象) # 可迭代对象.__iter__() 二，yield表达式应用 在函数内可以采用表达式形式的yield 12345678910111213141516171819202122# x = yield 返回值def dog(name): print(&#x27;狗哥%s准备吃东西啦...&#x27; %name) while True: # x 拿到的是yield接收到的值 x = yield # x = &#x27;一根骨头&#x27; print(&#x27;狗哥%s吃了%s&#x27; %(name,x))g=dog(&#x27;来福&#x27;)# 第一次传值必须为None，否则报错，可以理解为给生成器初始化g.send(None) # 等同于 next(g)g.send(&#x27;一根骨头&#x27;) # send是给yield传值，赋值给xg.send(&#x27;肉包子&#x27;)&#x27;&#x27;&#x27;结果输出：狗哥来福准备吃东西啦...狗哥来福吃了一根骨头狗哥来福吃了肉包子&#x27;&#x27;&#x27;g.close() # 关闭，关闭之后就不能在传值了。 表达式形式的yield也可以用于返回多次值，即变量名=yield 值的形式，如下 1234567891011121314151617181920# yield返回值def dog(name): food_list = [] print(&#x27;狗哥%s准备吃东西啦...&#x27; %name) while True: # x 拿到的是yield接收到的值 x = yield food_list # x = &#x27;肉包子&#x27; print(&#x27;狗哥%s吃了%s&#x27; %(name,x)) food_list.append(x) # [&#x27;一根骨头&#x27;, &#x27;肉包子&#x27;]g=dog(&#x27;来福&#x27;)# 第一次传值必须为None，否则报错，可以理解为给生成器初始化res = g.send(None) # 等同于 next(g)print(res)res = g.send(&#x27;一根骨头&#x27;) # send是给yield传值，赋值给x，x被food_list.append添加到food_list列表中，然后返回值是这个列表print(res)res = g.send(&#x27;肉包子&#x27;)print(res) 三，三元表达式、生成式3.1 三元表达式12345678910111213141516171819# 针对以下需求def func(x,y): if x &gt; y: return x else: return yres = func(1,2)print(res)# 三元表达式# 语法格式： 条件成立时返回的值 if 条件 else 条件不成立时要返回的值x = 1y = 2res = x if x &gt; y else yprint(res) # 2res = &#x27;张三&#x27; if x &gt; y else &#x27;哈哈哈&#x27; # 返回的不一定是x 或者 yprint(res) # 哈哈哈 3.2 列表生成式12345678910111213141516171819202122# 1、列表生成式l = [&#x27;zhangsan_dsb&#x27;,&#x27;lisi_dsb&#x27;,&#x27;wangwu_dsb&#x27;,&#x27;zhouliu&#x27;,&#x27;wangqi&#x27;]new_l = []for name in l: if name.endswith(&#x27;dsb&#x27;): # 判断dsb结尾的字符串 new_l.append(name)print(new_l) # [&#x27;zhangsan_dsb&#x27;, &#x27;lisi_dsb&#x27;, &#x27;wangwu_dsb&#x27;]# 使用列表生成式一行实现new_l = [name for name in l if name.endswith(&#x27;dsb&#x27;)]print(new_l) # [&#x27;zhangsan_dsb&#x27;, &#x27;lisi_dsb&#x27;, &#x27;wangwu_dsb&#x27;]# 不加for循环也可以，默认情况下就是 if Truenew_l = [name for name in l]# 案例：# 把所有小写字母全变成大写new_l = [name.upper() for name in l]print(new_l) # [&#x27;ZHANGSAN_DSB&#x27;, &#x27;LISI_DSB&#x27;, &#x27;WANGWU_DSB&#x27;, &#x27;ZHOULIU&#x27;, &#x27;WANGQI&#x27;]# 把所有名字去掉后缀_dsbnew_l = [name.replace(&#x27;_dsb&#x27;,&#x27;&#x27;) for name in l]print(new_l) # [&#x27;zhangsan&#x27;, &#x27;lisi&#x27;, &#x27;wangwu&#x27;, &#x27;zhouliu&#x27;, &#x27;wangqi&#x27;] 3.3 字典生成式123456789# 2、字典生成式keys = [&#x27;zhangsan&#x27;, &#x27;lisi&#x27;, &#x27;wangwu&#x27;]dic = &#123;i:None for i in keys&#125;print(dic) # &#123;&#x27;zhangsan&#x27;: None, &#x27;lisi&#x27;: None, &#x27;wangwu&#x27;: None&#125;# 案例2items = &#123;(&#x27;name&#x27;,&#x27;zhangsan&#x27;),(&#x27;age&#x27;,18),(&#x27;gender&#x27;,&#x27;male&#x27;)&#125;res = &#123;k:v for k,v in items if k != &#x27;gender&#x27;&#125;print(res) # &#123;&#x27;name&#x27;: &#x27;zhangsan&#x27;, &#x27;age&#x27;: 18&#125; 3.4 集合生成式1234# 3、集合生成式l = [&#x27;zhangsan&#x27;, &#x27;lisi&#x27;, &#x27;wangwu&#x27;, &#x27;zhouliu&#x27;, &#x27;wangqi&#x27;]set1 = &#123;i for i in l&#125;print(set1,type(set1)) # &#123;&#x27;wangwu&#x27;, &#x27;zhangsan&#x27;, &#x27;wangqi&#x27;, &#x27;lisi&#x27;, &#x27;zhouliu&#x27;&#125; &lt;class &#x27;set&#x27;&gt; 3.5 生成器表达式 创建一个生成器对象有两种方式，一种是调用带yield关键字的函数，另一种就是生成器表达式，与列表生成式的语法格式相同，只需要将[]换成()，即： （expression for item in iterable if condition） 12345678910111213141516171819202122232425262728# 4、生成器表达式g=(i for i in range(10) if i &gt; 3)# 强调：此刻g内部一个值也没有，print(g,type(g)) # &lt;generator object &lt;genexpr&gt; at 0x1096dae40&gt; &lt;class &#x27;generator&#x27;&gt;print(next(g)) # 4print(next(g)) # 5print(next(g)) # 7print(next(g)) # 7# 统计字符个数with open(&#x27;day19笔记.md&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f: # 方式一 # res = 0 # for line in f: # res += len(line) # print(res) # 416 # 方法二：缺陷，如果行数太多，也会造成压力 # res1 = sum([len(line) for line in f]) # sum()函数方法是对序列进行求和计算 # print(res1) # 416 # 方法三 # res2=sum((len(line) for line in f)) # 使用生成器表达式 # print(res2) # 416 # 上述可以简写成如下形式 res2 = sum(len(line) for line in f) # 不加括号默认就是生成器表达式 print(res2)","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"迭代器","slug":"diedaiqi","date":"2023-04-04T09:55:04.000Z","updated":"2023-04-04T09:55:04.000Z","comments":true,"path":"articles/2023/04/04/diedaiqi/","permalink":"https://kkabuzs.github.io/articles/2023/04/04/diedaiqi/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 16、迭代器特别鸣谢： — b站老男孩agen老师 一，迭代器介绍 迭代器指的是迭代取值的工具，迭代是一个重复的过程，每次重复都是基于上一次结果而继续的，单纯的重复并不是迭代。 1234# 单纯的重复并不是迭代while True: msg = input(&#x27;&gt;&gt;: &#x27;).strip() print(msg) 下述while循环才是一个迭代过程，不仅满足重复，而且以每次重新赋值后的count值作为下一次循环中新的索引进行取值，反复迭代，最终可以取尽条件满足的值 1234count = 0while count &lt; 5: print(count) count += 1 1.1 为何要有迭代器迭代器是用来迭代取值的工具，而涉及到把多个值循环取出来的类型有：列表、字符串、元组、字典、集合、打开的文件 12345l = [&#x27;zhangsan&#x27;,&#x27;lisi&#x27;,&#x27;wangwu&#x27;]i = 0while i &lt; len(l): print(l[i]) i+=1 上述迭代取值的方式只适用于有索引的数据类型：列表、字符串、元组 为了解决基于索引迭代取值的局限性，python必须提供一种能够不依赖于索引的取值方式，这就是迭代器。 1.2 可迭代对象 但凡内置有__iter__方法的都称之为可迭代对象 123456789101112131415161718s1 =&#x27;&#x27;# s1.__iter__()l = []# l.__iter__()t = ()# t.__iter__()d = &#123;&#x27;a&#x27;:1&#125;# d.__iter__()set1 = &#123;1,2,3&#125;# set1.__iter__()with open(&#x27;a.txt&#x27;,mode=&#x27;w&#x27;) as f:# f.__iter__() pass 1.3 迭代器对象123456789101112131415161718192021222324252627# 调用可迭代对象下的__iter__方法会将其转换成迭代器对象d = &#123;&#x27;a&#x27;:1,&#x27;b&#x27;:2&#125;d_iterator = d.__iter__()print(d_iterator) # &lt;dict_keyiterator object at 0x102761c20&gt;print(d_iterator.__next__()) # aprint(d_iterator.__next__()) # b# print(d_iterator.__next__()) # 抛出异常：StopIteration 没有值可以取了#使用while循环while True: try: # 捕获异常，后面会学到 print(d_iterator.__next__()) except StopIteration: break# 可迭代对象与迭代器详解# 可迭代对象（&quot;可以转换成迭代器的对象&quot;）：内置有__iter__方法的对象# 可迭代对象.__iter__(): 得到迭代器对象# 迭代器对象：内置有__next__方法并且内置有__iter__方法的对象# 迭代器对象.__next__(): 得到迭代器的下一个值# 迭代器对象.__iter__(): 得到迭代器本身，说白了调了跟没调一个样dic = &#123;&#x27;a&#x27;:1,&#x27;b&#x27;:2&#125;dic_iterator=dic.__iter__()print(dic_iterator is dic_iterator.__iter__()) # True 二，for循环原理for循环可以称为迭代器循环 12345678910111213141516d = &#123;&#x27;a&#x27;:1,&#x27;b&#x27;:2&#125;d_iterator = d.__iter__()while True: try: # 捕获异常，后面会学到 print(d_iterator.__next__()) except StopIteration: break# 使用for循环# 1、d.__iter__()得到一个迭代器对象# 2、迭代器对象.__next__()拿到一个返回值，然后将该返回值赋值给k# 3、循环往复步骤2，直到抛出StopIteration异常for循环会捕捉异常然后结束循环for k in d: print(k) 三，迭代器的优缺点3.1 优点 1、为序列和非序列类型提供了一种统一的迭代取值方式。 2、惰性计算：迭代器对象表示的是一个数据流，可以只在需要时才去调用next来计算出一个值，就迭代器本身来说，同一时刻在内存中只有一个值，因而可以存放无限大的数据流，而对于其他容器类型，如列表，需要把所有的元素都存放于内存中，受内存大小的限制，可以存放的值的个数是有限的。 3.2 缺点 1、除非取尽，否则无法获取迭代器的长度 2、只能取下一个值，不能回到开始，更像是‘一次性的’，迭代器产生后的唯一目标就是重复执行next方法直到值取尽，否则就会停留在某个位置，等待下一次调用next；若是要再次迭代同个对象，你只能重新调用iter方法去创建一个新的迭代器对象，如果有两个或者多个循环使用同一个迭代器，必然只会有一个循环能取到值。","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"装饰器","slug":"zhuangshiqi","date":"2023-04-03T03:34:01.000Z","updated":"2023-04-03T03:34:01.000Z","comments":true,"path":"articles/2023/04/03/zhuangshiqi/","permalink":"https://kkabuzs.github.io/articles/2023/04/03/zhuangshiqi/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 15、装饰器特别鸣谢： — b站老男孩agen老师 一，装饰器储备知识1234567891011121314151617181920212223242526272829303132333435363738# 一、储备知识# 1、*args、**kwargsdef index(x,y): print(&#x27;====&gt;&#x27;,x,y) # ====&gt; 1 2def wrapper(*args,**kwargs): index(*args,**kwargs) # wrapper函数是在给index函数传值wrapper(1,2)# 2、名称空间与作用域：名称空间的&quot;嵌套&quot;关系是在函数定义阶段，即检测语法的时候来确定的# 3、函数对象：# 可以把函数当作参数传入# 可以把函数当做返回值返回def index1(): return 123def foo(func): return funcfoo(index1)# 4、函数的嵌套定义def f1(): # f1函数可以传参，也可以传入一个函数，例：f1(func) def f2(): pass return f2# 5、闭包函数def out(): x = 1 def nei(): print(x) return neiout_one = out()# 传参方式一：通过参数的形式为函数体传值# 传参方式二：通过闭包的方式为函数体传值 二，装饰器介绍2.1 为何要用装饰器 软件的设计应该遵循开放封闭原则，即对扩展是开放的，而对修改是封闭的。对扩展开放，意味着有新的需求或变化时，可以对现有代码进行扩展，以适应新的情况。对修改封闭，意味着对象一旦设计完成，就可以独立完成其工作，而不要对其进行修改。 软件包含的所有功能的源代码以及调用方式，都应该避免修改，否则一旦改错，则极有可能产生连锁反应，最终导致程序崩溃，而对于上线后的软件，新需求或者变化又层出不穷，我们必须为程序提供扩展的可能性，这就用到了装饰器。 开放封闭原则 开放：指的是对拓展功能是开放的 封闭：指的是对修改源代码是封闭的 2.2 什么是装饰器 “装饰”代指为被装饰对象添加新的功能，”器”代指器具&#x2F;工具，装饰器与被装饰的对象均可以是任意可调用对象。概括地讲，装饰器的作用就是在不修改被装饰对象源代码和调用方式的前提下为被装饰对象添加额外的功能。装饰器经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等应用场景，装饰器是解决这类问题的绝佳设计，有了装饰器，就可以抽离出大量与函数功能本身无关的雷同代码并继续重用。 提示：可调用对象有函数，方法或者类，此处我们单以本章主题函数为例，来介绍函数装饰器，并且被装饰的对象也是函数。 器指的是工具，可以定义成函数 装饰指的是为其他事物添加额外的东西点缀 合到一起的解释： 装饰器指的是定义一个函数，该函数是用来为其他函数添加额外的功能 三，装饰器的实现 函数装饰器分为：无参装饰器和有参装饰两种，二者的实现原理一样，都是’函数嵌套+闭包+函数对象’的组合使用的产物。 3.1 无参装饰器的实现 需求：在不修改index函数的源代码以及调用方式的前提下为其添加统计运行时间的功能 1234567import timedef index(x,y): time.sleep(3) print(&#x27;index &#123;&#125; &#123;&#125;&#x27;.format(x,y))index(111,222)# index(y=222,x=111)# index(111,y=222) 解决方案一：失败 问题：没有修改被装饰对象的调用方式，但是修改了源代码 12345678import timedef index(x,y): start = time.time() time.sleep(3) print(&#x27;index &#123;&#125; &#123;&#125;&#x27;.format(x,y)) stop = time.time() print(&#x27;运行时间为：&#x27;,stop - start)index(111,222) 解决方案二：失败 问题：没有修改被装饰对象的调用方式，也没有修改源代码，增加了新功能，但是代码冗余 12345678910111213141516171819import timedef index(x,y): time.sleep(3) print(&#x27;index &#123;&#125; &#123;&#125;&#x27;.format(x,y))start = time.time()index(111,222)stop = time.time()print(&#x27;运行时间为：&#x27;,stop - start)start = time.time()index(111,222)stop = time.time()print(&#x27;运行时间为：&#x27;,stop - start)start = time.time()index(111,222)stop = time.time()print(&#x27;运行时间为：&#x27;,stop - start) 解决方案三：失败 问题：解决了方案二的代码冗余问题，但带来一个新问题，即改变了函数的调用方式 1234567891011import timedef index(x,y): time.sleep(3) print(&#x27;index &#123;&#125; &#123;&#125;&#x27;.format(x,y))def wrapper(): start = time.time() index(111,222) stop = time.time() print(&#x27;运行时间为：&#x27;, stop - start)wrapper() 方案三的优化（一）：将index的参数写活了 12345678910111213import timedef index(x,y): time.sleep(3) print(&#x27;index &#123;&#125; &#123;&#125;&#x27;.format(x,y))def wrapper(*args,**kwargs): start = time.time() index(*args,**kwargs) stop = time.time() print(&#x27;运行时间为：&#x27;, stop - start)wrapper(111,222)# 说明：只是将index的参数写活了，但是index还是写死的 方案三的优化（二）：在优化一的基础上把被装饰对象写活了，原来只能装饰index 1234567891011121314151617import timedef index(x,y): time.sleep(3) print(&#x27;index &#123;&#125; &#123;&#125;&#x27;.format(x,y))def outter(func): # func = index的内存地址 def wrapper(*args,**kwargs): start = time.time() func(*args,**kwargs) # index的内存地址 stop = time.time() print(&#x27;运行时间为：&#x27;, stop - start) return wrapperindex = outter(index) # index = outter(index的内存地址)# 现在的index = 当初那个wrapper函数的内存地址index(x=3,y=5) 方案三的优化（三）：将wrapper做的跟被装饰对象一摸一样，以假乱真 1234567891011121314151617181920212223242526272829import timedef index(x,y): time.sleep(3) print(&#x27;index &#123;&#125; &#123;&#125;&#x27;.format(x,y))def home(name): time.sleep(3) print(&#x27;welcome &#123;&#125; to home page&#x27;.format(name)) return 123# 装饰器def outter(func): # func = index的内存地址 def wrapper(*args,**kwargs): start = time.time() res = func(*args,**kwargs) # index的内存地址 stop = time.time() print(&#x27;运行时间为：&#x27;, stop - start) return res return wrapperindex = outter(index) # index = outter(index的内存地址)# 现在的index = 当初那个wrapper函数的内存地址index(x=3,y=5)home = outter(home) # index = outter(heme的内存地址)# 现在的home = 当初那个wrapper函数的内存地址res = home(&#x27;zhangsan&#x27;)print(res) 大方向：在方案三的基础上不改变函数调用方式 3.2 语法糖让你开心的语法 （糖衣语法） 1234567891011121314151617181920212223242526import time# 装饰器def outter(func): # func = index的内存地址 def wrapper(*args,**kwargs): start = time.time() res = func(*args,**kwargs) # index的内存地址 stop = time.time() print(&#x27;运行时间为：&#x27;, stop - start) return res return wrapper# 在被装饰对象正上方的单独一行写@装饰器的名字@outter # 等同于 index = outter(index)def index(x,y): time.sleep(3) print(&#x27;index &#123;&#125; &#123;&#125;&#x27;.format(x,y))@outterdef home(name): time.sleep(3) print(&#x27;welcome &#123;&#125; to home page&#x27;.format(name)) return 123index(x=3,y=5) 思考题：叠加多个装饰器的加载顺序与运行顺序。 1234567&#x27;&#x27;&#x27;@deco1 # index=deco1(deco2.wrapper的内存地址)@deco2 # deco2.wrapper的内存地址=deco2(deco3.wrapper的内存地址)@deco3 # deco3.wrapper的内存地址=deco3(index)def index(): pass&#x27;&#x27;&#x27; 3.3 总结无参装饰器模版1234567def outter(func): def wrapper(*args,**kwargs): # 1、调用原函数 # 2、为其增加新功能 res = func(*args,**kwargs) return res return wrapper 3.4 @的含义123456@print # @的含义：home = print(home)def home(a,b,c): pass# 说明：# @名字 等于 下方函数名字=名字（下方函数名字），上面为示例 3.5 装饰器补充之wraps12345678910111213141516171819202122from functools import wrapsdef outter(func): @wraps(func) # 作用就是把下面手动要实现的原函数属性赋给了wrapper def wrapper(*args, **kwargs): res = func(*args, **kwargs) # res = index(1,2) return res # 将原函数的属性赋值给wrapper函数 # 手动：（弊端是如果有100个要写一百行） # wrapper.__name__ = func.__name__ # wrapper.__doc__ = func.__doc__ return wrapper@outterdef index(x,y): &#x27;&#x27;&#x27;这个是主页功能&#x27;&#x27;&#x27; print(x,y)index(1,2)# 偷梁换柱：即 将原函数名指向的内存地址偷梁换柱wrapper函数# 所以：应该将wrapper做的和原函数一样 3.6 有参装饰器》 了解无参装饰器的实现原理后，我们可以再实现一个用来为被装饰对象添加认证功能的装饰器，实现的基本形式如下 3.6.1 前提-知识储备123456789101112131415161718# 一、知识储备# 由于语法糖@的限制，outter函数只能有一个参数，并且该参数只是用来接收被装饰对象的内存地址def outter(func): def wrapper(*args, **kwargs): res = func(*args, **kwargs) return res return wrapper# @outter # index=outter(index) # index =&gt; wrapper@outter # outter(index)def index(x,y): print(x,y)# 偷梁换柱之后# index的参数是什么样子，wrapper的参数就应该什么样子# index的返回值是什么样子，wrapper的返回值就应该什么样子# index的属性是什么样子，wrapper的属性就应该什么样子 ==&gt; from functools import wraps 3.6.2 有参装饰器实现 核心就是在无参装饰器的基础之上，在外面再包一层函数，也就是一共包三层，最外层的函数传参，不会收到语法糖的限制。 1234567891011121314151617181920212223242526272829303132333435363738# 有参装饰器def auth(db_type): def deco(func): def wrapper(*args, **kwargs): name = input(&#x27;your name=&gt;：&#x27;).strip() password = input(&#x27;your password=&gt;：&#x27;).strip() if db_type == &#x27;file&#x27;: print(&#x27;基于文件的验证&#x27;) if name == &#x27;zhangsan&#x27; and password == &#x27;123&#x27;: res = func(*args, **kwargs) return res else: print(&#x27;user or password error&#x27;) elif db_type == &#x27;mysql&#x27;: print(&#x27;基于mysql的验证&#x27;) elif db_type == &#x27;ldap&#x27;: print(&#x27;基于ldap的验证&#x27;) else: print(&#x27;不支持该db_tpye&#x27;) return wrapper return deco@auth(db_type=&#x27;file&#x27;) # @deco # index=deco(index) # index=wrapperdef index(x,y): print(&#x27;index --&gt; %s:%s&#x27; %(x,y))@auth(db_type=&#x27;mysql&#x27;) # @deco # home=deco(home) # home=wrapperdef home(name): print(&#x27;home --&gt; %s&#x27; %name)@auth(db_type=&#x27;ldap&#x27;) # 账号密码的来源是ldapdef transfar(): print(&#x27;transfar&#x27;)index(1,2)home(&#x27;zhangsan&#x27;)transfar() 3.7 有参装饰器模版总结12345678910111213# 总结有参装饰器模版def 有参装饰器(x,y,z): def outter(func): def wrapper(*args, **kwargs): res = func(*args, **kwargs) return res return wrapper return outter@有参装饰器(1,2,z=3)def 被装饰对象(): pass","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"函数对象和闭包","slug":"hanshuduixianghebibao","date":"2023-03-30T11:20:20.000Z","updated":"2023-03-30T11:20:20.000Z","comments":true,"path":"articles/2023/03/30/hanshuduixianghebibao/","permalink":"https://kkabuzs.github.io/articles/2023/03/30/hanshuduixianghebibao/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 14、函数对象和闭包特别鸣谢： — b站老男孩agen老师 一，函数对象 函数对象指的是函数可以被当做’数据’来处理，具体可以分为四个方面的使用，我们如下 1.1 函数可以被引用123456789# 精髓：可以把函数对象当成变量去用# func = 内存地址def func(): print(&#x27;from func&#x27;)# 1、可以赋值f = funcprint(f,func) # &lt;function func at 0x106692d30&gt; &lt;function func at 0x106692d30&gt;f() # from func 1.2 可以当作函数的参数传入12345def foo(x): # x = func的内存地址 print(x) # &lt;function func at 0x10e4bad30&gt; x() # from funcfoo(func) # foo(func的内存地址) 1.3 可以把函数当作另一个函数的返回值1234567def foo1(x): return xres = foo1(func)# foo1(func()) # func()加了括号就代表把func的运行结果的返回值给了foo1，而不加括号等于把这个函数的内存地址给了foo1print(res) # &lt;function func at 0x1067dbd30&gt;res() # from func 1.4 可以当作容器类型的一个元素1234567891011# 在列表中：l=[func,]# 说明：如何取到func，按照列表的取法 l[0]就是取到了func的内存地址，加了括号就是调用l[0]()lis = l[0]print(lis) # &lt;function func at 0x10bebbdc0&gt;l[0]() # from func# 在字典中dic = &#123;&#x27;k1&#x27;:func,&#125;print(dic) # &#123;&#x27;k1&#x27;: &lt;function func at 0x10a1b6d30&gt;&#125;dic[&#x27;k1&#x27;]() # from func # 调用就是加() 1.5 函数对象案例应用：ATM提款机程序优化12345678910111213141516171819202122232425262728293031def login(): print(&#x27;登录功能&#x27;)def transfer(): print(&#x27;转账功能&#x27;)def check_banlance(): print(&#x27;查询余额&#x27;)def withdraw(): print(&#x27;提现功能&#x27;)atm_dic = &#123; &#x27;0&#x27;:[&#x27;退出&#x27;,None], &#x27;1&#x27;:[&#x27;登录&#x27;,login], &#x27;2&#x27;:[&#x27;转账&#x27;,transfer], &#x27;3&#x27;:[&#x27;查询&#x27;,check_banlance], &#x27;4&#x27;:[&#x27;提现&#x27;,withdraw]&#125;while True: for i in atm_dic.keys(): print(&#x27;=&#x27;*5,i,atm_dic[i][0],&#x27;=&#x27;*5) regs = input(&#x27;请输出功能编号：&#x27;).strip() if not regs.isdigit(): print(&#x27;只能输入编号&#x27;) continue if regs == &#x27;0&#x27;: break if regs in atm_dic: atm_dic[regs][1]() else: print(&#x27;功能编号不存在&#x27;) 二，函数看套2.1 在调用一个函数的过程中又调用其他函数123456789101112131415161718# 1、函数的嵌套调用：在调用一个函数的过程中又调用其他函数def max2(x,y): if x &gt; y: return x else: return ydef max4(a,b,c,d): # 第一步：比较a，b得到res1 res1 = max2(a,b) # 第二步：比较res1，c得到res2 res2 = max2(res1,c) # 第三步：比较res2，d得到res3 res3 = max2(res2,d) return res3res = max4(1,2,3,4)print(res) # 4 2.2 在函数内定义其他函数12345678910111213141516171819202122232425# 2、函数的嵌套定义：在函数内定义其他函数def f1(): def f2(): pass# 圆形案例：# 求圆形的周长：2*pi*radius# 求圆形的面积：pi*(radius**2)def circle(radius,action=0): from math import pi def perimiter(radius): return 2*pi*radius def area(radius): return pi*(radius**2) if action == 0: return perimiter(radius) elif action == 1: return area(radius)rea1 = circle(33,action=1)print(rea1) 三，闭包函数 大前提：闭包函数&#x3D;名称空间与作用域+函数嵌套+函数对象核心点：名字的查找关系是以函数定义阶段为准 3.1 什么是闭包函数&quot;闭&quot;函数指的是该函数是内嵌函数 &quot;包&quot;函数指的是该函数包含对外层函数作用域名字的引用（不是全局作用域） 案例：闭包函数：名称空间与作用域+函数嵌套 1234567891011121314def f1(): x = 11111 def f2(): print(x) f2()x = 22222def bar(): x = 44444 f1()def foo(): x = 55555 bar()foo() # 11111 案例：闭包函数：函数对象 12345678def f3(): x = 3333333333 def f4(): print(x) return f4f=f3()print(f) # &lt;function f3.&lt;locals&gt;.f4 at 0x108c17e50&gt;f() # 3333333333 3.2 为何要有闭包函数 –&gt; 闭包函数的应用 两种为函数体传参的方式 1234567# 方式一：直接把函数体需要的参数定义成形参def func(x): print(x)func(1)func(2)# 方式二：利用上面函数对象的方式 3.3 闭包函数的应用123456789101112131415161718192021222324import requests# 1、传参的方案一：def get(url): response = requests.get(url) print(response.text)get(&#x27;https://www.baidu.com&#x27;)get(&#x27;https://blog.csdn.net&#x27;)get(&#x27;https://www.zhihu.com&#x27;)# 2、传参的方案二：def outter(url): # 在这里添加形参 # url = &#x27;https://www.baidu.com&#x27; # 这里写死了，所以注释 def get1(): response = requests.get(url) print(response.text) return get1baidu = outter(&#x27;https://www.baidu.com&#x27;)baidu()CSDN = outter(&#x27;https://blog.csdn.net&#x27;)CSDN()zhihu = outter(&#x27;https://www.zhihu.com&#x27;)zhihu()","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"名称空间与作用域","slug":"mingchengkongjianyuzuoyongyu","date":"2023-03-29T03:13:13.000Z","updated":"2023-03-29T03:13:13.000Z","comments":true,"path":"articles/2023/03/29/mingchengkongjianyuzuoyongyu/","permalink":"https://kkabuzs.github.io/articles/2023/03/29/mingchengkongjianyuzuoyongyu/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 13、名称空间与作用域特别鸣谢： — b站老男孩agen老师 一，名称空间 名称空间即存放名字与对象映射&#x2F;绑定关系的地方。对于x&#x3D;3，Python会申请内存空间存放对象3，然后将名字x与3的绑定关系存放于名称空间中，del x表示清除该绑定关系。 在程序执行期间最多会存在三种名称空间 1.1 内建名称空间 伴随python解释器的启动&#x2F;关闭而产生&#x2F;回收，因而是第一个被加载的名称空间，用来存放一些内置的名字，比如内建函数名 存放python解释器内置的名字 123456&gt;&gt;&gt; print&lt;built-in function print&gt;&gt;&gt;&gt; input&lt;built-in function input&gt;存活周期：python解释器启动则产生，python解释器关闭则销毁 1.2 全局名称空间 只要不是函数内定义、也不是内置的都是全局名称空间 12345678910111213# 存活周期：python文件执行产生，python文件运行完毕销毁import osx = 10def func(): passclass Foo: pass# os、x、func、Foo都是全局名称空间 1.3 局部名称空间 在调用函数时，运行函数体代码过程中产生的函数内的名字 12345678910# 存活周期：在调用函数是存活，函数低哦啊用完毕后则销毁def func1(): x = 10 y = 20func1()# 其中func1是全局名称空间，函数内的x、y是局部名称空间。同一个函数多次调用，调用一次产生一次局部名称空间，调四次产生四次。 1.4 名称空间的加载顺序 内置名称空间 -&gt; 全局名称空间 -&gt; 局部名称空间 1.5 销毁顺序 局部名称空间 -&gt; 全局名称空间 -&gt; 内置名称空间 1.6 名字的查找优先级当前所在的位置向上一层一层查找 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&#x27;&#x27;&#x27;内置名称空间全局名称空间局部名称空间# 如果当前在局部名称空间：局部名称空间 -&gt; 全局名称空间 -&gt; 内置名称空间# 如果当前在全局名称空间：全局名称空间 -&gt; 内置名称空间&#x27;&#x27;&#x27;# 示范1：def funca(): print(x)x = 111funca() # 111# 示范2：名称空间的&quot;嵌套&quot;关系是以函数定义节点为准，与调用位置无关a = 1def funca1(): print(x)def foo(): x = 222 funca1()foo() # 1# 示范3：函数嵌套定义input = 111def f1(): input = 222 def f2(): input=333 print(input) f2()f1() # 333# 说明：结果为333，注释f2函数内input变量，结果为222，在注释f1内input变量，结果为111，如果都注释，结果为&lt;built-in function input&gt;# 示范4：b = 111def b1(): print(b) b = 222b1() # 报错，UnboundLocalError: local variable &#x27;b&#x27; referenced before assignment# 分析说明：因为print输出b的值，是在定义阶段查找，定义阶段在b1函数内定义了b的值，所以在局部名称空间找到了，就不会再去全局名称空间去找，但是写在了print输出的下面，所以报错。 二，作用域12345678# 作用域 -&gt; 作用范围# 全局作用域：内置名称空间、全局名称空间# 1、全局存活# 2、全局有效：被所有函数共享（不同函数且没有相互调用时，不同函数之间相互独立）# 局部作用域：局部名称空间的名字# 1、临时存活# 2、局部有效：函数内有效 2.1 global与nonlocalglobal 123456789101112131415161718192021222324252627# 示范1：x = 111def func(): x = 222func()print(x) # 111# 示范2：如果在局部想要修改全局的名字对应的值（不可变类型），需要用到globalx = 111def func(): global x # 声明x为全局名称空间的名字 x = 222func()print(x) # 222# 示范3：l = [111,222]def func1(): l.append(333)func1()print(l) # [111, 222, 333] nonlocal（了解） 修改函数外层函数包含的名字对应的值（不可变类型） 对于嵌套多层的函数，使用nonlocal关键字可以将名字声明为来自外部嵌套函数定义的作用域（非全局） 12345678910111213# 示范：a = 0def f1(): x=11 def f2(): nonlocal x x = 22 f2() # 调用f2(),修改f1作用域中名字x的值 print(&#x27;f1内的x&#x27;,x) # f1内的x 22f1()# nonlocal x会从当前函数的外层函数开始一层层去查找名字x，若是一直到最外层函数都找不到，则会抛出异常。","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"函数的参数","slug":"hanshudecanshu","date":"2023-03-27T06:26:44.000Z","updated":"2023-03-27T06:26:44.000Z","comments":true,"path":"articles/2023/03/27/hanshudecanshu/","permalink":"https://kkabuzs.github.io/articles/2023/03/27/hanshudecanshu/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 12、函数的参数特别鸣谢： — b站老男孩agen老师 一，形参与实参介绍123456#函数的参数分为形式参数和实际参数，简称形参和实参：#形参：再定义函数阶段定义的参数称之为形式参数，简称形参，相当于变量def func(x,y): print(x,y)#实参：在调用函数阶段传入的值称之为实际参数，简称实参func(1,2) 1234567891011121314151617# 形参与实参的关系：# 1、在调用阶段，实参（变量值）的值会绑定给形参（变量名）# 2、这种绑定关系只能在函数体内使用# 3、实参与形参的绑定关系在函数调用时生效，函数调用结束后解除绑定关系# 实参是传入的值，但值可以是以下形式# 形式一：func(1,2)# 形式二：a=1b=2func(a,b)# 形式三：func(int(&#x27;1&#x27;),2)# func(func1(1,2,),func2(2,3),333) 二，形参与实参的具体使用2.1 位置参数按照从左到右的顺序依次定义的参数称之为位置参数 1234567# 位置形参：按照从左到右的顺序直接定义的&quot;变量名&quot;# 特点：必须被传值，多一个不行，少一个也不行def func1(x,y): print(x,y)# 位置实参：按照从左到右的顺序依次传入的值func1(1,2) 2.2 关键字参数123456789101112131415# 关键字实参：在函数调用阶段，按照key=value的形式传入的值# 特点：指名道姓给某个形参传值，可以完全不参照顺序。def func2(x,y): print(x,y)func2(y=2,x=1)# 混合使用，强调# 1、位置实参必须放在关键字实参前func2(5,y=2)# func2(y=1,2) # 报错# 2、不能为同一个形参重复传值# func2(5,x=3) # 报错# func2(1,2,x=3,y=4) # 报错，原理同上 2.3 默认参数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 默认形参：再定义函数阶段，就已经被赋值的形参，称之为默认参数/默认形参# 特点：再定义阶段就已经被赋值，意味着在调用节点可以不用为其赋值def func3(x,y=3): print(x,y)func3(1) # 1 3func3(x=1,y=444) # 1 444# 位置形参与默认形参混用，强调# 1、位置形参必须在默认形参的左边def test(x,y=2): pass# def test1(y=2,x): # 报错# pass# 2、默认参数的值是在函数定义阶段被赋值的，准确的说被赋予的是值的内存地址# 示范1：m = 2def func4(x,y=m): # y =&gt; 2的内存地址 print(x,y)m = 333333func4(1) # 1 2# 示范2：n = [1111,]def func5(x,y=n): # y =&gt; [1111,]的内存地址 print(x,y)n.append(&#x27;22222&#x27;)func5(1) # 1 [1111, &#x27;22222&#x27;]# 3、虽然默认值可以被指定为任意数据类型，但是不推荐使用可变类型# 函数的规范：函数的调用只跟函数本身有关系，不被外界代码所影响def func6(x,y,z,l=None): if l == None: l = [] l.append(x) l.append(y) l.append(z) print(l)func6(1,2,3) # [1, 2, 3]func6(4,5,6) # [4, 5, 6]new_l = [111,222]func6(1,2,3,new_l) # [111, 222, 1, 2, 3] 2.4 可变长度的参数（*与**的用法）说明：可变长度指的是在调用函数时，传入的值（实参）的个数不固定 而实参是用来为形参赋值的，所以对应着，针对溢出的实参必须有对应的形参来接收 2.4.1 可变长度的位置参数123456789101112131415161718192021222324252627282930313233# I：形参格式（*形参名）：用来接收溢出的位置实参，溢出的位置实参会被*保存成元组的格式，然后赋值给紧跟其后的形参名。# *后跟的可以是任意名字，但是约定俗成应该是argsdef funca(x,y,*z): # z = (3, 4, 5, 6) print(x,y,z)funca(1,2,3,4,5,6) # 1 2 (3, 4, 5, 6)# 小案例：传值累加求和，不限制传值数量def my_sum(*args): res = 0 for i in args: res += i return resres = my_sum(1,2,3,4,5,6)print(res) # 21# II：*可以用在实参中，实参中带*，先将*后的值打散成位置实参def funcb(x,y,z): print(x,y,z)funcb(*[11,22,33]) # 11 22 33l = [1,2,3]funcb(*l) # 1 2 3# III：形参与实参中都带*def funcc(x,y,*args): print(x,y,args)funcc(1,2,[3,4,5,6]) # 1 2 ([3, 4, 5, 6],)funcc(1,2,*[3,4,5,6]) # 1 2 (3,4,5,6) # 等同于funcc(1,2,3,4,5,6)funcc(*&#x27;hello&#x27;) # h e (&#x27;l&#x27;, &#x27;l&#x27;, &#x27;o&#x27;) 2.4.2 可变长度的关键字参数1234567891011121314151617181920212223242526272829303132333435363738394041424344# I：形参格式（**形参名）：用来接收溢出的关键字实参，**会将溢出的关键字实参保存成字典格式，然后赋值给紧跟其后的形参名。# **后跟的可以是任意名字，但是约定俗成应该是kwargsdef funca1(x,y,**kwargs): print(x,y,kwargs)funca1(1,y=2,a=1,b=2,c=3) # 1 2 &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125;# II：**可以用在实参中（**后跟的只能是字典），实参中带**，先将**后的值打散成关键字实参def funca2(x,y,z): print(x,y,z)# 数量要和形参对应，否则报错funca2(*&#123;&#x27;x&#x27;:1,&#x27;y&#x27;:2,&#x27;z&#x27;:1&#125;) # x y z # 等同于funca2(&#x27;x&#x27;,&#x27;y&#x27;,&#x27;z&#x27;)funca2(**&#123;&#x27;x&#x27;:1,&#x27;y&#x27;:2,&#x27;z&#x27;:1&#125;) # 1 2 1 # 等同于funca2(x=1,y=2,z=1)# III：形参与实参中都带**def funca3(x,y,**kwargs): print(x,y,kwargs)# funca3(y=222,x=111,a=333,b=444)funca3(**&#123;&#x27;y&#x27;:222,&#x27;x&#x27;:111,&#x27;a&#x27;:333,&#x27;b&#x27;:444&#125;) # 111 222 &#123;&#x27;a&#x27;: 333, &#x27;b&#x27;: 444&#125;# 混用*与**# *args必须在**kwargs之前def aaa(*args,**kwargs): print(args) print(kwargs)aaa(1,2,3,4,5,6,7,8,x=1,y=2,c=3)&#x27;&#x27;&#x27;输出：(1, 2, 3, 4, 5, 6, 7, 8)&#123;&#x27;x&#x27;: 1, &#x27;y&#x27;: 2, &#x27;c&#x27;: 3&#125;&#x27;&#x27;&#x27;def index(x,y): print(&#x27;====&gt;&#x27;,x,y) # ====&gt; 1 2def wrapper(a,b): index(a,b) # wrapper函数是在给index函数传值wrapper(1,2) 2.5 命名关键字参数12345678910111213141516# 再定义函数时，*后定义的参数，如下所示，称之为命名关键字参数# 特点：# 01、命名关键字实参必须按照key=value的形式为其传值def func(x,y,*,a,b): # 其中，a和b称之为命名关键字参数 print(x,y) print(a,b)func(1,2,a=3,b=4)# 示例：def func(x,y,*,a=111,b): # 解析：给a这个命名关键字参数赋了个值，所以无报错，并不是默认形参。 print(x,y) print(a,b)func(1,2,b=4) 2.6 组合使用1234567891011121314151617181920# 位置形参，默认形参，*args，命名关键字形参，**kwargs（不按这个顺序，会语法错误）def func1(x,y=111,*args,z,**kwargs): print(x) print(y) print(args) print(z) print(kwargs)# 实参混用顺序def func2(x,y,z,a,b,c): print(x) print(y) print(z) print(a) print(b) print(c)# func2(1,y=2,*[333,444],**&#123;&#x27;b&#x27;:555,&#x27;c&#x27;:666&#125;) # 报错，打散之后 y=2关键字实参在位置实参333，444的前面func2(1,*[333,444],a=2,**&#123;&#x27;b&#x27;:555,&#x27;c&#x27;:666&#125;)","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"函数的基本使用","slug":"hanshu","date":"2023-03-27T02:30:15.000Z","updated":"2023-03-27T02:30:15.000Z","comments":true,"path":"articles/2023/03/27/hanshu/","permalink":"https://kkabuzs.github.io/articles/2023/03/27/hanshu/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 11、函数的基本使用特别鸣谢： — b站老男孩agen老师 一，定义函数 函数的使用必须遵循’先定义，后调用’的原则。函数的定义就相当于事先将函数体代码保存起来，然后将内存地址赋值给函数名，函数名就是对这段代码的引用，这和变量的定义是相似的。没有事先定义函数而直接调用，就相当于在引用一个不存在的’变量名’。 定义函数的语法 123456789101112131415161718192021222324# 定义函数发生的事情：# 1、申请内存空间保存函数体代码# 2、将上述内存地址绑定给函数名# 3、定义函数不会执行函数体代码，但是会检测函数体语法# 调用函数发生的事情# 1、通过函数名找到函数的内存地址# 2、然后加括号就是在触发函数题代码的执行def 函数名(参数1,参数2,...): &quot;&quot;&quot;文档描述&quot;&quot;&quot; 函数体 return 值---# def: 定义函数的关键字；# 函数名：函数名指向函数内存地址，是对函数体代码的引用。函数的命名应该反映出函数的功能；# 括号：括号内定义参数，参数是可有可无的，且无需指定参数的类型；# 冒号：括号后要加冒号，然后在下一行开始缩进编写函数体的代码；# &quot;&quot;&quot;文档描述&quot;&quot;&quot;: 描述函数功能，参数介绍等信息的文档，非必要，但是建议加上，从而增强函数的可读性；# 函数体：由语句和表达式组成；# return 值：定义函数的返回值，return是可有可无的。 参数是函数的调用者向函数体传值的媒介，若函数体代码逻辑依赖外部传来的参数时则需要定义为参函数， 12345def funn(x,y): # x=1 y=2 print(x,y)funn(1,2) 否则定义为无参函数 12def func(): print(&#x27;hahahaha&#x27;) 函数体为pass代表什么都不做，称之为空函数。定义空函数通常是有用的，因为在程序设计的开始，往往是先想好程序都需要完成什么功能，然后把所有功能都列举出来用pass充当函数体“占位符”，这将使得程序的体系结构立见，清晰且可读性强。例如要编写一个ftp程序，我们可能想到的功能有用户认证，下载，上传，浏览，切换目录等功能，可以先做出如下定义： 12345678910111213141516171819def auth_user(): &quot;&quot;&quot;user authentication function&quot;&quot;&quot; passdef download_file(): &quot;&quot;&quot;download file function&quot;&quot;&quot; passdef upload_file(): &quot;&quot;&quot;upload file function&quot;&quot;&quot; passdef ls(): &quot;&quot;&quot;list contents function&quot;&quot;&quot; passdef cd(): &quot;&quot;&quot;change directory&quot;&quot;&quot; pass 之后我们便可以统筹安排编程任务，有选择性的去实现上述功能来替换掉pass，从而提高开发效率。 二，调用函数与函数返回值2.1 调用函数 函数的使用分为定义阶段与调用阶段，定义函数时只检测语法，不执行函数体代码，函数名加括号即函数调用，只有调用函数时才会执行函数体代码 12345678910111213141516#定义阶段def bar(): print(&#x27;from bar&#x27;)def foo(): print(bar) # &lt;function bar at 0x10738bdc0&gt; bar() # from bar print(&#x27;from foo&#x27;) # from foo#调用阶段foo()# 执行结果&lt;function bar at 0x10ac2ce50&gt;from barfrom foo 定义阶段函数foo与bar均无语法错误，而在调用阶段调用foo()时，函数foo与bar都早已经存在于内存中了，所以不会有任何问题。 按照在程序出现的形式和位置，可将函数的调用形式分为三种 1234567891011121314151617181920212223# 1、语句的形式：只加括号调用函数foo()add(1,2)# 2、表达式形式def add(x,y): # 参数 =》 原材料 # x = 20 # y = 30 res = x+y # print(res) return res # 返回值 =》 产品# 赋值表达式res = add(1,6)print(res)# 数学表达式res = add(1,2)*10 # 先执行函数得出结果3 再乘10 最终结果30print(res)# 3、函数调用可以当作参数res = add(add(1,2),10) # 等同于add(3,10)print(res) 2.2 函数返回值1234567891011121314151617# return是函数体代码结束的标志，即函数体代码一旦运行到return会立刻终止函数的运行，并且会将return后的值当作本次运行的结果返回:# 1、返回None：函数体内没有return# return# return None# 2、返回一个值：return 值def fu1(): return 10re = fu1()print(re)# 3、返回多个值：用逗号分隔开多值，会被return返回成元组def fu2(): return 10,&#x27;aa&#x27;,[1,2]re1 = fu2()print(re1,type(re1)) # (10, &#x27;aa&#x27;, [1, 2]) &lt;class &#x27;tuple&#x27;&gt; return是一个函数结束的标志,函数内可以有多个return，但只执行一次函数就结束了，并把return后定义的值作为本次调用的结果返回。","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"文件处理","slug":"wenjianchuli","date":"2023-03-21T06:38:21.000Z","updated":"2023-03-21T06:38:21.000Z","comments":true,"path":"articles/2023/03/21/wenjianchuli/","permalink":"https://kkabuzs.github.io/articles/2023/03/21/wenjianchuli/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 10、文件处理特别鸣谢： — b站老男孩agen老师 一，引入 应用程序运行过程中产生的数据最先都是存放于内存中的，若想永久保存下来，必须要保存于硬盘中。应用程序若想操作硬件必须通过操作系统，而文件就是操作系统提供给应用程序来操作硬盘的虚拟概念，用户或应用程序对文件的操作，就是向操作系统发起调用，然后由操作系统完成对硬盘的具体操作. 二，文件操作的基本流程2.1 基本流程有了文件的概念，我们无需再去考虑操作硬盘的细节，只需要关注操作文件的流程： 12345678# 1. 打开文件，由应用程序向操作系统发起系统调用open(...)，操作系统打开该文件，对应一块硬盘空间，并返回一个文件对象赋值给一个变量ff=open(&#x27;a.txt&#x27;,&#x27;r&#x27;,encoding=&#x27;utf-8&#x27;) #默认打开模式就为r# 2. 调用文件对象下的读/写方法，会被操作系统转换为读/写硬盘的操作data=f.read()# 3. 向操作系统发起关闭文件的请求，回收系统资源f.close() 2.2 资源回收与with上下文管理 打开一个文件包含两部分资源：应用程序的变量f和操作系统打开的文件。在操作完毕一个文件时，必须把与该文件的这两部分资源全部回收，回收方法为： 121、f.close() #回收操作系统打开的文件资源2、del f #回收应用程序级的变量 其中del f一定要发生在f.close()之后，否则就会导致操作系统打开的文件无法关闭，白白占用资源， 而python自动的垃圾回收机制决定了我们无需考虑del f，这就要求我们，在操作完毕文件后，一定要记住f.close()，虽然我们如此强调，但是大多数读者还是会不由自主地忘记f.close()，考虑到这一点，python提供了with关键字来帮我们管理上下文 12345678910# 1、在执行完子代码块后，with 会自动执行f.close()with open(&#x27;a.txt&#x27;,mode=&#x27;rt&#x27;) as f1: # f1=open(&#x27;a.txt&#x27;,mode=&#x27;rt&#x27;) pass# 2、可用用with同时打开多个文件，用逗号分隔开即可with open(&#x27;a.txt&#x27;,mode=&#x27;rt&#x27;) as f1,open(&#x27;b.txt&#x27;,mode=&#x27;rt&#x27;) as f2: res1 = f1.read() res2 = f2.read() print(res1) print(res2) 2.3 指定操作文本文件的字符编码12345678910111213141516171819&#x27;&#x27;&#x27;强调：t和b不能单独使用，必须跟r/w/a连用 t（文本模式） 1、读写都是以str（unicode）位单位的 2、文本文件 3、必须指定encoding=&#x27;utf-8&#x27;&#x27;&#x27;&#x27;# 没有指定encoding参数，操作系统会使用自己默认的编码# linux/mac系统默认utf-8# windows系统默认gbkwith open(&#x27;c.txt&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f: res = f.read() # t模式会将f.read()读出的结果解码成unicode print(type(res)) # &lt;class &#x27;str&#x27;&gt;# 内存：utf-8格式的二进制 ------ 解码 ------&gt; unicode# 硬盘（c.txt内容：utf-8格式的二进制） 三，文件的操作模式3.1 控制文件读写操作的模式123r（默认的）：只读模式w：只写模式a：只追加写模式 3.1.1 案例一：r 模式的使用12345678910111213141516171819# 以t模式为基础进行内存操作# 1、r（默认的操作模式）：只读模式，当文件不存在时会报错，当文件存在时，文件指针跳到开始位置with open(&#x27;c.txt&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f: print(&#x27;第一次读&#x27;.center(50,&#x27;*&#x27;)) res = f.read() # 把所有内容从硬盘读入内存 print(res) print(&#x27;第二次读&#x27;.center(50,&#x27;*&#x27;)) res1 = f.read() print(res1)输出：***********************第一次读***********************哈哈哈哈***********************第二次读***********************说明：第二次读内容是空的，因为文件指针在文件结尾，除非重新open文件，使文件指针跳到文件开始，才可以继续读出内容 案例应用 1234567891011121314151617#此时user.txt内容：zhangsan:123---inp_username = input(&#x27;your name: &#x27;).strip()inp_password = input(&#x27;your password: &#x27;).strip()with open(&#x27;user.txt&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as ff: ress = ff.read() username,password = ress.split(&#x27;:&#x27;) print(ress,type(ress))if inp_username == username and inp_password == password: print(&#x27;login seccessfull&#x27;)else: print(&#x27;username or password error&#x27;) 优化：多个账号密码的情况 12345678910111213141516171819#此时user.txt内容：zhangsan:123lisi:456wangwu:789---inp_username = input(&#x27;your name: &#x27;).strip()inp_password = input(&#x27;your password: &#x27;).strip()with open(&#x27;user.txt&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as ff: for line in ff: # 把用户输入的名字与密码与读出内容做比对 username,password=line.strip().split(&#x27;:&#x27;) if inp_username == username and inp_password == password: print(&#x27;login seccessfull&#x27;) break else: print(&#x27;username or password error&#x27;) 3.1.2 案例二：w 模式的使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152w：只写模式，当文件不存在时会创建空文件，当文件存在时会清空文件，指针位于开始位置。with open(&#x27;d.txt&#x27;,mode=&#x27;wt&#x27;,encoding=&#x27;utf-8&#x27;) as fw: # f.read() # 报错，不可读 fw.write(&#x27;哈哈哈我擦嘞&#x27;)&#x27;&#x27;&#x27;强调1：在以w模式打开文件没有关闭的情况下，连续的写，后写的内容一定跟在前写内容的后面&#x27;&#x27;&#x27;with open(&#x27;d1.txt&#x27;, mode=&#x27;wt&#x27;, encoding=&#x27;utf-8&#x27;) as fw1: fw1.write(&#x27;哈哈哈我擦嘞\\n&#x27;) fw1.write(&#x27;哈哈哈我擦嘞\\n&#x27;) fw1.write(&#x27;哈哈哈我擦嘞\\n&#x27;)# d1.txt文件内容：哈哈哈我擦嘞哈哈哈我擦嘞哈哈哈我擦嘞&#x27;&#x27;&#x27;强调2：如果重新以w模式打开文件，则会清空文件内容&#x27;&#x27;&#x27;with open(&#x27;d2.txt&#x27;, mode=&#x27;wt&#x27;, encoding=&#x27;utf-8&#x27;) as fw2: fw2.write(&#x27;哈哈哈我擦嘞\\n&#x27;)with open(&#x27;d2.txt&#x27;, mode=&#x27;wt&#x27;, encoding=&#x27;utf-8&#x27;) as fw2: fw2.write(&#x27;哈哈哈我擦嘞\\n&#x27;)with open(&#x27;d2.txt&#x27;, mode=&#x27;wt&#x27;, encoding=&#x27;utf-8&#x27;) as fw2: fw2.write(&#x27;哈哈哈我擦嘞\\n&#x27;)# d2.txt文件内容：哈哈哈我擦嘞---# w模式用来创建全新的文件# 案例：文件的copy工具with open(&#x27;e.txt&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f1, \\ open(&#x27;f.txt&#x27;,mode=&#x27;wt&#x27;,encoding=&#x27;utf-8&#x27;) as f2 : res = f1.read() f2.write(res)# 优化：src_file = input(&#x27;源文件的绝对路径：&#x27;).strip()dst_file = input(&#x27;目标存放绝对路径：&#x27;).strip()with open(r&#x27;&#123;&#125;&#x27;.format(src_file),mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f1, \\ open(r&#x27;&#123;&#125;&#x27;.format(dst_file),mode=&#x27;wt&#x27;,encoding=&#x27;utf-8&#x27;) as f2 :# res = f1.read() # 数据量大会导致内存压力很大，甚至内存溢出# f2.write(res) for line in f1: f2.write(line) 3.1.3 案例三：a 模式的使用12345678910111213141516171819202122232425262728293031323334a：只追加写，当文件不存在时，会创建空文档，在文件存在时，文件指针会直接跳到末尾。with open(&#x27;e.txt&#x27;,mode=&#x27;at&#x27;,encoding=&#x27;utf-8&#x27;) as fa: # fa.read() # 报错，不能读 fa.write(&#x27;aaaaa\\n&#x27;)# e.txt文件内容：aaaaawith open(&#x27;e.txt&#x27;,mode=&#x27;at&#x27;,encoding=&#x27;utf-8&#x27;) as fa: fa.write(&#x27;bbbbbb\\n&#x27;) fa.write(&#x27;cccccc\\n&#x27;)# e.txt文件内容：aaaaaaaaaa # 为什么两行aaaaa，因为上面代码也运行了 也会追加写一行bbbbbbcccccc#强调 w 模式与 a 模式的异同：# 1 相同点：在打开的文件不关闭的情况下，连续的写入，新写的内容总会跟在前写的内容之后# 2 不同点：以 a 模式重新打开文件，不会清空原文件内容，会将文件指针直接移动到文件末尾，新写的内容永远写在最后---# a模式用来在原有的文件内容基础之上写入新的内容，比如记录日志，比如注册# 案例：注册功能name = input(&#x27;your name: &#x27;)pwd = input(&#x27;your pwd: &#x27;)with open(&#x27;db.txt&#x27;,mode=&#x27;at&#x27;,encoding=&#x27;utf-8&#x27;) as fa1: fa1.write(&#x27;&#123;&#125;:&#123;&#125;\\n&#x27;.format(name,pwd)) 3.1.4 案例四：+ 模式的使用(了解)12# r+ w+ a+ :可读可写#在平时工作中，我们只单纯使用r/w/a，要么只读，要么只写，一般不用可读可写的模式 3.2 控制文件读写内容的模式12345x：x模式（控制文件操作的模式） --&gt; 了解 1. x，只写模式【不可读；不存在则创建，存在则报错】 # with open(&#x27;a.txt&#x27;,mode=&#x27;x&#x27;,encoding=&#x27;utf-8&#x27;) as f:# pass 3.2.1 案例一：t 模式的使用12345678910# t 模式：如果我们指定的文件打开模式为r/w/a，其实默认就是rt/wt/at with open(&#x27;a.txt&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f: res=f.read() print(type(res)) # 输出结果为：&lt;class &#x27;str&#x27;&gt; with open(&#x27;a.txt&#x27;,mode=&#x27;wt&#x27;,encoding=&#x27;utf-8&#x27;) as f: s=&#x27;abc&#x27; f.write(s) # 写入的也必须是字符串类型 #强调：t 模式只能用于操作文本文件,无论读写，都应该以字符串为单位，而存取硬盘本质都是二进制的形式，当指定 t 模式时，内部帮我们做了编码与解码 3.2.2 案例二： b 模式的使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354大前提: tb模式均不能单独使用,必须与r/w/a之一结合使用&#x27;&#x27;&#x27;控制文件读写内容的模式：t： 1、读写都是以字符串为单位 2、只能针对文本文件 3、必须指定字符编码，即必须指定encoding参数b：binary模式 1、读写都是以bytes为单位 2、可以针对所有文件 3、一定不能指定字符编码，即一定不能指定encoding参数总结：1、在操作纯文本文件方面t模式帮我们省去了编码与解码的环节，b模式则需要手动编码与解码，所以此时t模式更为方便2、针对非文本文件（如图片、视频、音频等）只能使用b模式&#x27;&#x27;&#x27;# 错误演示：t模式只能读文本文件# with open(r&#x27;tupian.png&#x27;,mode=&#x27;rt&#x27;) as f:# f.read() # 硬盘的二进制读入内存 -&gt; t模式会将读入内存的内容进行decode解码操作with open(r&#x27;tupian.png&#x27;,mode=&#x27;rb&#x27;) as f: res = f.read() # 硬盘的二进制读入内存 -&gt; b模式下不做任何转换，直接读入内存 print(res) # bytes类型 -&gt; 当成二进制 print(type(res))---## 文件拷贝工具src_file = input(&#x27;源文件的绝对路径：&#x27;).strip()dst_file = input(&#x27;目标存放绝对路径：&#x27;).strip()with open(r&#x27;&#123;&#125;&#x27;.format(src_file),mode=&#x27;rb&#x27;) as f1, \\ open(r&#x27;&#123;&#125;&#x27;.format(dst_file),mode=&#x27;wb&#x27;) as f2 : # res = f1.read() # 内存压力很大，甚至内存溢出 # f2.write(res) for line in f1: f2.write(line)---### 循环读取文件# 方式一：自己控制每次读取的数据量with open(r&#x27;tupian.png&#x27;,mode=&#x27;rb&#x27;) as f: while True: res = f.read(1024) if len(res) == 0: break print(len(res))# 方式二：以行为单位读，当一行内容过长时会导致一次性读入内存的数据量过大# for循环with open(r&#x27;tupian.png&#x27;,mode=&#x27;rb&#x27;) as f: for line in f: print(line) 四，操作文件的方法4.1 重点1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# 读操作f.read() # 读取所有内容,执行完该操作后，文件指针会移动到文件末尾f.readline() # 读取一行内容,光标移动到第二行首部f.readlines() # 读取每一行内容,存放于列表中# f.readline():with open(r&#x27;g.txt&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f: res1 = f.readline() print(res1) # 123# f.readlines():with open(r&#x27;g.txt&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f: res1 = f.readlines() print(res1) # [&#x27;123\\n&#x27;, &#x27;456\\n&#x27;, &#x27;6798\\n&#x27;, &#x27;hahah\\n&#x27;]# 强调：# f.read()与f.readlines()都是将内容一次性读入内容，如果内容过大会导致内存溢出，若还想将内容全读入内存，则必须分多次读入，使用上面的循环读入---# 写操作f.write(&#x27;1111\\n222\\n&#x27;) # 针对文本模式的写,需要自己写换行符f.write(&#x27;1111\\n222\\n&#x27;.encode(&#x27;utf-8&#x27;)) # 针对b模式的写,需要自己写换行符f.writelines([&#x27;333\\n&#x27;,&#x27;444\\n&#x27;]) # 文件模式f.writelines([bytes(&#x27;333\\n&#x27;,encoding=&#x27;utf-8&#x27;),&#x27;444\\n&#x27;.encode(&#x27;utf-8&#x27;)]) #b模式# f.writelines()with open(&#x27;h.txt&#x27;,mode=&#x27;wt&#x27;,encoding=&#x27;utf-8&#x27;) as f: f.write(&#x27;1111\\n222\\n3333\\n&#x27;)# 写入列表with open(&#x27;h.txt&#x27;, mode=&#x27;wt&#x27;, encoding=&#x27;utf-8&#x27;) as f: l = [&#x27;111111\\n&#x27;,&#x27;2222&#x27;,&#x27;3333&#x27;] for line in l: f.write(line)# 优化：with open(&#x27;h.txt&#x27;, mode=&#x27;wt&#x27;, encoding=&#x27;utf-8&#x27;) as f: l = [&#x27;111111\\n&#x27;, &#x27;2222&#x27;, &#x27;3333&#x27;] f.writelines(l)---# 写b模式with open(&#x27;h.txt&#x27;, mode=&#x27;wb&#x27;, encoding=&#x27;utf-8&#x27;) as f: l = [ &#x27;111111\\n&#x27;.encode(&#x27;utf-8&#x27;), &#x27;2222&#x27;.encode(&#x27;utf-8&#x27;), &#x27;3333&#x27;.encode(&#x27;utf-8&#x27;) ] f.writelines(l)#--------# 纯英文字符情况下简写：with open(&#x27;h.txt&#x27;, mode=&#x27;wb&#x27;, encoding=&#x27;utf-8&#x27;) as f: l = [ b&#x27;111111\\n&#x27;, b&#x27;2222&#x27;, b&#x27;3333&#x27; ] f.writelines(l)# 补充：&#x27;上&#x27;.encode(&#x27;utf-8&#x27;) 等同于 bytes(&#x27;上&#x27;,encoding=&#x27;utf-8&#x27;)with open(&#x27;h.txt&#x27;, mode=&#x27;wb&#x27;, encoding=&#x27;utf-8&#x27;) as f: l = [ bytes(&#x27;上&#x27;,encoding=&#x27;utf-8&#x27;), bytes(&#x27;冲&#x27;,encoding=&#x27;utf-8&#x27;), bytes(&#x27;呀&#x27;,encoding=&#x27;utf-8&#x27;) ] f.writelines(l) 4.2 了解123456789101112f.readable() # 文件是否可读f.writable() # 文件是否可读f.closed # 文件是否关闭f.encoding # 如果文件打开模式为b,则没有该属性f.flush() # 立刻将文件内容从内存刷到硬盘f.name---# 3、flushwith open(&#x27;h.txt&#x27;,mode=&#x27;wt&#x27;,encoding=&#x27;utf-8&#x27;) as f: f.write(&#x27;哈哈哈&#x27;) f.flush() # 立刻将文件内容从内存刷到硬盘 五，主动控制文件内指针移动1234567891011121314151617181920212223242526272829303132&#x27;&#x27;&#x27;指针移动的单位都是以bytes/字节为单位只有一种情况特殊： t模式下的read(n)，n代表的是字符个数&#x27;&#x27;&#x27;with open(r&#x27;aaa.txt&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f: res = f.read(4) print(res) # abc你&#x27;&#x27;&#x27;# 之前文件内指针的移动都是由读/写操作而被动触发的，若想读取文件某一特定位置的数据，则则需要用f.seek方法主动控制文件内指针的移动，详细用法如下：# f.seek(指针移动的字节数,模式控制): # 模式控制:# 0: 默认的模式,该模式代表指针移动的字节数是以文件开头为参照的# 1: 该模式代表指针移动的字节数是以当前所在的位置为参照的# 2: 该模式代表指针移动的字节数是以文件末尾的位置为参照的# 强调:其中0模式可以在t或者b模式使用,而1跟2模式只能在b模式下用&#x27;&#x27;&#x27;# 强调：只有0模式可以在t下使用，1、2必须在b模式下使用# 0：参照物是文件开头位置f.seek(9,0)f.seek(3,0) # 3# 1：参照物是当前指针所在位置f.seek(9,1)f.seek(3,1) # 12# 2：参照物是文件末尾位置，应该倒着移动f.seek(-9,2) # 3f.seek(-3,2) # 9 5.1 案例一： 0模式详解12345678910111213# aaa.txt用utf-8编码，内容如下（abc各占1个字节，中文“你好”各占3个字节）abc你好# 0模式的使用with open(&#x27;aaa.txt&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f: f.seek(3,0) # 参照文件开头移动了3个字节 print(f.tell()) # 查看当前文件指针距离文件开头的位置，输出结果为3 print(f.read()) # 从第3个字节的位置读到文件末尾，输出结果为：你好 # 注意：由于在t模式下，会将读取的内容自动解码，所以必须保证读取的内容是一个完整中文数据，否则解码失败with open(&#x27;aaa.txt&#x27;,mode=&#x27;rb&#x27;) as f: f.seek(6,0) print(f.read().decode(&#x27;utf-8&#x27;)) #输出结果为: 好 5.2 案例二： 1模式详解123456# 1模式的使用with open(&#x27;aaa.txt&#x27;,mode=&#x27;rb&#x27;) as f: f.seek(3,1) # 从当前位置往后移动3个字节，而此时的当前位置就是文件开头 print(f.tell()) # 输出结果为：3 f.seek(4,1) # 从当前位置往后移动4个字节，而此时的当前位置为3 print(f.tell()) # 输出结果为：7 5.3 案例三： 2模式详解123456789# aaa.txt用utf-8编码，内容如下（abc各占1个字节，中文“你好”各占3个字节）abc你好# 2模式的使用with open(&#x27;aaa.txt&#x27;,mode=&#x27;rb&#x27;) as f: f.seek(0,2) # 参照文件末尾移动0个字节， 即直接跳到文件末尾 print(f.tell()) # 输出结果为：9 f.seek(-3,2) # 参照文件末尾往前移动了3个字节 print(f.read().decode(&#x27;utf-8&#x27;)) # 输出结果为：好 123456789101112131415# tail -f access.log 程序详解import timewith open(r&#x27;./access.log&#x27;,mode=&#x27;rb&#x27;) as f: # 1、指针跳到文件末尾 # f.read() # 错误 f.seek(0,2) while True: line=f.readline() if len(line) == 0: # 没有内容 time.sleep(0.5) else: print(line.decode(&#x27;utf-8&#x27;),end=&#x27;&#x27;) 六，文件的修改123456789101112131415161718# 文件a.txt内容如下张一蛋 山东 179 49 12344234523李二蛋 河北 163 57 13913453521王全蛋 山西 153 62 18651433422# 执行操作with open(r&#x27;a.txt&#x27;,mode=&#x27;r+t&#x27;,encoding=&#x27;utf-8&#x27;) as f: f.seek(9,0) f.write(&#x27;&lt;男妇女主任&gt;&#x27;)# 文件修改后的内容如下张一蛋&lt;男妇女主任&gt;9 49 12344234523李二蛋 河北 163 57 13913453521王全蛋 山西 153 62 18651433422# 强调：# 1、硬盘空间是无法修改的,硬盘中数据的更新都是用新内容覆盖旧内容# 2、内存中的数据是可以修改的 文件对应的是硬盘空间,硬盘不能修改对应着文件本质也不能修改, 那我们看到文件的内容可以修改,是如何实现的呢? 大致的思路是将硬盘中文件内容读入内存,然后在内存中修改完毕后再覆盖回硬盘 具体的实现方式分为两种 6.1 文件修改方式一12345678910# 实现思路：将文件内容发一次性全部读入内存,然后在内存中修改完毕后再覆盖写回原文件# 优点: 在文件修改过程中同一份数据只有一份# 缺点: 会过多地占用内存# 方式一：文本编辑器就是采用的这种方式with open(r&#x27;c.txt&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f: res = f.read() data = res.replace(&#x27;zhangsan&#x27;,&#x27;wangwu&#x27;)with open(r&#x27;c.txt&#x27;,mode=&#x27;wt&#x27;,encoding=&#x27;utf-8&#x27;) as f1: f1.write(data) 6.2 文件修改方式二12345678910111213# 实现思路：以读的方式打开原文件,以写的方式打开一个临时文件,一行行读取原文件内容,修改完后写入临时文件...,删掉原文件,将临时文件重命名原文件名# 优点: 不会占用过多的内存# 缺点: 在文件修改过程中同一份数据存了两份import oswith open(r&#x27;c.txt&#x27;,mode=&#x27;rt&#x27;,encoding=&#x27;utf-8&#x27;) as f,\\ open(r&#x27;.c.txt.swap&#x27;,mode=&#x27;wt&#x27;,encoding=&#x27;utf-8&#x27;) as f1: for line in f: f1.write(line.replace(&#x27;wangwu&#x27;,&#x27;zhouliu&#x27;))os.remove(&#x27;c.txt&#x27;) # 删除原文件os.rename(&#x27;.c.txt.swap&#x27;,&#x27;c.txt&#x27;) # 新文件改名","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"字符编码","slug":"zifubianma","date":"2023-03-17T09:59:16.000Z","updated":"2023-03-17T09:59:16.000Z","comments":true,"path":"articles/2023/03/17/zifubianma/","permalink":"https://kkabuzs.github.io/articles/2023/03/17/zifubianma/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 9、字符编码特别鸣谢： — b站老男孩agen老师 一，引入 字符串类型、文本文件的内容都是由字符组成的，但凡涉及到字符的存取，都需要考虑字符编码的问题。 二，知识储备2.1 三大核心硬件所有软件都是运行硬件之上的，与运行软件相关的三大核心硬件为cpu、内存、硬盘，我们需要明确三点 1、软件运行前，软件的代码及其相关数据都是存放于硬盘中的 2、任何软件的启动都是将数据从硬盘中读入内存，然后cpu从内存中取出指令并执行 3、软件运行过程中产生的数据最先都是存放于内存中的，若想永久保存软件产生的数据，则需要将数据由内存写入硬盘 2.2 文本编辑器读取文件内容的流程1234567以python test.py为例，执行流程如下#阶段1、启动python解释器，此时就相当于启动了一个文本编辑器#阶段2、python解释器相当于文本编辑器，从硬盘上将test.py的内容读入到内存中#阶段3、python解释器解释执行刚刚读入的内存的内容，开始识别python语法 2.4 总结python解释器与文件本编辑的异同如下 12345#1、相同点：前两个阶段二者完全一致，都是将硬盘中文件的内容读入内存，详解如下python解释器是解释执行文件内容的，因而python解释器具备读py文件的功能，这一点与文本编辑器一样#2、不同点：在阶段3时，针对内存中读入的内容处理方式不同，详解如下文本编辑器将文件内容读入内存后，是为了显示或者编辑，根本不去理会python的语法，而python解释器将文件内容读入内存后，可不是为了给你瞅一眼python代码写的啥，而是为了执行python代码、会识别python语法 三，字符编码介绍3.1 什么是字符编码？ 人类在与计算机交互时，用的都是人类能读懂的字符，如中文字符、英文字符、日文字符等 而计算机只能识别二进制数,详解如下 二进制数即由0和1组成的数字，例如010010101010。计算机是基于电工作的，电的特性即高低电平，人类从逻辑层面将高电平对应为数字1,低电平对应为数字0，这直接决定了计算机可以识别的是由0和1组成的数字 毫无疑问，由人类的字符到计算机中的数字，必须经历一个过程，如下 翻译的过程必须参照一个特定的标准，该标准称之为字符编码表，该表上存放的就是字符与数字一一对应的关系。 字符编码中的编码指的是翻译或者转换的意思，即将人能理解的字符翻译成计算机能识别的数字 3.2 字符编码表的发展史 (了解)字符编码的发展经历了三个重要的阶段，如下 3.2.1 阶段一：一家独大 现代计算机起源于美国，所以最先考虑仅仅是让计算机识别英文字符，于是诞生了ASCII表 1234# ASCII表的特点: 1、只有英文字符与数字的一一对应关系 2、一个英文字符对应1Bytes，1Bytes=8bit，8bit最多包含256个数字，可以对应256个字符，足够表示所有英文字符 3.2.2 阶段二：诸侯割据、天下大乱 为了让计算机能够识别中文和英文，中国人定制了GBK 1234567# GBK表的特点： 1、只有中文字符、英文字符与数字的一一对应关系 2、一个英文字符对应1Bytes 一个中文字符对应2Bytes 补充说明： 1Bytes=8bit，8bit最多包含256个数字，可以对应256个字符，足够表示所有英文字符 2Bytes=16bit，16bit最多包含65536个数字，可以对应65536个字符，足够表示所有中文字符 每个国家都各自的字符，为让计算机能够识别自己国家的字符外加英文字符，各个国家都制定了自己的字符编码表 12345# Shift_JIS表的特点： 1、只有日文字符、英文字符与数字的一一对应关系# Euc-kr表的特点： 1、只有韩文字符、英文字符与数字的一一对应关系 此时,美国人用的计算机里使用字符编码标准是ASCII、中国人用的计算机里使用字符编码标准是GBK、日本人用的计算机里使用字符编码标准是Shift_JIS,如下图所示， 图1中，文本编辑存取文件的原理如下 1234567文本文件内容全都为字符，无论存取都是涉及到字符编码问题#1、存文本文件人类通过文本编辑器输入的字符会被转化成ASCII格式的二进制存放于内存中，如果需要永久保存，则直接将内存中的ASCII格式的二进制写入硬盘#2、读文本文件直接将硬盘中的ASCII格式的二进制读入内存，然后通过ASCII表反解成英文字符 图2图3都是相同的过程，此时无论是存还是取由于采用的字符编码表一样，所以肯定不会出现乱码问题，但问题是在美国人用的计算机里只能输入英文字符，而在中国人用的计算机里只能输入中文字符和英文字符….,毫无疑问我们希望计算机允许我们输入万国字符均可识别、不乱码，而现阶段计算机采用的字符编码ASCII、GBK、Shift_JIS都无法识别万国字符，所以我们必须定制一个兼容万国字符的编码表，请看阶段三 3.2.3 阶段三：分久必合 unicode于1990年开始研发，1994年正式公布，具备两大特点： 123#1. 存在所有语言中的所有字符与数字的一一对应关系,即兼容万国字符#2. 与传统的字符编码的二进制数都有对应关系，详解如下 很多地方或老的系统、应用软件仍会采用各种各样传统的编码，这是历史遗留问题。此处需要强调：软件是存放于硬盘的，而运行软件是要将软件加载到内存的，面对硬盘中存放的各种传统编码的软件，想让我们的计算机能够将它们全都正常运行而不出现乱码，内存中必须有一种兼容万国的编码，并且该编码需要与其他编码有相对应的映射&#x2F;转换关系，这就是unicode的第二大特点产生的缘由 文本编辑器输入任何字符都是最新存在于内存中，是unicode编码的，存放于硬盘中，则可以转换成任意其他编码，只要该编码可以支持相应的字符 12345678# 英文字符可以被ASCII识别英文字符---&gt;unciode格式的数字---&gt;ASCII格式的数字# 中文字符、英文字符可以被GBK识别中文字符、英文字符---&gt;unicode格式的数字---&gt;gbk格式的数字# 日文字符、英文字符可以被shift-JIS识别日文字符、英文字符---&gt;unicode格式的数字---&gt;shift-JIS格式的数字 3.3 编码与解码 由字符转换成内存中的unicode，以及由unicode转换成其他编码的过程，都称为编码encode 由内存中的unicode转换成字符，以及由其他编码转换成unicode的过程，都称为解码decode 在诸多文件类型中，只有文本文件的内存是由字符组成的，因而文本文件的存取也涉及到字符编码的问题 3.4 utf-8的由来 注意：如果保存到硬盘的是GBK格式二进制，当初用户输入的字符只能是中文或英文，同理如果保存到硬盘的是Shift_JIS格式二进制，当初用户输入的字符只能是日文或英文……如果我们输入的字符中包含多国字符，那么该如何处理？ 123456#多国字符—√—》内存（unicode格式的二进制）——X—》硬盘（GBK格式的二进制）#多国字符—√—》内存（unicode格式的二进制）——X—》硬盘（Shift_JIS格式的二进制）#多国字符—√—》内存（unicode格式的二进制）——√—》硬盘（???格式的二进制） 理论上是可以将内存中unicode格式的二进制直接存放于硬盘中的，但由于unicode固定使用两个字节来存储一个字符，如果多国字符中包含大量的英文字符时，使用unicode格式存放会额外占用一倍空间（英文字符其实只需要用一个字节存放即可），然而空间占用并不是最致命的问题，最致命地是当我们由内存写入硬盘时会额外耗费一倍的时间，所以将内存中的unicode二进制写入硬盘或者基于网络传输时必须将其转换成一种精简的格式，这种格式即utf-8（全称Unicode Transformation Format，即unicode的转换格式） # 多国字符—√—》内存（unicode格式的二进制）——√—》硬盘（utf-8格式的二进制） 那为何在内存中不直接使用utf-8呢？ 123utf-8是针对Unicode的可变长度字符编码：一个英文字符占1Bytes，一个中文字符占3Bytes，生僻字用更多的Bytes存储unicode更像是一个过渡版本，我们新开发的软件或文件存入硬盘都采用utf-8格式，等过去几十年，所有老编码的文件都淘汰掉之后，会出现一个令人开心的场景，即硬盘里放的都是utf-8格式，此时unicode便可以退出历史舞台，内存里也改用utf-8，天下重新归于统一 四，字符编码的应用学习字符编码就是为了存取字符时不发生乱码问题： 1234567#1、内存中固定使用unicode无论输入任何字符都不会发生乱码#2、我们能够修改的是存/取硬盘的编码方式，如果编码设置不正确将会出现乱码问题。乱码问题分为两种：存乱了，读乱了#2.1 存乱了：如果用户输入的内容中包含中文和日文字符，如果单纯以shift_JIS存，日文可以正常写入硬盘，而由于中文字符在shift_jis中没有找到对应关系而导致存乱了#2.2 读乱了：如果硬盘中的数据是shift_JIS格式存储的，采GBK格式读入内存就读乱了 总结： 保证存的时候不乱：在由内存写入硬盘时，必须将编码格式设置为支持所输入字符的编码格式 保证存的时候不乱：在由硬盘读入内存时，必须采用与写入硬盘时相同的编码格式 12345678910# coding:utf-8x = &#x27;上&#x27;res = x.encode(&#x27;gbk&#x27;) # unicode --&gt; gbkprint(res,type(res)) # b&#x27;\\xc9\\xcf&#x27; &lt;class &#x27;bytes&#x27;&gt;print(res.decode(&#x27;gbk&#x27;)) 4.1 总结1234567891011121314151617181920212223242526272829303132结论： 1、内存固定使用unicode，我们可以改变的是存入硬盘采用的格式 英文+汉字 -&gt; unicode -&gt; GBK 英文+日文 -&gt; unicode -&gt; shift-jis 万国字符 -&gt; unicode -&gt; utf-8 2、文本文件存取乱码问题 存乱了：解决方法是，编码格式应该设置成支持文件内容字符串的格式。 取乱了：解决方法是，文件是以什么编码格式存入硬盘的，就应该以什么编码格式读入内存。 3、python解释器默认读文件的编码 python3默认的是utf-8 python3默认是ASCII码 指定文件头修改默认的编码： 在py文件的首行写： # coding:utf-8 4、保证运行python程序前两个阶段不乱码的核心法则： 指定文件头 # coding:文件当初存入硬盘是所采取的编码格式 5、python3的str类型默认直接存成unicode格式，无论如何都不会乱码 保证python2的str类型不乱码 x=u&#x27;上&#x27; 6、了解 python2解释器有两种字符串类型：str、unicode # str类型 x=&#x27;上&#x27; # 字符串值会按照文件头指定的编码格式存入变量的值 # unicode类型 x=u&#x27;上&#x27; # 强制存成unicode","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"基本数据类型及内置方法","slug":"jibenshujuleixingheneizhifangfa","date":"2023-03-16T08:22:45.000Z","updated":"2023-03-16T08:22:45.000Z","comments":true,"path":"articles/2023/03/16/jibenshujuleixingheneizhifangfa/","permalink":"https://kkabuzs.github.io/articles/2023/03/16/jibenshujuleixingheneizhifangfa/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 4、基本数据类型及内置方法特别鸣谢： — b站老男孩agen老师 一，引子 数据类型是用来记录事物状态的，而事物的状态是不断变化的(如:一个人年龄的增长（操作int类型） ，单个人名的修改（操作str类型），学生列表中增加学生（操作list类型）等)，这意味着我们在开发程序时需要频繁对数据进行操作，为了提升我们的开发效率， python针对这些常用的操作，为每一种数据类型内置了一系列方法。本章的主题就是带大家详细了解下它们，以及每种数据类型的详细定义、类型转换。 二，数字类型int与float2.1 定义1234567891011121314# 一：int类型# 作用：# 1、定义：age = 10 # age = int(10)# 二：float类型# 1、作用：# 2、定义：salary = 3.1 # salary = float(3.1)# 注意：名字+括号的意思就是调用某个功能，比如# print(...)调用打印功能# int(...)调用创建整型数据的功能# float(...)调用创建浮点型数据的功能 2.1.1 进制转换(了解内容)1234567891011121314151617181920212223# 2.2（了解）# 10进制 ---&gt; 二进制# 11 -&gt; 1011# 1011 -&gt; 8+2+1print(bin(11)) # 0b1011# 10进制 ---&gt; 八进制print(oct(11)) # 0o13# 10进制 ---&gt; 十六进制print(hex(11)) # 0xbprint(hex(123)) # 0x7b# 其他进制转换为十进制# 二进制 ---&gt; 10进制print(int(&#x27;0b1011&#x27;,2))# 八进制 ---&gt; 10进制print(int(&#x27;0o13&#x27;,8))# 十六进制 ---&gt; 10进制print(int(&#x27;0xb&#x27;,16)) 2.2 类型转换1234567891011121314# 2.1、纯数字的字符串转成intres = int(&#x27;100011&#x27;)print(res,type(res))输出：100011 &lt;class &#x27;int&#x27;&gt;# 2.2、float类型转换res = float(&#x27;3.1&#x27;)print(res,type(res))输出：3.1 &lt;class &#x27;float&#x27;&gt; 2.3 使用int与float没有需要掌握的使用方法，他们使用的就是数学+比较运算 三，字符串3.1 定义12345msg = &#x27;hello&#x27; # msg = str(&#x27;hello&#x27;)print(msg,type(msg))输出：hello &lt;class &#x27;str&#x27;&gt; 3.2 类型转换123456# str可以把任意类型都转成字符串res = str(&#123;&#x27;a&#x27;:1&#125;)print(res,type(res))输出：&#123;&#x27;a&#x27;: 1&#125; &lt;class &#x27;str&#x27;&gt; 3.3 使用3.3.1 优先掌握的操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&gt;&gt;&gt; str1 = &#x27;hello python!&#x27;# 1.按索引取值(正向取，反向取)：# 1.1 正向取(从左往右)&gt;&gt;&gt; str1[6]p# 1.2 反向取(负号表示从右往左)&gt;&gt;&gt; str1[-4]h# 1.3 对于str来说，只能按照索引取值，不能改&gt;&gt;&gt; str1[0]=&#x27;H&#x27; # 报错TypeError# 2.切片(顾头不顾尾，步长)# 2.1 顾头不顾尾：取出索引为0到8的所有字符&gt;&gt;&gt; str1[0:9] hello pyt# 2.2 步长：0:9:2,第三个参数2代表步长，会从0开始，每次累加一个2即可，所以会取出索引0、2、4、6、8的字符&gt;&gt;&gt; str1[0:9:2] hlopt # 2.3 反向切片&gt;&gt;&gt; str1[::-1] # -1表示从右往左依次取值!nohtyp olleh# 3.长度len# 3.1 获取字符串的长度，即字符的个数，但凡存在于引号内的都算作字符)&gt;&gt;&gt; len(str1) # 空格也算字符13# 4.成员运算 in 和 not in# 4.1 int:判断hello 是否在 str1里面&gt;&gt;&gt; &#x27;hello&#x27; in str1 True# 4.2 not in:判断tony 是否不在 str1里面&gt;&gt;&gt; &#x27;tony&#x27; not in str1 True# 5.strip移除字符串首尾指定的字符(默认移除空格)# 5.1 括号内不指定字符，默认移除首尾空白字符(空格、\\n、\\t)&gt;&gt;&gt; str1 = &#x27; life is short! &#x27;&gt;&gt;&gt; str1.strip() life is short!# 5.2 括号内指定字符，移除首尾指定的字符&gt;&gt;&gt; str2 = &#x27;**tony**&#x27; &gt;&gt;&gt; str2.strip(&#x27;*&#x27;) tony# 6.切分split# 6.1 括号内不指定字符，默认以空格作为切分符号&gt;&gt;&gt; str3=&#x27;hello world&#x27;&gt;&gt;&gt; str3.split()[&#x27;hello&#x27;, &#x27;world&#x27;]# 6.2 括号内指定分隔字符，则按照括号内指定的字符切割字符串&gt;&gt;&gt; str4 = &#x27;127.0.0.1&#x27;&gt;&gt;&gt; str4.split(&#x27;.&#x27;) [&#x27;127&#x27;, &#x27;0&#x27;, &#x27;0&#x27;, &#x27;1&#x27;] # 注意:split切割得到的结果是列表数据类型# 7.循环&gt;&gt;&gt; str5 = &#x27;今天你好吗？&#x27;&gt;&gt;&gt; for line in str5: # 依次取出字符串中每一个字符... print(line)...今天你好吗？ 3.3.2 需要掌握的操作 strip,lstrip,rstrip 123456789msg=&#x27;*******zhangsan*******&#x27;print(msg.strip(&#x27;*&#x27;)) # 去两边print(msg.lstrip(&#x27;*&#x27;)) # 去左边print(msg.rstrip(&#x27;*&#x27;)) # 去右边输出：zhangsanzhangsan**************zhangsan lower()，upper() 1234567ms = &#x27;AbbbCCC&#x27;print(ms.lower()) # 全小写print(ms.upper()) # 全大写输出：abbbcccABBBCCC startswith，endswith 123456print(&#x27;zhangsan is sb&#x27;.startswith(&#x27;zhangsan&#x27;)) # 是不是以zhangsan开头print(&#x27;zhangsan is sb&#x27;.endswith(&#x27;sb&#x27;)) # 是不是以sb结尾输出：TrueTrue 格式化输出之format 之前我们使用%s来做字符串的格式化输出操作，在传值时，必须严格按照位置与%s一一对应，而字符串的内置方法format则提供了一种不依赖位置的传值方式 12345678# format括号内在传参数时完全可以打乱顺序，但仍然能指名道姓地为指定的参数传值，name=‘tony’就是传给&#123;name&#125;&gt;&gt;&gt; str4 = &#x27;my name is &#123;name&#125;, my age is &#123;age&#125;!&#x27;.format(age=18,name=&#x27;tony&#x27;)&gt;&gt;&gt; str4 &#x27;my name is tony, my age is 18!&#x27;&gt;&gt;&gt; str4 = &#x27;my name is &#123;name&#125;&#123;name&#125;&#123;name&#125;, my age is &#123;name&#125;!&#x27;.format(name=&#x27;tony&#x27;, age=18)&gt;&gt;&gt; str4 &#x27;my name is tonytonytony, my age is tony!&#x27; split，replit 123info = &#x27;zhang:san:age:18&#x27;print(info.split(&#x27;:&#x27;,1)) # [&#x27;zhang&#x27;, &#x27;san:age:18&#x27;]print(info.rsplit(&#x27;:&#x27;,1)) # [&#x27;zhang:san:age&#x27;, &#x27;18&#x27;] join：把列表拼接成字符串 1234567l = [&#x27;zhangsan&#x27;,&#x27;18&#x27;,&#x27;male&#x27;]# res = l[0]+&#x27;:&#x27;+l[1]+&#x27;:&#x27;+l[2]res = &#x27;:&#x27;.join(l) # 按照某个分隔符把元素全为纯字符串的列表，拼接成一个大字符串print(res)输出：zhangsan:18:male replace 1234567msgg = &#x27;you can you up no can no bb&#x27;print(msgg.replace(&#x27;you&#x27;,&#x27;YOU&#x27;)) # 替换print(msgg.replace(&#x27;you&#x27;,&#x27;YOU&#x27;,1)) # 指定修改的个数输出：YOU can YOU up no can no bbYOU can you up no can no bb isdigit 123456789# 判断字符串是否是纯数字组成，返回结果为True或Falseprint(&#x27;123&#x27;.isdigit())print(&#x27;12a3&#x27;.isdigit())print(&#x27;12.3&#x27;.isdigit())输出：TrueFalseFalse 123456789101112# 案例应用age = input(&#x27;请输入你的年龄：&#x27;).strip()if age.isdigit(): age = int(age) if age &gt; 18: print(&#x27;猜大了&#x27;) elif age &lt; 18: print(&#x27;猜小了&#x27;) else: print(&#x27;猜对了&#x27;)else: print(&#x27;必须输入纯数字&#x27;) 3.3.3 了解操作 find，rfind，index，rindex，count 123456789101112131415161718msg = &#x27;hello egon hahahahah&#x27;# 找到返回起始索引print(msg.find(&#x27;e&#x27;)) # 返回要找的字符串在大字符串中起始索引print(msg.find(&#x27;egon&#x27;))print(msg.index(&#x27;e&#x27;))print(msg.index(&#x27;egon&#x27;))# 找不到print(msg.find(&#x27;xxx&#x27;)) # 返回 -1，代表找不到print(msg.index(&#x27;xxx&#x27;)) # 抛出异常----msg = &#x27;hello egon hahahahah egon、 egon&#x27;print(msg.count(&#x27;egon&#x27;)) # 统计个数输出：3 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# 2.center,ljust,rjust,zfill&gt;&gt;&gt; name=&#x27;tony&#x27;&gt;&gt;&gt; name.center(30,&#x27;-&#x27;) # 总宽度为30，字符串居中显示，不够用-填充-------------tony-------------&gt;&gt;&gt; name.ljust(30,&#x27;*&#x27;) # 总宽度为30，字符串左对齐显示，不够用*填充tony**************************&gt;&gt;&gt; name.rjust(30,&#x27;*&#x27;) # 总宽度为30，字符串右对齐显示，不够用*填充**************************tony&gt;&gt;&gt; name.zfill(50) # 总宽度为50，字符串右对齐显示，不够用0填充0000000000000000000000000000000000000000000000tony# 3.expandtabs&gt;&gt;&gt; name = &#x27;tony\\thello&#x27; # \\t表示制表符(tab键)&gt;&gt;&gt; nametony hello&gt;&gt;&gt; name.expandtabs(1) # 修改\\t制表符代表的空格数tony hello# 4.captalize,swapcase,title# 4.1 captalize：首字母大写&gt;&gt;&gt; message = &#x27;hello everyone nice to meet you!&#x27;&gt;&gt;&gt; message.capitalize()Hello everyone nice to meet you! # 4.2 swapcase：大小写翻转&gt;&gt;&gt; message1 = &#x27;Hi girl, I want make friends with you!&#x27;&gt;&gt;&gt; message1.swapcase() hI GIRL, i WANT MAKE FRIENDS WITH YOU! #4.3 title：每个单词的首字母大写&gt;&gt;&gt; msg = &#x27;dear my friend i miss you very much&#x27;&gt;&gt;&gt; msg.title()Dear My Friend I Miss You Very Much # 5.is数字系列#在python3中num1 = b&#x27;4&#x27; #bytesnum2 = u&#x27;4&#x27; #unicode,python3中无需加u就是unicodenum3 = &#x27;四&#x27; #中文数字num4 = &#x27;Ⅳ&#x27; #罗马数字#isdigt:bytes,unicode&gt;&gt;&gt; num1.isdigit()True&gt;&gt;&gt; num2.isdigit()True&gt;&gt;&gt; num3.isdigit()False&gt;&gt;&gt; num4.isdigit() False#isdecimal:uncicode(bytes类型无isdecimal方法)&gt;&gt;&gt; num2.isdecimal() True&gt;&gt;&gt; num3.isdecimal() False&gt;&gt;&gt; num4.isdecimal() False#isnumberic:unicode,中文数字,罗马数字(bytes类型无isnumberic方法)&gt;&gt;&gt; num2.isnumeric() True&gt;&gt;&gt; num3.isnumeric() True&gt;&gt;&gt; num4.isnumeric() True# 三者不能判断浮点数&gt;&gt;&gt; num5 = &#x27;4.3&#x27;&gt;&gt;&gt; num5.isdigit()False&gt;&gt;&gt; num5.isdecimal()False&gt;&gt;&gt; num5.isnumeric()False&#x27;&#x27;&#x27;总结: 最常用的是isdigit,可以判断bytes和unicode类型,这也是最常见的数字应用场景 如果要判断中文数字或罗马数字,则需要用到isnumeric。&#x27;&#x27;&#x27;# 6.is其他&gt;&gt;&gt; name = &#x27;tony123&#x27;&gt;&gt;&gt; name.isalnum() #字符串中既可以包含数字也可以包含字母True&gt;&gt;&gt; name.isalpha() #字符串中只包含字母False&gt;&gt;&gt; name.isidentifier()True&gt;&gt;&gt; name.islower() # 字符串是否是纯小写True&gt;&gt;&gt; name.isupper() # 字符串是否是纯大写False&gt;&gt;&gt; name.isspace() # 字符串是否全是空格False&gt;&gt;&gt; name.istitle() # 字符串中的单词首字母是否都是大写False 四，列表4.1 定义 作用：按位置存放多个值 12345l = [1,2.2,&#x27;a&#x27;] # l = list([1,2.2,&#x27;a&#x27;])print(type(l))输出：&lt;class &#x27;list&#x27;&gt; 4.2 类型转换但凡能被for循环遍历的类型都可以当作参数传给list()转成列表 1234567891011res = list(&#x27;hello&#x27;)print(res)输出：[&#x27;h&#x27;, &#x27;e&#x27;, &#x27;l&#x27;, &#x27;l&#x27;, &#x27;o&#x27;]res1 = list(&#123;&#x27;k1&#x27;:111,&#x27;k2&#x27;:222,&#x27;k3&#x27;:333&#125;)print(res1)输出：[&#x27;k1&#x27;, &#x27;k2&#x27;, &#x27;k3&#x27;] 4.3 使用4.3.1 优先掌握的操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100# 1、按索引存取值（正向存取+反向存取）：即可以取也可以改l = [111,&#x27;zhangsan&#x27;,&#x27;hello&#x27;]# 正向取print(l[0]) # 结果：111# 反向取print(l[-1]) # 结果：hello# 可以取也可以改l[0] = 222 # 索引存在则修改对应的值print(l)输出：[222, &#x27;zhangsan&#x27;, &#x27;hello&#x27;]# 无论是取值操作还是赋值操作：索引不存在，则报错# l[3] = 333 # IndexError: list assignment index out of range# 2、切片（顾头不顾尾，步长）l = [111,&#x27;zhangsan&#x27;,&#x27;hello&#x27;,&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;]print(l[0:3]) # [111, &#x27;zhangsan&#x27;, &#x27;hello&#x27;]print(l[0:5:2]) # [111, &#x27;hello&#x27;, &#x27;b&#x27;]print(l[0:len(l)]) # [111, &#x27;zhangsan&#x27;, &#x27;hello&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;]print(l[:]) # [111, &#x27;zhangsan&#x27;, &#x27;hello&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;]new_l = l[:] # 切片等同于拷贝行为，而且相当于浅拷贝print(id(l)) # 4518562752print(id(new_l)) # 4431592064# 列表倒过来print(l[::-1]) # [&#x27;d&#x27;, &#x27;c&#x27;, &#x27;b&#x27;, &#x27;a&#x27;, &#x27;hello&#x27;, &#x27;zhangsan&#x27;, 111]# 3、长度print(len([1,2,3])) # 3 统计的是个数# 4、成员运算in和not inprint(&#x27;aaa&#x27; in [&#x27;aaa&#x27;,2,3]) # Trueprint(&#x27;ll&#x27; not in [1,2,3]) # True# 5、往列表中添加值# 5.1、追加l = [111,&#x27;zhangsan&#x27;,&#x27;hello&#x27;]l.append(3333)l.append(4444)print(l) # [111, &#x27;zhangsan&#x27;, &#x27;hello&#x27;, 3333, 4444]# 5.2、插入值l = [111,&#x27;zhangsan&#x27;,&#x27;hello&#x27;]l.insert(1,&#x27;lisi&#x27;)l.insert(0,&#x27;wangwu&#x27;)print(l) # [&#x27;wangwu&#x27;, 111, &#x27;lisi&#x27;, &#x27;zhangsan&#x27;, &#x27;hello&#x27;]# 5.3、extend添加值new_l = [1,2,3]l = [111,&#x27;zhangsan&#x27;,&#x27;hello&#x27;]l.append(new_l)print(l) # [111, &#x27;zhangsan&#x27;, &#x27;hello&#x27;, [1, 2, 3]]# 代码实现for item in new_l: l.append(item)print(l) # [111, &#x27;zhangsan&#x27;, &#x27;hello&#x27;, 1, 2, 3]# extend 实现了上述代码l.extend(new_l)print(l) # [111, &#x27;zhangsan&#x27;, &#x27;hello&#x27;, 1, 2, 3]l.extend(&#x27;abc&#x27;)print(l) # [111, &#x27;zhangsan&#x27;, &#x27;hello&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;]# 6、删除# 方式一：通用的删除方法，没有返回值，只是单纯删除l = [111,&#x27;zhangsan&#x27;,&#x27;hello&#x27;]del l[1]# x = del l[1] # 抛出异常，不支持赋值语法print(l) # [111, &#x27;hello&#x27;]# 方式二：l.pop() 根据索引删除,会返回删除的值l = [111,&#x27;zhangsan&#x27;,&#x27;hello&#x27;]# l.pop() 不指定索引，默认删除最后一个# print(l) # [111, &#x27;zhangsan&#x27;]res = l.pop(1)print(l) # [111, &#x27;hello&#x27;]print(res) # zhangsan# 方式三：l.remove() 根据元素删除l = [111,&#x27;zhangsan&#x27;,[1,2,3],&#x27;hello&#x27;]l.remove([1,2,3])print(l) # [111, &#x27;zhangsan&#x27;, &#x27;hello&#x27;]res = l.remove(&#x27;zhangsan&#x27;)print(res) # None# 7、循环for x in [1,2,3,4]: print(x) 12345678910111213141516171819202122232425262728293031323334# 需要掌握操作l = [111,&#x27;zhangsan&#x27;,&#x27;hello&#x27;,&#x27;aaa&#x27;,&#x27;aaa&#x27;,&#x27;aaa&#x27;]# 1、l.count() # 统计个数print(l.count(&#x27;aaa&#x27;)) # 3# 2、l.index() #返回第一次找到aaa的索引值print(l.index(&#x27;aaa&#x27;)) # 3# 3、l.clear() # 清空列表# 4、l.reverse() # 不是排序，就是将列表倒过来l.reverse()print(l) # [&#x27;aaa&#x27;, &#x27;aaa&#x27;, &#x27;aaa&#x27;, &#x27;hello&#x27;, &#x27;zhangsan&#x27;, 111]# 5、l.sort()：列表内元素必须是同种类型才可以排序l = [11,-3,9,2]l.sort() # 默认从小到大排，升序print(l) # [-3, 2, 9, 11]l.sort(reverse=True) # 降序print(l) # [11, 9, 2, -3]# 了解：字符串也可以比大小，按照对应的位置的字符依次pk# 字符串的大小按照ASCI码表的先后顺序区别字符大小，表中排在后面的字符大于前面的print(&#x27;a&#x27; &gt; &#x27;b&#x27;) # Falseprint(&#x27;abz&#x27; &gt; &#x27;abcdefg&#x27;) # True# 了解：列表也可以比大小，原理同字符串一样（对应位置必须为同种类型，否则报错）l1 = [1,&#x27;abc&#x27;,&#x27;zaa&#x27;]l2 = [10]print(l1 &gt; l2) # False 4.3.2 队列与堆栈1234567891011121314151617181920212223242526272829## 补充# 队列：first in first out（简称FIFO） ，先进先出ll = []# 入队操作ll.append(&#x27;first&#x27;)ll.append(&#x27;second&#x27;)ll.append(&#x27;third&#x27;)print(ll) # [&#x27;first&#x27;, &#x27;second&#x27;, &#x27;third&#x27;]# 出队操作print(ll.pop(0)) # firstprint(ll.pop(0)) # secondprint(ll.pop(0)) # third# 堆栈：last in first out（简称LIFO），后进先出lll = []# 入队操作lll.append(&#x27;first&#x27;)lll.append(&#x27;second&#x27;)lll.append(&#x27;third&#x27;)print(lll) # [&#x27;first&#x27;, &#x27;second&#x27;, &#x27;third&#x27;]# 出队操作print(lll.pop()) # thirdprint(lll.pop()) # secondprint(lll.pop()) # first 4.3.3 了解操作123456789&gt;&gt;&gt; l=[1,2,3,4,5,6]&gt;&gt;&gt; l[0:3:1] [1, 2, 3] # 正向步长&gt;&gt;&gt; l[2::-1] [3, 2, 1] # 反向步长# 通过索引取值实现列表翻转&gt;&gt;&gt; l[::-1][6, 5, 4, 3, 2, 1] 五，元组5.1 作用 元组就是”一个不可变的列表”作用：按照索引&#x2F;位置存放多个值，只用于取，不用于改 5.2 定义方式123456789101112131415161718192021定义：()内用逗号分隔开多个任意类型的元素t = (1,1.3,&#x27;aa&#x27;)print(type(t)) # &lt;class &#x27;tuple&#x27;&gt;x = (10) # 单独一个括号代表包含的意思print(x,type(x)) # 10 &lt;class &#x27;int&#x27;&gt;t1 = (10,) # 如果元组中只有一个元素，必须加逗号分隔开print(t1,type(t1)) # (10,) &lt;class &#x27;tuple&#x27;&gt;t2 = (1,1.3,&#x27;aa&#x27;) # t2=(索引0 -&gt; 值1的内存地址，索引1 -&gt; 值1.3的内存地址，索引2 -&gt; 值&#x27;aa&#x27;的内存地址)# t2[0] = 1111 # 报错：TypeError: &#x27;tuple&#x27; object does not support item assignmentt3 = (1,[11,22]) # t3=(索引0 -&gt; 值1的内存地址，索引1 -&gt; 值[11,22]的内存地址)print(id(t3[0]),id(t3[1])) # 4478720304 4479892096# t3[0] = 1111111 # 报错：TypeError: &#x27;tuple&#x27; object does not support item assignment# t3[1] = 2222222 # 报错：TypeError: &#x27;tuple&#x27; object does not support item assignmentt3[1][0] = 1111111print(t3) # (1, [1111111, 22])print(id(t3[0]),id(t3[1])) # 4401031472 4402203264 # 原理，元组是不可变类型，元组里的元素是不可以修改的，但是元组里嵌套的列表本身不可被修改，但是列表里的值可以被修改，因为列表是可变类型 5.3 类型转换123print(tuple(&#x27;hello&#x27;)) # (&#x27;h&#x27;, &#x27;e&#x27;, &#x27;l&#x27;, &#x27;l&#x27;, &#x27;o&#x27;)print(tuple([1,2,&#x27;aa&#x27;])) # (1, 2, &#x27;aa&#x27;)print(tuple(&#123;&#x27;k1&#x27;:11,&#x27;k2&#x27;:22&#125;)) # (&#x27;k1&#x27;, &#x27;k2&#x27;) 5.4 内置方法12345678910111213141516171819202122232425# 优先掌握的方法# 1、按索引取值（正向取、反向取）：只能取tup = (&#x27;aa&#x27;,&#x27;bb&#x27;,&#x27;cc&#x27;,&#x27;ddd&#x27;,&#x27;eee&#x27;)print(tup[0]) # aaprint(tup[-1]) # eee# 2、切片（顾头不顾尾，步长）print(tup[0:3]) # (&#x27;aa&#x27;, &#x27;bb&#x27;, &#x27;cc&#x27;)print(tup[::-1]) # (&#x27;eee&#x27;, &#x27;ddd&#x27;, &#x27;cc&#x27;, &#x27;bb&#x27;, &#x27;aa&#x27;)# 3、长度print(len(tup)) # 5# 4、成员运算in和not inprint(&#x27;aa&#x27; in tup)# 5、循环for i in tup: print(i)# 内置方法tup1 = (2,3,111,111,111,111)print(tup1.index(111)) # 2 # 统计第一次出现111的索引是多少print(tup1.count(111)) # 4 # 统计111出现的次数 六，字典6.1 定义方式12345678910111213定义：在&#123;&#125;内用逗号分隔开多元素，每一个元素都是key:value的形式，其中value可以是任意类型，而key则必须是不可变类型，且不能重复d = &#123;&#x27;k1&#x27;:111,(1,2,3):222&#125; # d = dict(...)print(d[&#x27;k1&#x27;]) # 111print(d[(1,2,3)]) # 222 # 一般没人这么做# 通常key应该是str类型，因为str类型会对value有描述性的功能print(type(d)) # &lt;class &#x27;dict&#x27;&gt;d1 = &#123;&#125; # 默认定义的是空字典print(d1,type(d1)) # &#123;&#125; &lt;class &#x27;dict&#x27;&gt;d2 = dict(x=1,y=2,z=3)print(d2,type(d2)) # &#123;&#x27;x&#x27;: 1, &#x27;y&#x27;: 2, &#x27;z&#x27;: 3&#125; &lt;class &#x27;dict&#x27;&gt; 6.2 数据类型转换123456789101112131415161718192021222324252627282930313233343536info = [ [&#x27;name&#x27;,&#x27;zhangsan&#x27;], (&#x27;age&#x27;,18), [&#x27;gender&#x27;,&#x27;male&#x27;]]# 需求，将info列表整合成一个字典dd = &#123;&#125;for item in info: # item = [&#x27;name&#x27;,&#x27;zhangsan&#x27;] dd[item[0]]=item[1]print(dd) # &#123;&#x27;name&#x27;: &#x27;zhangsan&#x27;, &#x27;age&#x27;: 18, &#x27;gender&#x27;: &#x27;male&#x27;&#125;# 优化dd1 = &#123;&#125;for k,v in info: dd1[k]=vprint(dd1) # &#123;&#x27;name&#x27;: &#x27;zhangsan&#x27;, &#x27;age&#x27;: 18, &#x27;gender&#x27;: &#x27;male&#x27;&#125;# 一行代码搞定上述for循环的工作res = dict(info)print(res) # &#123;&#x27;name&#x27;: &#x27;zhangsan&#x27;, &#x27;age&#x27;: 18, &#x27;gender&#x27;: &#x27;male&#x27;&#125;# 造字典的方式：快速初始化字典keys = [&#x27;name&#x27;,&#x27;age&#x27;,&#x27;gender&#x27;]value = Nonedic = &#123;&#125;for k in keys: dic[k]=Noneprint(dic) # &#123;&#x27;name&#x27;: None, &#x27;age&#x27;: None, &#x27;gender&#x27;: None&#125;# 一行实现，dic1 = &#123;&#125;.fromkeys(keys,None)print(dic1) # &#123;&#x27;name&#x27;: None, &#x27;age&#x27;: None, &#x27;gender&#x27;: None&#125; 6.3 内置方法6.3.1 优先掌握的操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# 1、按key取值：可存可取di = &#123;&#x27;k1&#x27;:111&#125;# 针对赋值操作：key存在，则修改di[&#x27;k1&#x27;] = 222print(di) # &#123;&#x27;k1&#x27;: 222&#125;# 针对赋值操作：key不存在，则创建新值di[&#x27;k2&#x27;] = 3333print(di) # &#123;&#x27;k1&#x27;: 222, &#x27;k2&#x27;: 3333&#125;# 2、长度lendia = &#123;&#x27;k1&#x27;: 222, &#x27;k2&#x27;: 3333&#125;print(len(dia)) # 2# 3、成员运算in和not in：根据key判断dia1 = &#123;&#x27;k1&#x27;: 222, &#x27;k2&#x27;: 3333&#125;print(&#x27;k1&#x27; in dia1) # Trueprint(222 in dia1) # False# 4、删除dia2 = &#123;&#x27;k1&#x27;: 222, &#x27;k2&#x27;: 3333&#125;# 4.1 通用删除del dia2[&#x27;k1&#x27;]print(dia2) # &#123;&#x27;k2&#x27;: 3333&#125;# 4.2 pop删除：根据key删除元素，返回删除key对应的value值pr = dia2.pop(&#x27;k2&#x27;)print(dia2) # &#123;&#125;print(pr) # 3333# 4.3 popitem删除：随机删除，返回一个元组（删除的key,删除的value）dia3 = &#123;&#x27;k1&#x27;: 222, &#x27;k2&#x27;: 3333&#125;res = dia3.popitem()print(dia3) # &#123;&#x27;k1&#x27;: 222&#125;print(res) # (&#x27;k2&#x27;, 3333)# 5、键keys()，值values()，键值对items() =》 python3中做了优化&#x27;&#x27;&#x27;在python2中：Python 2.7.16 (v2.7.16:413a49145e, Mar 2 2019, 14:32:10)[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; dist1 = &#123;&#x27;k1&#x27;: 222, &#x27;k2&#x27;: 3333&#125;&gt;&gt;&gt; dist1.keys()[&#x27;k2&#x27;, &#x27;k1&#x27;]&gt;&gt;&gt; dist1.values()[3333, 222]&gt;&gt;&gt; dist1.items()[(&#x27;k2&#x27;, 3333), (&#x27;k1&#x27;, 222)]&gt;&gt;&gt; dict(dist1.items())&#123;&#x27;k2&#x27;: 3333, &#x27;k1&#x27;: 222&#125;在python3中：Python 3.9.13 (v3.9.13:6de2ca5339, May 17 2022, 11:37:23)[Clang 13.0.0 (clang-1300.0.29.30)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; dist1 = &#123;&#x27;k1&#x27;: 222, &#x27;k2&#x27;: 3333&#125;&gt;&gt;&gt; dist1.keys()dict_keys([&#x27;k1&#x27;, &#x27;k2&#x27;])&gt;&gt;&gt; dist1.values()dict_values([222, 3333])&gt;&gt;&gt; dist1.items()dict_items([(&#x27;k1&#x27;, 222), (&#x27;k2&#x27;, 3333)])&#x27;&#x27;&#x27;# 6、循环dist1 = &#123;&#x27;k1&#x27;: 222, &#x27;k2&#x27;: 3333&#125;for d in dist1.keys(): print(d)输出：k1k2for d1 in dist1: print(d1)输出：k1k2for d2 in dist1.values(): print(d2) 输出：2223333for item in dist1.items(): print(item)输出：(&#x27;k1&#x27;, 222)(&#x27;k2&#x27;, 3333)# 做解压操作for k,v in dist1.items(): print(k,v)输出：k1 222k2 3333print(list(dist1.keys())) # [&#x27;k1&#x27;, &#x27;k2&#x27;]print(list(dist1.values())) # [222, 3333]print(list(dist1.items())) # [(&#x27;k1&#x27;, 222), (&#x27;k2&#x27;, 3333)] 6.3.2 需要掌握的内置方法1234567891011121314151617181920212223242526272829## 其他：需要掌握的内置方法d_1 = &#123;&#x27;k1&#x27;:111&#125;# 1、d_1.clear() # 清空字典# 2、d_1.update() # 更新字典d_2 = &#123;&#x27;k2&#x27;:222,&#x27;k3&#x27;:333&#125;d_1.update(d_2)print(d_1) # &#123;&#x27;k1&#x27;: 111, &#x27;k2&#x27;: 222, &#x27;k3&#x27;: 333&#125;d_1.update(&#123;&#x27;k1&#x27;:11111111&#125;)print(d_1) # &#123;&#x27;k1&#x27;: 11111111, &#x27;k2&#x27;: 222, &#x27;k3&#x27;: 333&#125;# d_1.get()：根据key取值，容错性好print(d_1[&#x27;k1&#x27;]) # 11111111 # key不存在则报错print(d_1.get(&#x27;k1&#x27;)) # 11111111print(d_1.get(&#x27;k5&#x27;)) # None # key不存在返回None# 还可以附加说明print(d_1.get(&#x27;kkk&#x27;,&#x27;没有找到值哦～&#x27;)) # 没有找到值哦～print(d_1.get(&#x27;k1&#x27;,&#x27;没有找到值哦～&#x27;)) # 11111111# d_1.setdefault()：如果k1有，则不添加，如果没有，则添加，返回值：返回字典中key对应的值pres = d_1.setdefault(&#x27;k1&#x27;,123)print(d_1) # &#123;&#x27;k1&#x27;: 11111111, &#x27;k2&#x27;: 222, &#x27;k3&#x27;: 333&#125;print(pres) # 11111111d_1.setdefault(&#x27;kkk&#x27;,123321)print(d_1) # &#123;&#x27;k1&#x27;: 11111111, &#x27;k2&#x27;: 222, &#x27;k3&#x27;: 333, &#x27;kkk&#x27;: 123321&#125; 七，集合 集合、list、tuple、dict一样都可以存放多个值，但是集合主要用于：去重、关系运算 7.1 定义12345678910111213&quot;&quot;&quot;定义：在&#123;&#125;内用逗号分隔开多个元素，集合具备以下三个特点： 1：每个元素必须是不可变类型 2：集合内没有重复的元素 3：集合内元素无序&quot;&quot;&quot;s = &#123;1,2,3,4&#125; # 本质 s = set(&#123;1,2,3,4&#125;)# 注意1：列表类型是索引对应值，字典是key对应值，均可以取得单个指定的值，而集合类型既没有索引也没有key与值对应，所以无法取得单个的值，而且对于集合来说，主要用于去重与关系元素，根本没有取出单个指定值这种需求。# 注意2:&#123;&#125;既可以用于定义dict，也可以用于定义集合，但是字典内的元素必须是key:value的格式，现在我们想定义一个空字典和空集合，该如何准确去定义两者?d = &#123;&#125; # 默认是空字典 s = set() # 这才是定义空集合 7.3 类型转换12345678# set(&#123;1,2,3&#125;)res = set(&#x27;hellolllllll&#x27;)print(res) # &#123;&#x27;e&#x27;, &#x27;l&#x27;, &#x27;h&#x27;, &#x27;o&#x27;&#125;print(set([1,1,1,1,1,1,1])) # &#123;1&#125;# print(set([1,1,1,1,1,1,1,[111,222]])) # 报错print(set(&#123;&#x27;k1&#x27;:1,&#x27;k2&#x27;:2&#125;)) # &#123;&#x27;k1&#x27;, &#x27;k2&#x27;&#125; 7.4 使用7.4.1 关系运算123456789101112131415161718192021222324252627282930313233343536373839404142434445friends1 = &#123;&#x27;zhangsan&#x27;,&#x27;lisi&#x27;,&#x27;wangwu&#x27;,&#x27;zhaoliu&#x27;&#125;friends2 = &#123;&#x27;zhangsan&#x27;,&#x27;lisi&#x27;,&#x27;wangwu&#x27;,&#x27;liuba&#x27;&#125;# 4.1、取交集：两者共同的好友res = friends1 &amp; friends2print(res) # &#123;&#x27;wangwu&#x27;, &#x27;lisi&#x27;, &#x27;zhangsan&#x27;&#125;# 使用方法也可以print(friends1.intersection(friends2)) # &#123;&#x27;zhangsan&#x27;, &#x27;lisi&#x27;, &#x27;wangwu&#x27;&#125;# 4.2、取并集/合集：两者所有的好友res1 = friends1 | friends2print(res1) # &#123;&#x27;zhangsan&#x27;, &#x27;wangwu&#x27;, &#x27;zhaoliu&#x27;, &#x27;lisi&#x27;, &#x27;liuba&#x27;&#125;print(friends1.union(friends2)) # # &#123;&#x27;zhangsan&#x27;, &#x27;wangwu&#x27;, &#x27;zhaoliu&#x27;, &#x27;lisi&#x27;, &#x27;liuba&#x27;&#125;# 4.3、取差集：取friend1独有的好友res2 = friends1 - friends2 # 有顺序要求，保留firends1独有的好友print(res2) # &#123;&#x27;zhaoliu&#x27;&#125;print(friends1.difference(friends2)) # &#123;&#x27;zhaoliu&#x27;&#125;print(friends2 - friends1) # &#123;&#x27;liuba&#x27;&#125; # 取firends2独有的好友# 4.4、对称差集：取两者独有的好友（即去掉共同好友）print(friends1 ^ friends2) # &#123;&#x27;zhaoliu&#x27;, &#x27;liuba&#x27;&#125;print(friends1.symmetric_difference(friends2)) # &#123;&#x27;zhaoliu&#x27;, &#x27;liuba&#x27;&#125;# 4.5 父子集：包含的关系s1 = &#123;1,2,3&#125;s2 = &#123;1,2,4&#125;# 不存在包含关系，下面比较均为Falseprint(s1 &gt; s2) # Falseprint(s1 &lt; s2) # Falses3 = &#123;1,2&#125;# 当s1大于或者等于s3时，才能说s1是s3的父集print(s1 &gt; s3) # Trueprint(s1.issuperset(s3)) # Trueprint(s3.issubset(s1)) # Trues4 = &#123;1,2,3&#125;# 互为父子集print(s1 == s4) # Trueprint(s1.issuperset(s4))print(s4.issuperset(s1)) 7.4.2 去重集合去重复有局限性 # 1. 只能针对不可变类型 # 2. 集合本身是无序的，去重之后无法保留原来的顺序 12345678910111213141516171819202122232425# 1、只能针对不可变类型去重print(set([1,1,1,1,1,2])) # &#123;1, 2&#125;# 2、无法保证原来的顺序l = [1,&#x27;a&#x27;,&#x27;b&#x27;,&#x27;z&#x27;,1,1,1,2,2,2]l = list(set(l))print(l) # [1, 2, &#x27;z&#x27;, &#x27;b&#x27;, &#x27;a&#x27;]lis=[ &#123;&#x27;name&#x27;:&#x27;lili&#x27;,&#x27;age&#x27;:18,&#x27;sex&#x27;:&#x27;male&#x27;&#125;, &#123;&#x27;name&#x27;:&#x27;jack&#x27;,&#x27;age&#x27;:73,&#x27;sex&#x27;:&#x27;male&#x27;&#125;, &#123;&#x27;name&#x27;:&#x27;tom&#x27;,&#x27;age&#x27;:20,&#x27;sex&#x27;:&#x27;female&#x27;&#125;, &#123;&#x27;name&#x27;:&#x27;lili&#x27;,&#x27;age&#x27;:18,&#x27;sex&#x27;:&#x27;male&#x27;&#125;, &#123;&#x27;name&#x27;:&#x27;lili&#x27;,&#x27;age&#x27;:18,&#x27;sex&#x27;:&#x27;male&#x27;&#125;,]new_l = []for dic in lis: if dic not in new_l: new_l.append(dic)print(new_l) 输出：[&#123;&#x27;name&#x27;: &#x27;lili&#x27;, &#x27;age&#x27;: 18, &#x27;sex&#x27;: &#x27;male&#x27;&#125;, &#123;&#x27;name&#x27;: &#x27;jack&#x27;, &#x27;age&#x27;: 73, &#x27;sex&#x27;: &#x27;male&#x27;&#125;, &#123;&#x27;name&#x27;: &#x27;tom&#x27;, &#x27;age&#x27;: 20, &#x27;sex&#x27;: &#x27;female&#x27;&#125;] 7.4.3 其他操作123456789101112131415# 其他内置方法s = &#123;1,2,3&#125;s.discard(3) # 指定元素删除，元素不存在不报错print(s) # &#123;1, 2&#125;# s.remove(3) # 删除不存在的元素报错# print(s)s.update(&#123;1,3,5&#125;) # 更新并去重print(s) # &#123;1, 2, 3, 5&#125;# 了解s.difference_update(&#123;3,4,5&#125;) #取差集，不影响原集合。等于s = s.difference(&#123;3,4,5&#125;)print(s) # &#123;1, 2&#125; 八，可变不可变类型# 可变类型：值改变，id不变，证明改变的是原值，证明原值是可以被改变的 # 不可变类型：值改变，id也改变。证明是产生新的值，压根没有改变原值，证明原值是不可以被修改的 8.1 验证8.1.1 int是不可变类型1234x = 10print(id(x)) # 4559862352x = 11 # 产生新的值print(id(x)) # 4441344624 8.1.2 float是不可变类型1234x = 3.1print(id(x)) # 4549023280x = 3.2print(id(x)) # 4312741360 8.1.3 str是不可变类型1234x = &#x27;aaa&#x27;print(id(x)) # 4516407088x = &#x27;bbb&#x27;print(id(x)) # 4383786800 小结：int、float、str都被设计成了不可分割的整体，不能够被改变 8.1.4 lsit是可变类型1234l = [&#x27;aaa&#x27;,&#x27;bbb&#x27;,&#x27;ccc&#x27;]print(id(l)) # 4415875904l[0] = &#x27;AAA&#x27;print(id(l)) # 4415875904 8.1.5 dict是可变类型1234dic = &#123;&#x27;k1&#x27;:111,&#x27;k2&#x27;:222&#125;print(id(dic)) # 4500481664dic[&#x27;k1&#x27;] = 3333print(id(dic)) # 4500481664 123456789101112# 关于字典补充# 定义：&#123;&#125;内用逗号分隔开多key:value，其中value可以是任意类型，但是key必须是不可变类型。dic1 = &#123; &#x27;k1&#x27;:111, &#x27;k2&#x27;:3.1, &#x27;k3&#x27;:[333,], &#x27;k4&#x27;:&#x27;name&#x27;, &#x27;k5&#x27;:&#123;&#x27;name&#x27;:&#x27;zhangsan&#x27;&#125;, 333:111, 3.3:&#x27;hello&#x27; # 百分之99的key类型都是str&#125; 8.1.6 bool是不可变类型九，数据类型总结|按存值个数区分|||-||只能存一个值：可称为标量&#x2F;原子类型|数字、字符串||可以存放多个值：可称为容器类型|列表、元组、字典||按照访问方式区分|||直接访问：只能通过变量名访问整个值|数字||顺序访问：可以用索引访问固定的值，索引代表顺序，又称为序列类型|字符串、列表、元组||key访问：可以用key访问指定的值，又称为映射类型|字典||按可变不可变区分|||可变类型|列表、字典||不可变类型|数字、元组、字符串|","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"Python语法入门之流程控制","slug":"pythonyufarumenzhiliuchengkongzhi","date":"2023-03-16T07:52:14.000Z","updated":"2023-03-16T07:52:14.000Z","comments":true,"path":"articles/2023/03/16/pythonyufarumenzhiliuchengkongzhi/","permalink":"https://kkabuzs.github.io/articles/2023/03/16/pythonyufarumenzhiliuchengkongzhi/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 7、Python语法入门之流程控制特别鸣谢： — b站老男孩agen老师 一，引子 流程控制即控制流程，具体指控制程序的执行流程，而程序的执行流程分为三种结构：顺序结构（之前我们写的代码都是顺序结构）、分支结构（用到if判断）、循环结构（用到while与for） 二，分支结构2.1 什么是分支结构分支结构就是根据条件判断的真假去执行不同分支对应的子代码 2.2 为什么要用分支结构 人类某些时候需要根据条件来决定做什么事情，比如：如果今天下雨，就带伞 所以程序中必须有相应的机制来控制计算机具备人的这种判断能力 2.3 如何使用分支结构2.3.1 if语法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134&#x27;&#x27;&#x27;语法1：if 条件： 代码1 代码2 代码3&#x27;&#x27;&#x27;# 判断年龄和美丽age = 18is_beautiful = Truestar = &#x27;水瓶座&#x27;if age &gt; 16 and age &lt; 20 and is_beautiful and star == &#x27;水瓶座&#x27;: print(&#x27;喜欢&#x27;)print(&#x27;==============&#x27;)&#x27;&#x27;&#x27;语法2：if 条件： 代码1 代码2 代码3else: 代码1 代码2 代码3&#x27;&#x27;&#x27;# 以上案例加上elseage = 18is_beautiful = Truestar = &#x27;水瓶座&#x27;if age &gt; 16 and age &lt; 20 and is_beautiful and star == &#x27;水瓶座&#x27;: print(&#x27;喜欢&#x27;)else: print(&#x27;闹玩呢！&#x27;)print(&#x27;==============&#x27;)&#x27;&#x27;&#x27;语法3：if 条件1： 代码1 代码2 代码3elif 条件2： 代码1 代码2 代码3&#x27;&#x27;&#x27;num = 9if num &gt;= 10: print(&#x27;厉害&#x27;)elif num &gt;= 8: print(&#x27;还可以&#x27;)# 判断成绩案例sorce = int(input(&quot;请输入成绩：&quot;))if sorce &gt;= 90: print(&#x27;you&#x27;)elif sorce&gt;= 80: print(&#x27;liang&#x27;)elif sorce &gt;= 70: print(&#x27;jige&#x27;)&#x27;&#x27;&#x27;语法4：if 条件1： 代码1 代码2 代码3elif 条件2： 代码1 代码2 代码3elif 条件3： 代码1 代码2 代码3elif 条件4： 代码1 代码2 代码3else: 代码1 代码2 代码3&#x27;&#x27;&#x27;# 判断成绩优化sorce = int(input(&quot;请输入成绩：&quot;))if sorce &gt;= 90: print(&#x27;you&#x27;)elif sorce&gt;= 80: print(&#x27;liang&#x27;)elif sorce &gt;= 70: print(&#x27;jige&#x27;)else: print(&#x27;xiaolaji&#x27;)print(&#x27;=======&gt;&#x27;)&#x27;&#x27;&#x27;if嵌套if&#x27;&#x27;&#x27;# 嵌套案例age = 18is_beautiful = Truestar = &#x27;水瓶座&#x27;if age &gt; 16 and age &lt; 20 and is_beautiful and star == &#x27;水瓶座&#x27;: print(&#x27;表白&#x27;) is_beautiful = True if is_beautiful: print(&quot;成功了&quot;)else: print(&#x27;闹玩呢！&#x27;)print(&#x27;==============&#x27;) 三，循环结构3.1 什么是循环结构循环结构就是重复执行某段代码块 3.2 为什么要用循环结构 人类某些时候需要重复做某件事情 所以程序中必须有相应的机制来控制计算机具备人的这种循环做事的能力 3.3 如何使用循环结构3.3.1 while循环语法python中有while与for两种循环机制，其中while循环称之为条件循环，语法如下 1234567while 条件: 代码1 代码2 代码3while的运行步骤：步骤1：如果条件为真，那么依次执行：代码1、代码2、代码3、......步骤2：执行完毕后再次判断条件,如果条件为True则再次执行：代码1、代码2、代码3、......，如果条件为False,则循环终止 3.3.2 while循环应用案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209# 1、循环的语法与基本使用&#x27;&#x27;&#x27;while 条件: 代码1 代码2 代码3&#x27;&#x27;&#x27;count = 0while count &lt; 5: print(count) count += 1print(&#x27;dj=========&gt;&#x27;)# 2、死循环与效率问题count = 0while count &lt; 5: print(count)while True: name = input(&quot;your name &gt;&gt;&gt;&gt;&gt;&quot;) print(name)# 纯计算，无io的死循环会导致致命的效率问题# while True:# 1+1# 3、循环的应用username = &#x27;zhangsan&#x27;passwd = &#x27;123&#x27;# 两个问题：# 1、重复代码# 2、输对了不应该再重复while 1: inp_name = input(&#x27;请输入账号：&#x27;) inp_pwd = input(&#x27;请输入密码：&#x27;) if inp_name == username and inp_pwd == passwd: print(&#x27;登陆成功&#x27;) else: print(&#x27;账号加密码错误&#x27;)# 4、退出循环的两种方式# 方式一：将条件改为False，等到下次循环判断条件时才会生效tag = Truewhile tag: inp_name = input(&#x27;请输入账号：&#x27;) inp_pwd = input(&#x27;请输入密码：&#x27;) if inp_name == username and inp_pwd == passwd: print(&#x27;登陆成功&#x27;) tag = False # 之后的代码还会运行，下次循环判断条件时才会生效 else: print(&#x27;账号加密码错误&#x27;) print(&#x27;===end===&#x27;) # 这行代码亦然会执行# 方式二：break，只要运行到break就会立刻终止本层循环while True: inp_name = input(&#x27;请输入账号：&#x27;) inp_pwd = input(&#x27;请输入密码：&#x27;) if inp_name == username and inp_pwd == passwd: print(&#x27;登陆成功&#x27;) break # 立刻终止本层循环 else: print(&#x27;账号加密码错误&#x27;) print(&#x27;===end===&#x27;) # 这个代码不会执行# 7、while循环嵌套&#x27;&#x27;&#x27;tag = Truewhile tag: while tag: while tag: tag = False# 每一层都必须配一个breakwhile True: while True: while True: break break break&#x27;&#x27;&#x27;### break的方式while True: inp_name = input(&#x27;请输入账号：&#x27;) inp_pwd = input(&#x27;请输入密码：&#x27;) if inp_name == username and inp_pwd == passwd: print(&#x27;登陆成功&#x27;) while True: cmd=input(&#x27;输入编号&gt;&gt;:&#x27;) if cmd == &#x27;q&#x27;: break print(&#x27;命令&#123;x&#125;正在运行&#x27;.format(x=cmd)) break # 立刻终止本层循环 else: print(&#x27;账号加密码错误&#x27;)### 改变条件的方式tag = Truewhile tag: inp_name = input(&#x27;请输入账号：&#x27;) inp_pwd = input(&#x27;请输入密码：&#x27;) if inp_name == username and inp_pwd == passwd: print(&#x27;登陆成功&#x27;) while tag: cmd=input(&#x27;输入编号&gt;&gt;:&#x27;) if cmd == &#x27;q&#x27;: tag = False else: print(&#x27;命令&#123;x&#125;正在运行&#x27;.format(x=cmd)) else: print(&#x27;账号加密码错误&#x27;)# 8、while +continue # 结束本次循环，直接进入下一次循环# 强调：在continue之后添加同级代码毫无意义，因为永远无法运行count = 0while count &lt; 6: if count == 4: count += 1 continue # count += 1 # 错误 print(count) count += 1# 9、while +else # 针对breakcount = 0while count &lt; 6: if count == 4: count += 1 continue print(count) count += 1else: print(&#x27;else包含的代码会在while循环结束后，并且while循环是在没有被break打断的情况下正常结束的，才会运行&#x27;)# 不会运行count = 0while count &lt; 6: if count == 4: break print(count) count += 1else: print(&#x27;else包含的代码会在while循环结束后，并且while循环是在没有被break打断的情况下正常结束的，才会运行&#x27;)# 应用案例：# 版本1：count = 0tag = Truewhile tag: if count == 3: print(&#x27;输错超过三次，退出程序&#x27;) break inp_name = input(&#x27;请输入账号：&#x27;) inp_pwd = input(&#x27;请输入密码：&#x27;) if inp_name == username and inp_pwd == passwd: print(&#x27;登陆成功&#x27;) while tag: cmd=input(&#x27;输入编号&gt;&gt;:&#x27;) if cmd == &#x27;q&#x27;: tag = False else: print(&#x27;命令&#123;x&#125;正在运行&#x27;.format(x=cmd)) else: print(&#x27;账号加密码错误&#x27;) count += 1# 版本2：优化count = 0while count &lt; 3: inp_name = input(&#x27;请输入账号：&#x27;) inp_pwd = input(&#x27;请输入密码：&#x27;) if inp_name == username and inp_pwd == passwd: print(&#x27;登陆成功&#x27;) while True: cmd=input(&#x27;输入编号&gt;&gt;:&#x27;) if cmd == &#x27;q&#x27;: # 整个程序结束，退出while循环 break else: print(&#x27;命令&#123;x&#125;正在运行&#x27;.format(x=cmd)) break else: print(&#x27;账号加密码错误&#x27;) count += 1else: print(&#x27;输错三次，退出&#x27;) 3.3.3 for循环语法 循环结构的第二种实现方式是for循环，for循环可以做的事情while循环都可以实现，之所以用for循环是因为在循环取值（即遍历值）时for循环比while循环的使用更为简洁， for循环语法如下 123456789101112131、什么是for循环 循环就是重复做某件事，for循环是python提供的第二种循环机制2、为何要有for循环 理论上for能做的事情，while循环都可以做，之所以要有for循环，是因为for循环在循环取值（遍历取值）比while循环更简洁3、如何用for循环语法：for 变量名 in 可迭代对象: # 可迭代对象可以是：列表、字典、字符串、元组、集合 代码1 代码2 代码3 ... 3.3.4 for循环应用案例12345678910111213141516171819202122232425262728293031323334# 一、基本使用之循环取值# 案例1：列表循环取值# 简单版：l = [&#x27;zhangsan&#x27;,&#x27;lisi&#x27;,&#x27;wangwu&#x27;]for x in l: print(x)# 复杂版：l = [&#x27;zhangsan&#x27;,&#x27;lisi&#x27;,&#x27;wangwu&#x27;]i = 0while i &lt; 3: print(l[i]) i += 1# 案例2：字典循环取值# 简单版：dic = &#123;&#x27;k1&#x27;:111,&#x27;k2&#x27;:222,&#x27;k3&#x27;:333&#125;for k in dic: print(k,dic[k])# 复杂版：while循环可以遍历字典，太麻烦了# 案例3：字符串循环取值# 简单版：msg = &#x27;you can you up,no can no bb&#x27;for x in msg: print(x) 四，深浅copy123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100list1 = [ &#x27;zhangsan&#x27;, &#x27;lxx&#x27;, [1,2]]# 1、二者分隔不开，list1改，list2也跟着改，因为指向的就是同一个地址list2 = list1 # 这不叫copylist1[0] = &#x27;lisi&#x27;print(list2)# 2、需求：# 2.1、拷贝以下原列表，产生一个新的列表# 2.2、想让两个列表完全独立开，针对的是改操作的独立，而不是读操作# 3、如何copy列表# 3.1、浅copy&#x27;&#x27;&#x27;浅拷贝是把原列表第一层的内存地址不加区分完全copy一份给新的列表&#x27;&#x27;&#x27;list1 = [ &#x27;zhangsan&#x27;, &#x27;lxx&#x27;, [1,2]]list3 = list1.copy()print(list3)print(id(list1),id(list1[0]),id(list1[1]),id(list1[2]))print(id(list3),id(list3[0]),id(list3[1]),id(list3[2]))输出：[&#x27;zhangsan&#x27;, &#x27;lxx&#x27;, [1, 2]]4431313536 4431275824 4431313392 44313138564431653376 4431275824 4431313392 4431313856# 实验1：对于不可变类型的赋值，都是产生了新值，让原列表的索引指向新的的内存地址，并不会影响新列表list1[0] = &#x27;ZHANGSAN&#x27;list1[1] = &#x27;LXX&#x27;list1[2] = 123# 实验2：但对于可变类型，我们可以改变可变类型中包含的值，但内存地址不变，即原列表的索引指向仍然指向原来的内存地址，于是新的列表也跟着一起受影响。list1[2][0] = 111 # 改的是列表内的列表内的值list1[2][1] = 222print(list1)print(list3)输出：[&#x27;zhangsan&#x27;, &#x27;lxx&#x27;, [111, 222]][&#x27;zhangsan&#x27;, &#x27;lxx&#x27;, [111, 222]]&#x27;&#x27;&#x27;综合实验1和实验2可以得出，要想copy得到的新列表与原列表的改操作完全独立开，必须有一种可以区分开可变类型与不可变类型的copy机制，这就是深copy&#x27;&#x27;&#x27;# 3.2、深copyimport copylist1 = [ &#x27;zhangsan&#x27;, &#x27;lxx&#x27;, [1,2]]list3 = copy.deepcopy(list1)print(id(list1))print(id(list3))print(id(list1[0]),id(list1[1]),id(list1[2]))print(id(list3[0]),id(list3[1]),id(list3[2]))输出：432685017643268125444326434608 4326472496 43267624324326434608 4326472496 4327024704print(id(list3))print(id(list1[2][0]),id(list3[2][0]))list1[0] = &#x27;ZHANGSAN&#x27;list1[1] = &#x27;LXX&#x27;list1[2][0] = 111list1[2][1] = 222print(list1)print(list3)输出：44991802884497668400 4497668400[&#x27;ZHANGSAN&#x27;, &#x27;LXX&#x27;, [111, 222]][&#x27;zhangsan&#x27;, &#x27;lxx&#x27;, [1, 2]]","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"Python语法入门之与用户交互、运算符","slug":"pythonyufarumenzhiyonghujiaohu-yunsuanfu","date":"2023-03-16T06:31:43.000Z","updated":"2023-03-16T06:31:43.000Z","comments":true,"path":"articles/2023/03/16/pythonyufarumenzhiyonghujiaohu-yunsuanfu/","permalink":"https://kkabuzs.github.io/articles/2023/03/16/pythonyufarumenzhiyonghujiaohu-yunsuanfu/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 6、Python语法入门之与用户交互、运算符特别鸣谢： — b站老男孩agen老师 一，程序与用户交互1.1 什么是与用户交互 用户交互就是人往计算机中input&#x2F;输入数据，计算机print&#x2F;输出结果 1.2 为什么要与用户交互？ 为了让计算机能够像人一样与用户沟通交流 比如，过去我们去银行取钱，用户需要把帐号密码告诉柜员，而现在，柜员被ATM机取代，ATM机就是一台计算机，所以用户同样需要将帐号密码告诉计算机，于是我们的程序中必须有相应的机制来控制计算机接收用户输入的内容，并且输出结果 1.3 如何与用户交互 交互的本质就是输入、输出 1.3.1 输入input1234567# 在python3中，input会将用户输入的所有内容都存成字符串类型。username = input(&quot;请输入您的账号：&quot;) # &quot;zhangsan&quot;print(username,type(username))输出请输入您的账号：zhangsanzhangsan &lt;class &#x27;str&#x27;&gt; 1.3.2 输出print12345678910111213141516&gt;&gt;&gt; print(&#x27;hello world&#x27;) # 只输出一个值hello world&gt;&gt;&gt; print(&#x27;first&#x27;,&#x27;second&#x27;,&#x27;third&#x27;) # 一次性输出多个值，值用逗号隔开first second third# 默认print功能有一个end参数，该参数的默认值为&quot;\\n&quot;(代表换行)，可以将end参数的值改成任意其它字符print(&quot;aaaa&quot;,end=&#x27;&#x27;)print(&quot;bbbb&quot;,end=&#x27;&amp;&#x27;)print(&quot;cccc&quot;,end=&#x27;@&#x27;)#整体输出结果为：aaaabbbb&amp;cccc@age = input(&quot;请输入你的年龄：&quot;)print(age,type(age))age = int(age) # int只能将纯数字的字符串转成整型print(age &gt; 16) 1.3.3 输出之格式化输出 什么是格式化输出？ 把一段字符串里面的某些内容替换掉之后再输出，就是格式化输出。 为什么要格式化输出？ 我们经常会输出具有某种固定格式的内容，比如：’亲爱的xxx你好！你xxx月的话费是xxx，余额是xxx‘，我们需要做的就是将xxx替换为具体的内容。 如何格式化输出？ 3.1 %（最不推荐） 值按照位置与%s一一对应，少一个不行，多一个也不行 12345res = &quot;my name is %s my age is %s&quot; %(&#x27;zhangsan&#x27;,&quot;18&quot;)print(res)输出：my name is zhangsan my age is 18 以字典的形式传值，打破位置规则 12345res = &quot;我的名字是%(name)s 我的年龄是%(age)s&quot; %&#123;&quot;name&quot;:&quot;zhangsan&quot;,&quot;age&quot;:18&#125;print(res)输出：我的名字是zhangsan 我的年龄是18 123456789101112131415# %s可以接收任意类型print(&#x27;my age is %s&#x27; %18)print(&#x27;my age is %s&#x27; %[1,23])print(&#x27;my age is %s&#x27; %&#123;&#x27;a&#x27;:222&#125;)print(&#x27;my age is %d&#x27; %18) # %d 只能接受intprint(&#x27;my age is %s&#x27; %&#x27;aaa&#x27;)输出：my age is 18my age is [1, 23]my age is &#123;&#x27;a&#x27;: 222&#125;my age is 18my age is aaa 3.2 str.format：特点是兼容性好（速度第二） 12345678910111213141516171819202122232425262728293031323334353637383940# 按照位置取res = &#x27;我的名字是&#123;&#125; 我的年龄是&#123;&#125;&#x27;.format(&#x27;zhangsan&#x27;,18)print(res)输出：我的名字是zhangsan 我的年龄是18res = &#x27;我的名字是&#123;0&#125;&#123;0&#125; 我的年龄是&#123;1&#125;&#x27;.format(&#x27;zhangsan&#x27;,18)print(res)输出：我的名字是zhangsanzhangsan 我的年龄是18# 打破位置限制，按照key=value传值res = &quot;我的名字是&#123;name&#125; 我的年龄是&#123;age&#125;&quot;.format(age=18,name=&#x27;zhangsan&#x27;)print(res)输出：我的名字是zhangsan 我的年龄是18---# format新增：# print(&#x27;&#123;x&#125;********&#x27;.format(x=&#x27;开始执行&#x27;))print(&#x27;&#123;x:*&lt;10&#125;&#x27;.format(x=&#x27;开始执行&#x27;)) # &lt; 表示居左显示，*号填充，10代表一共十位print(&#x27;&#123;x:*&gt;10&#125;&#x27;.format(x=&#x27;开始执行&#x27;)) # &gt; 表示居右显示，*号填充，10代表一共十位print(&#x27;&#123;x:*^10&#125;&#x27;.format(x=&#x27;开始执行&#x27;)) # ^ 表示居中显示，*号填充，10代表一共十位# 四舍五入print(&#x27;&#123;sal:.3f&#125;&#x27;.format(sal=123132.12351)) # 精确到小数点后三位 .3f表示小数点后三位# f的新用法：&#123;&#125;内的字符串可以被当作表达式运行res = f&#x27;&#123;10 + 3&#125;&#x27;print(res)f&#x27;&#123;print(&quot;aaaa&quot;)&#125;&#x27; 3.3 f：python3.5之后才推出。（速度最快） 123456789x = input(&#x27;your name: &#x27;)y = input(&#x27;your age: &#x27;)res = f&#x27;我的名字是&#123;x&#125;,我的年龄是&#123;y&#125;&#x27;print(res)输出：your name: 张三your age: 18我的名字是张三,我的年龄是18 二，基本运算符2.1 算术运算符 ​ python支持的算数运算符与数学上计算的符号使用是一致的，我们以x&#x3D;9，y&#x3D;2为例来依次介绍它们 算数运算符 描述 示例 + 加，两个对象相加 x + y得11 - 减，两个对象相减 x - y得7 * 乘，两个对象相乘 x * y得18 &#x2F; 除，相除后得到的返回值保留整数与小数部分 x &#x2F; y得4.5 &#x2F;&#x2F; 取整除，相除后得到的返回值只保留整数部分 x &#x2F;&#x2F; y得4 % 取余，相除后只返回余数 x % y得1 ** 幂，取一个数的n次方 x ** y得81 2.2 比较运算符 比较运算用来对两个值进行比较，返回的是布尔值True或False，我们以x&#x3D;9，y&#x3D;2为例来依次介绍它们 比较运算符 描述 示例 &#x3D;&#x3D; 等于，两个对象相等则返回True，否则返回False x &#x3D;&#x3D; y得False !&#x3D; 不等于，两个对象相等则返回False，否则返回True x !&#x3D; y得True &gt; 大于 x &gt; y得True &gt;&#x3D; 大于等于 x &gt;&#x3D; y得True &lt; 小于 x &lt; y得False &lt;&#x3D; 小于等于 x &lt;&#x3D; y得False 2.3 赋值运算符 python语法中除了有&#x3D;号这种简单的赋值运算外，还支持增量赋值、链式赋值、交叉赋值、解压赋值，这些赋值运算符存在的意义都是为了让我们的代码看起来更加精简。我们以x&#x3D;9，y&#x3D;2为例先来介绍一下增量赋值 2.3.1 增量赋值|赋值运算符|描述|示例||-||&#x3D;|简单赋值运算|x &#x3D; 10||+&#x3D;|加法赋值运算|x &#x3D; 10，x +&#x3D; 1 等同于 x &#x3D; x + 1，x &#x3D; 11||-&#x3D;|减法赋值运算|y &#x3D; 20，y -&#x3D; 1 等同于 y &#x3D; y - 1，y &#x3D; 19||*&#x3D;|乘法赋值运算|同上||&#x2F;&#x3D;|除法赋值运算|同上||&#x2F;&#x2F;&#x3D;|取整除赋值运算|同上||%&#x3D;|取余赋值运算|同上||**&#x3D;|幂赋值运算|同上| 123456789age = 10age = age + 1age += 1 # 等于 age = age + 1print(age)age *= 3age /= 3age %= 3age **= 3 2.3.2 链式赋值123456x = 10y = xz = yz = y = x = 10 # 链式赋值print(x,y,z)print(id(x),id(y),id(z)) 2.3.3 交叉赋值123456789101112131415161718192021m = 10n = 20print(m,n)输出：10 20# 交换值temp = mm = nn = tempprint(m,n)输出：20 10m,n = n,m # 交叉赋值print(m,n)输出：10 20 2.3.4 解压赋值12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455salaris = [111,222,333,444,555]# 把五个月的工资取出来分别赋值给不同的变量名mon0 = salaris[0]mon1 = salaris[1]mon2 = salaris[2]mon3 = salaris[3]mon4 = salaris[4]# 解压赋值mon0,mon1,mon2,mon3,mon4 = salarisprint(mon0)print(mon1)print(mon2)print(mon3)print(mon4)输出：111222333444555# mon0,mon1,mon2,mon3 = salaris # 对应的变量名少一个不行# mon0,mon1,mon2,mon3,mon4,mon5 = salaris # 对应的变量名多一个也不行# 引用*可以帮助我们取两头的值，无法取中间的值# 取前三个值x,y,z,*_ = salaris = [111,222,333,444,555] # *会将没有对应关系的值存成列表然后赋值给紧跟其后的那个变量名，此时这个变量名为_print(x,y,z)输出：111 222 333# 取后三个值*_,x,y,z = salaris = [111,222,333,444,555]print(x,y,z)输出：333 444 555x,*_,y,z = salaris = [111,222,333,444,555]print(x,y,z)输出：111 444 555# 解压字典默认解压出来的是字典的keyx,y,z = dic = &#123;&#x27;a&#x27;:1,&#x27;b&#x27;:2,&#x27;c&#x27;:1&#125;print(x,y,z)输出：a b c 2.4 逻辑运算符 逻辑运算符用于连接多个条件，进行关联判断，会返回布尔值True或False |逻辑运算符|描述|示例||-||and|逻辑与，用于连接两个条件，两个条件同时成立时才返回True，但凡有一个条件为False则返回False|3&gt;1 and 4&#x3D;&#x3D;4，返回True||or|逻辑或，用于连接两个条件，两个条件但凡有一个成立就返回True，只有在两个条件同时为False则返回False|1&gt;2 or 3&gt;1，返回True||not|取反|not 1 &gt; 3 ，返回True| 2.4.1 连续多个and 可以用and连接多个条件，会按照从左到右的顺序依次判断，一旦某一个条件为False，则无需再往右判断，可以立即判定最终结果就为False，只有在所有条件的结果都为True的情况下，最终结果才为True。 12&gt;&gt;&gt; 2 &gt; 1 and 1 != 1 and True and 3 &gt; 2 # 判断完第二个条件，就立即结束，得的最终结果为FalseFalse 2.4.2 连续多个or 可以用or连接多个条件，会按照从左到右的顺序依次判断，一旦某一个条件为True，则无需再往右判断，可以立即判定最终结果就为True，只有在所有条件的结果都为False的情况下，最终结果才为False 12&gt;&gt;&gt; 2 &gt; 1 or 1 != 1 or True or 3 &gt; 2 # 判断完第一个条件，就立即结束，得的最终结果为TrueTrue 2.4.3 优先级not&gt;and&gt;or12345678910111213141516171819202122232425262728# not：把紧跟其后的那个条件取反# ps：not与紧跟其后的那个条件是一个不可分割的整体print(not 16 &gt; 13)print(not True)print(not 10)print(not 0)print(not None)print(not &#x27;&#x27;)# and：逻辑与，and用来链接左右连个条件，两个结果同时为真，最终结果才为真print(True and 10 &gt; 3)print(True and 10 &gt; 3 and 10 and 0) # 条件全为真，最终结果才为Trueprint(True and 10 &gt; 3 and 10 and 0 and 1 &gt; 3 and 4 == 4) # 偷懒原则# or：逻辑或，只要有一个条件为真，最终结果为真print(3&gt;2 or 0)print(10 or 3 != 2 or 3 &gt; 2 or True) # 偷懒原则# 优先级# 区分优先级：not &gt; and &gt; or# ps：如果单独就只是一串and链接，或者说单独只是一串or链接，按照从左导游的顺序依次运算即可（偷懒原则）# 如果是混用，则需要考虑优先级res = 3 &gt; 4 and not 4 &gt; 3 or 1 == 3 and &#x27;x&#x27; == &#x27;x&#x27; or 3 &gt; 3print(res)res = (3 &gt; 4 and (not 4 &gt; 3)) or (1 == 3 and &#x27;x&#x27; == &#x27;x&#x27;) or 3 &gt; 3print(res) 2.5 成员运算符|成员运算符|描述||-||in|某一个对象包含于另一个对象，则返回True。字符串、列表、元组、字典、集合都支持成员运算符||not in|某一个对象没有包含于另一个对象，则返回True| 12345678910111213# 1、成员运算符print(&#x27;zhang&#x27; in &#x27;zhangsan&#x27;) # 判断一个字符串是否存在于一个大字符串中print(&#x27;z&#x27; in &#x27;zhangsan&#x27;) # 判断一个字符串是否存在于一个大字符串中print(111 in [111,222,333]) # 判断元素是否存在与列表# 判断key是否存在于字典print(111 in &#123;&#x27;k1&#x27;:111,&#x27;k2&#x27;:222&#125;)print(&#x27;k1&#x27; in &#123;&#x27;k1&#x27;:111,&#x27;k2&#x27;:222&#125;)# not inprint(&#x27;zhang&#x27; not in &#x27;zhangsan&#x27;) # 推荐使用print(not &#x27;zhang&#x27; in &#x27;zhangsan&#x27;) # 逻辑同上，但语义不明确，不推荐 2.6 身份运算符|身份运算符|描述||-||is|is比较的是id，两个对象的id相同则返回True||is not|在两个对象点id不同时，is not会返回True| 需要强调的是：&#x3D;&#x3D;双等号比较的是value是否相等，而is比较的是id是否相等 1234567891011121314#1. id相同，内存地址必定相同，意味着type和value必定相同#2. value相同type肯定相同，但id可能不同,如下&gt;&gt;&gt; x=&#x27;Info Tony:18&#x27;&gt;&gt;&gt; y=&#x27;Info Tony:18&#x27;&gt;&gt;&gt; id(x),id(y) # x与y的id不同，但是二者的值相同(4327422640, 4327422256)&gt;&gt;&gt; x == y # 等号比较的是valueTrue&gt;&gt;&gt; type(x),type(y) # 值相同type肯定相同(&lt;class &#x27;str&#x27;&gt;, &lt;class &#x27;str&#x27;&gt;)&gt;&gt;&gt; x is y # is比较的是id，x与y的值相等但id可以不同False","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"Python语法入门之垃圾回收机制","slug":"pythonyufarumenzhilajihuishoujizhi","date":"2023-03-16T03:40:30.000Z","updated":"2023-03-16T03:40:30.000Z","comments":true,"path":"articles/2023/03/16/pythonyufarumenzhilajihuishoujizhi/","permalink":"https://kkabuzs.github.io/articles/2023/03/16/pythonyufarumenzhilajihuishoujizhi/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 5、Python语法入门之垃圾回收机制特别鸣谢： — b站老男孩agen老师 一，引入 解释器在执行到定义变量的语法时，会申请内存空间来存放变量的值，而内存的容量是有限的，这就涉及到变量值所占用内存空间的回收问题，当一个变量值没有用了（简称垃圾）就应该将其占用的内存给回收掉，那什么样的变量值是没有用的呢？ 单从逻辑层面分析，我们定义变量将变量值存起来的目的是为了以后取出来使用，而取得变量值需要通过其绑定的直接引用（如x&#x3D;10，10被x直接引用）或间接引用（如l&#x3D;[x,]，x&#x3D;10，10被x直接引用，而被容器类型l间接引用），所以当一个变量值不再绑定任何引用时，我们就无法再访问到该变量值了，该变量值自然就是没有用的，就应该被当成一个垃圾回收。 毫无疑问，内存空间的申请与回收都是非常耗费精力的事情，而且存在很大的危险性，稍有不慎就有可能引发内存溢出问题，好在Cpython解释器提供了自动的垃圾回收机制来帮我们解决了这件事。 二，什么是垃圾回收机制？垃圾回收机制（简称GC）是Python解释器自带一种机制，专门用来回收不可用的变量值所占用的内存空间 三，为什么要用垃圾回收机制？ 程序运行过程中会申请大量的内存空间，而对于一些无用的内存空间如果不及时清理的话会导致内存使用殆尽（内存溢出），导致程序崩溃，因此管理内存是一件重要且繁杂的事情，而python解释器自带的垃圾回收机制把程序员从繁杂的内存管理中解放出来。 四，垃圾回收机制原理分析 Python的GC模块主要运用了“引用计数”（reference counting）来跟踪和回收垃圾。在引用计数的基础上，还可以通过“标记-清除”（mark and sweep）解决容器对象可能产生的循环引用的问题，并且通过“分代回收”（generation collection）以空间换取时间的方式来进一步提高垃圾回收的效率。 4.1 什么是引用计数？12345引用计数就是：变量值被变量名关联的次数如：age=18变量值18被关联了一个变量名age，称之为引用计数为1 12345引用计数增加：age=18 （此时，变量值18的引用计数为1）m=age （把age的内存地址给了m，此时，m,age都关联了18，所以变量值18的引用计数为2） 123456引用计数减少：age=10（名字age先与值18解除关联，再与3建立了关联，变量值18的引用计数为1）del m（del的意思是解除变量名x与变量值18的关联关系，此时，变量18的引用计数为0） 值18的引用计数一旦变为0，其占用的内存地址就应该被解释器的垃圾回收机制回收 4.2 引用计数扩展阅读变量值被关联次数的增加或减少，都会引发引用计数机制的执行（增加或减少值的引用计数），这存在明显的效率问题。 如果说执行效率还仅仅是引用计数机制的一个软肋的话，那么很不幸，引用计数机制还存在着一个致命的弱点，即循环引用（也称交叉引用） 123456789101112131415# 如下我们定义了两个列表，简称列表1与列表2，变量名l1指向列表1，变量名l2指向列表2&gt;&gt;&gt; l1=[&#x27;xxx&#x27;] # 列表1被引用一次，列表1的引用计数变为1 &gt;&gt;&gt; l2=[&#x27;yyy&#x27;] # 列表2被引用一次，列表2的引用计数变为1 &gt;&gt;&gt; l1.append(l2) # 把列表2追加到l1中作为第二个元素，列表2的引用计数变为2&gt;&gt;&gt; l2.append(l1) # 把列表1追加到l2中作为第二个元素，列表1的引用计数变为2# l1与l2之间有相互引用# l1 = [&#x27;xxx&#x27;的内存地址,列表2的内存地址]# l2 = [&#x27;yyy&#x27;的内存地址,列表1的内存地址]&gt;&gt;&gt; l1[&#x27;xxx&#x27;, [&#x27;yyy&#x27;, [...]]]&gt;&gt;&gt; l2[&#x27;yyy&#x27;, [&#x27;xxx&#x27;, [...]]]&gt;&gt;&gt; l1[1][4][0]&#x27;xxx&#x27; 循环引用会导致：值不再被任何名字关联，但是值的引用计数并不会为0，应该被回收但不能被回收，什么意思呢？试想一下，请看如下操作 12&gt;&gt;&gt; del l1 # 列表1的引用计数减1，列表1的引用计数变为1&gt;&gt;&gt; del l2 # 列表2的引用计数减1，列表2的引用计数变为1 此时，只剩下列表1与列表2之间的相互引用，两个列表的引用计数均不为0，但两个列表不再被任何其他对象关联，没有任何人可以再引用到它们，所以它俩占用内存空间应该被回收，但由于相互引用的存在，每一个对象的引用计数都不为0，因此这些对象所占用的内存永远不会被释放，所以循环引用是致命的，这与手动进行内存管理所产生的内存泄露毫无区别。 所以Python引入了“标记-清除” 与“分代回收”来分别解决引用计数的循环引用与效率低的问题 4.2.1 标记-清除 容器对象（比如：list，set，dict，class，instance）都可以包含对其他对象的引用，所以都可能产生循环引用。而“标记-清除”计数就是为了解决循环引用的问题。 在了解标记清除算法前，我们需要明确一点，关于变量的存储，内存中有两块区域：堆区与栈区，在定义变量时，变量名与值内存地址的关联关系存放于栈区，变量值存放于堆区，内存管理回收的则是堆区的内容，详解如下图, 定义了两个变量x &#x3D; 10、y &#x3D; 20 当我们执行x&#x3D;y时，内存中的栈区与堆区变化如下 标记/清除算法的做法是当应用程序可用的内存空间被耗尽的时，就会停止整个程序，然后进行两项工作，第一项则是标记，第二项则是清除 1234567#1、标记通俗地讲就是：标记的过程就行相当于从栈区出发一条线，“连接”到堆区，再由堆区间接“连接”到其他地址，凡是被这条自栈区起始的线连接到内存空间都属于可以访达的，会被标记为存活具体地：标记的过程其实就是，遍历所有的GC Roots对象(栈区中的所有内容或者线程都可以作为GC Roots对象），然后将所有GC Roots的对象可以直接或间接访问到的对象标记为存活的对象，其余的均为非存活对象，应该被清除。 #2、清除清除的过程将遍历堆中所有的对象，将没有标记存活的对象全部清除掉。 直接引用指的是从栈区出发直接引用到的内存地址，间接引用指的是从栈区出发引用到堆区后再进一步引用到的内存地址，以我们之前的两个列表l1与l2为例画出如下图像 当我们同时删除l1与l2时，会清理到栈区中l1与l2的内容 这样在启用标记清除算法时，发现栈区内不再有l1与l2（只剩下堆区内二者的相互引用），于是列表1与列表2都没有被标记为存活，二者会被清理掉，这样就解决了循环引用带来的内存泄漏问题。 4.2.2 分代回收 背景：基于引用计数的回收机制，每次回收内存，都需要把所有对象的引用计数都遍历一遍，这是非常消耗时间的，于是引入了分代回收来提高回收效率，分代回收采用的是用“空间换时间”的策略。 分代： 分代回收的核心思想是：在历经多次扫描的情况下，都没有被回收的变量，gc机制就会认为，该变量是常用变量，gc对其扫描的频率会降低，具体实现原理如下： 123分代指的是根据存活时间来为变量划分不同等级（也就是不同的代）新定义的变量，放到新生代这个等级中，假设每隔1分钟扫描新生代一次，如果发现变量依然被引用，那么该对象的权重（权重本质就是个整数）加一，当变量的权重大于某个设定得值（假设为3），会将它移动到更高一级的青春代，青春代的gc扫描的频率低于新生代（扫描时间间隔更长），假设5分钟扫描青春代一次，这样每次gc需要扫描的变量的总个数就变少了，节省了扫描的总时间，接下来，青春代中的对象，也会以同样的方式被移动到老年代中。也就是等级（代）越高，被垃圾回收机制扫描的频率越低 回收： 回收依然是使用引用计数作为回收的依据 虽然分代回收可以起到提升效率的效果，但也存在一定的缺点： 例如一个变量刚刚从新生代移入青春代，该变量的绑定关系就解除了，该变量应该被回收，但青春代的扫描频率低于新生代，所以该变量的回收就会被延迟。","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"Python语法入门之基本数据类型","slug":"pythonyufarumenzhishujuleixing","date":"2023-03-16T03:35:37.000Z","updated":"2023-03-16T03:35:37.000Z","comments":true,"path":"articles/2023/03/16/pythonyufarumenzhishujuleixing/","permalink":"https://kkabuzs.github.io/articles/2023/03/16/pythonyufarumenzhishujuleixing/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 4、Python语法入门之基本数据类型特别鸣谢： — b站老男孩agen老师 一，引入 我们学习变量是为了让计算机能够像人一样去记忆事物的某种状态，而变量的值就是用来存储事物状态的，很明显事物的状态分成不同种类的（比如人的年龄，身高，职位，工资等等），所以变量值也应该有不同的类型，例如 123salary = 3.1 # 用浮点型去记录薪资age = 18 # 用整型去记录年龄name = &#x27;lisi&#x27; # 用字符串类型去记录人名 二，数字类型2.1 int整型2.1.1 作用记录年龄、身份证号、个数等整数相关的状态 2.1.2 定义1234age = 18# 查看类型print(type(age)) # &lt;class &#x27;int&#x27;&gt; 2.2 float浮点型2.2.1 作用用来记录人的身高，体重，薪资等小数相关的状态 2.2.2 定义123456height=173.3weight=73.5salary=15000.89# 查看类型print(type(height)) # &lt;class &#x27;float&#x27;&gt; 2.3 数字类型的使用123456789101112131415161718192021# 数学运算level = 1level = level + 1print(level)print(10 * 3)print(10+3.3) # int 与 float之间可以相加# 输出23013.3# 比较大小age = 19print(age &gt; 18)# 输出True 三，字符串类型str3.1 作用用来记录人的名字，家庭住址，性别等描述性质的状态 3.2 定义12345name = &#x27;zhangsan&#x27;address = &#x27;北京市海淀区&#x27;sex = &#x27;男&#x27; 用单引号、双引号、多引号，都可以定义字符串，本质上是没有区别的，但是 12345678#1、需要考虑引号嵌套的配对问题msg = &quot;My name is Tony , I&#x27;m 18 years old!&quot; #内层有单引号，外层就需要用双引号#2、多引号可以写多行字符串msg = &#x27;&#x27;&#x27; 天下只有两种人。比如一串葡萄到手，一种人挑最好的先吃，另一种人把最好的留到最后吃。 照例第一种人应该乐观，因为他每吃一颗都是吃剩的葡萄里最好的；第二种人应该悲观，因为他每吃一颗都是吃剩的葡萄里最坏的。 不过事实却适得其反，缘故是第二种人还有希望，第一种人只有回忆。 &#x27;&#x27;&#x27; 3.3 使用1234567数字可以进行加减乘除等运算，字符串呢？也可以，但只能进行&quot;相加&quot;和&quot;相乘&quot;运算。&gt;&gt;&gt; name = &#x27;tony&#x27;&gt;&gt;&gt; age = &#x27;18&#x27;&gt;&gt;&gt; name + age #相加其实就是简单的字符串拼接&#x27;tony18&#x27;&gt;&gt;&gt; name * 5 #相乘就相当于将字符串相加了5次&#x27;tonytonytonytonytony&#x27; 使用案例 123456789101112131415161718192021222324252627282930313233343536373839404142name = &quot;zhangsan&quot;print(name)输出zhangsanx = 18print(type(x))x = &quot;18&quot; # 由数字组成的字符串，是字符串类型，不是int类型print(type(x))输出&lt;class &#x27;int&#x27;&gt;&lt;class &#x27;str&#x27;&gt;# &#x27;name&#x27; = &quot;zhangsan&quot; # 语法错误，等号左边是变量名，变量名的命名不能有引号# xxx # 代表访问变量的名字# &#x27;xxx&#x27; # 代表的是值# 其他作用：# 字符串的嵌套，注意：外层用单引号，内层用双引号，反之亦然print(&#x27;my name is &quot;zhangsan&quot;&#x27;)print(&#x27;my name is \\&#x27;zhangsan\\&#x27;&#x27;)输出my name is &quot;zhangsan&quot;my name is &#x27;zhangsan&#x27;# 字符串之间可以相加，但仅限于str与str之间进行，代表字符串的拼接。了解即可，不推荐使用，因为str之间的相加效率非常低print(&#x27;my name is &#x27;+&#x27;zhangsan&#x27; )输出my name is zhangsanprint(&#x27;=&#x27;*20)print(&#x27;hello world&#x27;)print(&#x27;=&#x27;*20)输出====================hello world==================== 四，列表list4.1 作用按位置记录多个值（同一个人的多个爱好、同一个班级的多个姓名等），并且可以按照索引取指定位置的值 4.2 定义在[]内用逗号分隔开多个任意类型的值，一个值称之为一个元素 1l = [10,3.1,&#x27;aaa&#x27;,[&#x27;bbb&#x27;,&#x27;ccc&#x27;],&#x27;ddd&#x27;] 4.3 使用1234567891011121314151617181920212223242526# 列表类型是用索引来对应值，索引代表的是数据的位置，从0开始计数l = [10,3.1,&#x27;aaa&#x27;,[&#x27;bbb&#x27;,&#x27;ccc&#x27;],&#x27;ddd&#x27;]print(l)print(l[1])print(l[2])print(l[3][1]) # 取列表里的cccprint(l[-1]) # 倒着取 -1输出[10, 3.1, &#x27;aaa&#x27;, [&#x27;bbb&#x27;, &#x27;ccc&#x27;], &#x27;ddd&#x27;]3.1aaacccddd# 列表可以嵌套，嵌套取值如下students_info=[ [&#x27;tony&#x27;,18,[&#x27;jack&#x27;,]], [&#x27;jason&#x27;,18,[&#x27;play&#x27;,&#x27;sleep&#x27;]]]# 取出第一个学生的第一个爱好print(students_info[0][2][0])输出jack 五，字典dict5.1 作用key对应值，其中key通常为字符串类型，所以key对值可以有描述性的功能，默认是无序的 索引反应的是顺序、位置，对值没有任何描述性的功能 用来存多个值，每个值都有唯一一个key与其对应，key对值有描述行功能 5.2 定义1234567d = &#123;&#x27;a&#x27;:1,&#x27;b&#x27;:2&#125;print(type(d))print(d[&#x27;a&#x27;])输出&lt;class &#x27;dict&#x27;&gt;1 5.3 使用123456789101112131415161718192021person_info=&#123; &#x27;name&#x27;:&#x27;tony&#x27;, &#x27;age&#x27;:18, &#x27;height&#x27;:185.3&#125;print(person_info[&#x27;name&#x27;])输出tony# 字典可以嵌套，嵌套取值如下students=[ &#123;&#x27;name&#x27;:&#x27;tony&#x27;,&#x27;age&#x27;:38,&#x27;hobbies&#x27;:[&#x27;play&#x27;,&#x27;sleep&#x27;]&#125;, &#123;&#x27;name&#x27;:&#x27;jack&#x27;,&#x27;age&#x27;:18,&#x27;hobbies&#x27;:[&#x27;read&#x27;,&#x27;sleep&#x27;]&#125;, &#123;&#x27;name&#x27;:&#x27;rose&#x27;,&#x27;age&#x27;:58,&#x27;hobbies&#x27;:[&#x27;music&#x27;,&#x27;read&#x27;,&#x27;sleep&#x27;]&#125;, ]print(students[1][&#x27;hobbies&#x27;][1]) #取第二个学生的第二个爱好输出sleep 六，布尔bool6.1 作用用来记录真假这两种状态 6.2 定义123456is_ok = Trueis_ok = Falseprint(type(is_ok))输出&lt;class &#x27;bool&#x27;&gt; 6.3 使用通常用来当作判断的条件，我们将在if判断中用到它","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"Python语法入门之变量","slug":"pythonyufarumenzhibianliang","date":"2023-03-16T02:58:53.000Z","updated":"2023-03-16T02:58:53.000Z","comments":true,"path":"articles/2023/03/16/pythonyufarumenzhibianliang/","permalink":"https://kkabuzs.github.io/articles/2023/03/16/pythonyufarumenzhibianliang/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 3、Python语法入门之变量特别鸣谢： — b站老男孩agen老师 一，引入 我们知道学习 Python 语言的目的是为了与计算机进行沟通&#x2F;交流，从而控制计算机帮助我们做一些事情，所以，在 Python 语言的所有语法中，每个语法存在的意义都是为了让计算机能够像人类一样，直白地讲，就是让计算机具备我们人类的某一项技能。这是我们理解后续所有 Python 语法的根本，一定要熟记。 二，变量2.1 什么是变量？变量就是可以变化的量，量指的是事物的状态，比如人的年龄、性别，游戏角色的等级、金钱等等 2.2 为什么要有变量？为了让计算机能够像人一样去记忆事物的某种状态，并且状态是可以发生变化的 详细地说： 程序执行的本质就是一系列状态的变化，变是程序执行的直接体现，所以我们需要有一种机制能够反映或者说是保存下来程序执行时状态，以及状态的变化。 2.3 怎么使用变量（先定义、后使用）2.3.1 变量的定义与使用 变量的定义由三部分组成，如下图 1234name = &#x27;Jason&#x27; # 记下人的名字为&#x27;Jason&#x27;sex = &#x27;男&#x27; # 记下人的性别为男性age = 18 # 记下人的年龄为18岁salary = 30000.1 # 记下人的薪资为30000.1元 解释器执行到变量定义的代码时会申请内存空间存放变量值，然后将变量值的内存地址绑定给变量名，以变量的定义age&#x3D;18为例，如下图 插图：定义变量申请内存 通过变量名即可引用到对应的值 12# 通过变量名即可引用到值，我们可以结合print()功能将其打印出来print(age) # 通过变量名age找到值18，然后执行print(18),输出：18 2.3.2 变量名的命名规范 变量名的命名应该见名知意 1234# 如果我们要存储的数据18代表的是一个人的年龄，那么变量名推荐命名为ageage = 18 # 如果我们要存储的数据18代表的是一个人的等级，那么变量名推荐命名为levellevel = 18 其他详细规范如下 12345678910111213141516171819# 命名规范1. 变量名只能是 字母、数字或下划线的任意组合2. 变量名的第一个字符不能是数字3. 关键字不能声明为变量名，常用关键字如下[&#x27;and&#x27;, &#x27;as&#x27;, &#x27;assert&#x27;, &#x27;break&#x27;, &#x27;class&#x27;, &#x27;continue&#x27;, &#x27;def&#x27;, &#x27;del&#x27;, &#x27;elif&#x27;, &#x27;else&#x27;, &#x27;except&#x27;, &#x27;exec&#x27;, &#x27;finally&#x27;, &#x27;for&#x27;, &#x27;from&#x27;,&#x27;global&#x27;, &#x27;if&#x27;, &#x27;import&#x27;, &#x27;in&#x27;, &#x27;is&#x27;, &#x27;lambda&#x27;, &#x27;not&#x27;, &#x27;or&#x27;, &#x27;pass&#x27;, &#x27;print&#x27;, &#x27;raise&#x27;, &#x27;return&#x27;, &#x27;try&#x27;, &#x27;while&#x27;, &#x27;with&#x27;, &#x27;yield&#x27;]# 错误示范如下：*a=123$b=456c$=7892_name=&#x27;lili&#x27;123=&#x27;lili&#x27;and=123年龄=18 # 强烈建议不要使用中文命名# 正确示范如下age_of_jason=31page1=&#x27;首页&#x27;_class=&#x27;终极一班&#x27; 2.3.3 变量名的命名风格1234567# 风格一：驼峰体AgeOfTony = 56 NumberOfStudents = 80# 风格二：纯小写下划线(在python中，变量名的命名推荐使用该风格)age_of_tony = 56 number_of_students = 80 2.3.4 变量值的三大特性变量的值具备三大特性 12345678#1、id反应的是变量在内存中的唯一编号，内存地址不同id肯定不同#2、type变量值的类型#3、value变量值 查看变量值三大特性的方式如下，我们将会在运算符中用到变量值的三大特性 12345678➜ ~ python3Python 3.9.13 (v3.9.13:6de2ca5339, May 17 2022, 11:37:23)[Clang 13.0.0 (clang-1300.0.29.30)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; x = &#x27;hello&#x27;&gt;&gt;&gt; id(x),type(x),x(4389636528, &lt;class &#x27;str&#x27;&gt;, &#x27;hello&#x27;)&gt;&gt;&gt; 三，常量3.1 什么是常量？常量指在程序运行过程中不会改变的量 3.2 为什么要有常量？在程序运行过程中，有些值是固定的、不应该被改变，比如圆周率 3.141592653... 3.3 怎么使用常量？在Python中没有一个专门的语法定义常量，约定俗成是用全部大写的变量名表示常量。如：PI=3.14159。所以单从语法层面去讲，常量的使用与变量完全一致。","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"编程语言与Python介绍","slug":"bianchengyuyanpythonjieshao","date":"2023-03-16T02:37:17.000Z","updated":"2023-03-16T02:37:17.000Z","comments":true,"path":"articles/2023/03/16/bianchengyuyanpythonjieshao/","permalink":"https://kkabuzs.github.io/articles/2023/03/16/bianchengyuyanpythonjieshao/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 2、编程语言与Python介绍特别鸣谢： — b站老男孩agen老师 一，引子 基于上一章所学，有了计算机硬件，再在硬件之上安装好操作系统，我们就有了一个应用程序的运行平台，我们接下来的任务就是学习如何使用某款编程语言来开发应用程序。 本章的主题是先带大家了解下编程语言，然后重点介绍python这门编程语言 二，编程语言分类2.1 机器语言 机器语言是站在计算机(奴隶)的角度，说计算机能听懂&#x2F;理解的语言，而计算机能直接理解的就是二进制指令，所以机器语言就是直接用二进制编程，这意味着机器语言是直接操作硬件的，因此机器语言属于低级语言，此处的低级指的是底层、贴近计算机硬件（贴近代指需要详细了解计算机硬件细节、直接控制硬件），详解如下 12345678910111213141516171819202122232425262728#机器语言 用二进制代码0和1描述的指令称为机器指令，由于计算机内部是基于二进制指令工作的，所以机器语言是直接控制计算机硬件。 用机器语言编写程序，编程人员要首先熟记所用计算机的全部指令代码以及代码的含义，然后在编写程序时，程序员得自己处理每条指令和每一数据的存储分配和输入输出，还得记住编程过程中每步所使用的工作单元处在何种状态。这是一件十分繁琐的工作。编写程序花费的时间往往是实际运行时间的几十倍或几百倍。而且，编出的程序全是些0和1的指令代码，直观性差，不便阅读和书写，还容易出错，且依赖于具体的计算机硬件型号，局限性很大。除了计算机生产厂家的专业人员外，绝大多数的程序员已经不再去学习机器语言了。 机器语言是被微处理器理解和使用的，存在有多至100000种机器语言的指令，下述是一些简单示例 #指令部份的示例 0000 代表 加载（LOAD） 0001 代表 存储（STORE） ... #暂存器部份的示例 0000 代表暂存器 A 0001 代表暂存器 B ... #存储器部份的示例 000000000000 代表地址为 0 的存储器 000000000001 代表地址为 1 的存储器 000000010000 代表地址为 16 的存储器 100000000000 代表地址为 2^11 的存储器 #集成示例 0000,0000,000000010000 代表 LOAD A, 16 0000,0001,000000000001 代表 LOAD B, 1 0001,0001,000000010000 代表 STORE B, 16 0001,0001,000000000001 代表 STORE B, 1[1] 总结机器语言 12345678# 1、执行效率最高编写的程序可以被计算机无障碍理解、直接运行，执行效率高 。# 2、开发效率最低复杂，开发效率低# 3、跨平台性差贴近/依赖具体的硬件，跨平台性差 2.2 汇编语言 汇编语言仅仅是用一个英文标签代表一组二进制指令，毫无疑问，比起机器语言，汇编语言是一种进步，但汇编语言的本质仍然是直接操作硬件，因此汇编语言仍是比较低级&#x2F;底层的语言、贴近计算机硬件 123456789101112131415161718192021#汇编语言汇编语言的实质和机器语言是相同的，都是直接对硬件操作，只不过指令采用了英文缩写的标识符，更容易识别和记忆。它同样需要编程者将每一步具体的操作用命令的形式写出来。汇编程序的每一句指令只能对应实际操作过程中的一个很细微的动作。例如移动、自增，因此汇编源程序一般比较冗长、复杂、容易出错，而且使用汇编语言编程需要有更多的计算机专业知识，但汇编语言的优点也是显而易见的，用汇编语言所能完成的操作不是一般高级语言所能够实现的，而且源程序经汇编生成的可执行文件不仅比较小，而且执行速度很快。汇编的hello world，打印一句hello world, 需要写十多行，如下; hello.asm section .data ; 数据段声明 msg db &quot;Hello, world!&quot;, 0xA ; 要输出的字符串 len equ $ - msg ; 字串长度 section .text ; 代码段声明 global _start ; 指定入口函数 _start: ; 在屏幕上显示一个字符串 mov edx, len ; 参数三：字符串长度 mov ecx, msg ; 参数二：要显示的字符串 mov ebx, 1 ; 参数一：文件描述符(stdout) mov eax, 4 ; 系统调用号(sys_write) int 0x80 ; 调用内核功能 ; 退出程序 mov ebx, 0 ; 参数一：退出代码 mov eax, 1 ; 系统调用号(sys_exit) int 0x80 ; 调用内核功能 总结汇编语言 12345678# 1、执行效率高相对于机器语言，使用英文标签编写程序相对简单，执行效率高，但较之机器语言稍低，# 2、开发效率低：仍然是直接操作硬件，比起机器语言来说，复杂度稍低，但依旧居高不下，所以开发效率依旧较低# 3、跨平台性差同样依赖具体的硬件，跨平台性差 2.3 高级语言 高级语言是站在人(奴隶主)的角度，说人话，即用人类的字符去编写程序，而人类的字符是在向操作系统发送指令，而非直接操作硬件，所以高级语言是与操作系统打交道的，此处的高级指的是高层、开发者无需考虑硬件细节，因而开发效率可以得到极大的提升，但正因为高级语言离硬件较远，更贴近人类语言，人类可以理解，而计算机则需要通过翻译才能理解，所以执行效率会低于低级语言。 按照翻译的方式的不同，高级语言又分为两种： 2.3.1 编译型（如C语言） 类似谷歌翻译，是把程序所有代码编译成计算机能识别的二进制指令，之后操作系统会拿着编译好的二进制指令直接操作硬件，详细如下 12345678910111213141516# 1、执行效率高编译是指在应用源程序执行之前，就将程序源代码“翻译”成目标代码（即机器语言），因此其目标程序可以脱离其语言环境独立执行，使用比较方便，执行效率较高。# 2、开发效率低：应用程序一旦需要修改，必须先修改源代码，然后重新编译、生成新的目标文件才能执行，而在只有目标文件而没有源代码，修改会很不方便。所以开发效率低于解释型# 3、跨平台性差编译型代码是针对某一个平台翻译的，当前平台翻译的结果无法拿到不同的平台使用，针对不同的平台必须重新编译，即跨平台性差# 其他现在大多数的编程语言都是编译型的。编译程序将源程序翻译成目标程序后保存在另一个文件中，该目标程序可脱离编译程序直接在计算机上多次运行。大多数软件产品都是以目标程序形式发行给用户的，不仅便于直接运行，同时又使他人难于盗用其中的技术。C、C++、Ada、Pascal都是编译实现的 2.3.2 解释型（如python） 类似同声翻译，需要有一个解释器，解释器会读取程序代码，一边翻译一边执行，详细如下 123456789101112131415161718# 1、执行效率低解释型语言的实现中，翻译器并不产生目标机器代码，而是产生易于执行的中间代码。这种中间代码与机器代码是不同的，中间代码的解释是由软件支持的，不能直接使用硬件，软件解释器通常会导致执行效率较低。# 2、开发效率高用解释型语言编写的程序是由另一个可以理解中间代码的解释程序执行的，与编译程序不同的是，解释程序的任务是逐一将源程序的语句解释成可执行的机器指令，不需要将源程序翻译成目标代码再执行。解释程序的优点是当语句出现语法错误时，可以立即引起程序员的注意，而程序员在程序开发期间就能进行校正。# 3、跨平台性强代码运行是依赖于解释器，不同平台有对应版本的解释器，所以解释型的跨平台性强# 其他对于解释型Basic语言，需要一个专门的解释器解释执行Basic程序，每条语句只有在执行时才被翻译，这种解释型语言每执行一次就翻译一次，因而效率低下。一般地，动态语言都是解释型的，例如：Tcl、Perl、Ruby、VBScript、JavaScript等 ps：混合型语言 Java是一类特殊的编程语言，Java程序也需要编译，但是却没有直接编译为机器语言，而是编译为字节码， 然后在Java虚拟机上以解释方式执行字节码。 2.4 总结 综上选择不同编程语言来开发应用程序对比 12345#1、执行效率：机器语言&gt;汇编语言&gt;高级语言（编译型&gt;解释型）#2、开发效率：机器语言&lt;汇编语言&lt;高级语言（编译型&lt;解释型）#3、跨平台性：解释型具有极强的跨平台型 三，python介绍 谈及python，涉及两层意思，一层代表的是python这门语言的语法风格，另外一层代表的则是专门用来解释该语法风格的应用程序：python解释器。 python的创始人为吉多·范罗苏姆（Guido van Rossum）。Python这个名字，来自Guido所挚爱的电视剧Monty Python’s Flying Circus，他希望这个新的叫做Python的语言，能符合他的理想：创造一种C和shell之间，语法能够像shell一样简洁，易学易用、可拓展性强，同时兼顾C的强大功能。于是Guido在1989年的圣诞节期间，开始编写能够解释Python语言语法的解释器。 Python崇尚优美、清晰、简单，是一个优秀并广泛使用的语言。最新的TIOBE排行榜https://www.tiobe.com/tiobe-index/，Python已飙升至世界第三。 Python可以应用于众多领域，如：人工智能、数据分析、爬虫、金融量化、云计算、WEB开发、自动化运维&#x2F;测试、游戏开发、网络服务、图像处理等众多领域。目前业内几乎所有大中型互联网企业都在使用Python，如：Youtube、Dropbox、BT、Quora（中国知乎）、豆瓣、知乎、Google、Yahoo!、Facebook、NASA、百度、腾讯、汽车之家、美团等。 3.1 Python解释器的发展史12345678910111213141516171819202122232425262728293031321989年，Guido开始写Python语言的编译器。 1991年，第一个Python编译器诞生。它是用C语言实现的，并能够调用C语言的库文件。从一出生，Python已经具有了：类，函数，异常处理，包含表和词典在内的核心数据类型，以及模块为基础的拓展系统。Granddaddy of Python web frameworks, Zope 1 was released in 1999Python 1.0 - January 1994 增加了 lambda, map, filter and reduce.Python 2.0 - October 16, 2000，加入了内存回收机制，构成了现在Python语言框架的基础Python 2.4 - November 30, 2004, 同年目前最流行的WEB框架Django 诞生Python 2.5 - September 19, 2006Python 2.6 - October 1, 2008Python 2.7 - July 3, 2010In November 2014, it was announced that Python 2.7 would be supported until 2020, and reaffirmed that there would be no 2.8 release as users were expected to move to Python 3.4+ as soon as possiblePython 3.0 - December 3, 2008 (细心的读者会发现，08年时就推出了3.0，2010年反而又推出了2.7？是因为3.0不向下兼容2.0，而很多公司已经基于2.0版本开发出了大量程序，公司已然投入了大量的人财物力，这就导致大家都拒绝升级3.0，无奈官方只能推出2.7过渡版本，之后我们都应该采用3.0解释器开发程序，但为了方便读者维护2.0版本的软件，我们在遇到两种版本的差异时会专门指出来)Python 3.1 - June 27, 2009Python 3.2 - February 20, 2011Python 3.3 - September 29, 2012Python 3.4 - March 16, 2014Python 3.5 - September 13, 2015Python 3.6 - 2016-12-23 发布python3.6.0版 3.2 Python解释器有哪些种类? 官方的Python解释器本质就是基于C语言开发的一个软件，该软件的功能就是读取以.py结尾的文件内容，然后按照Guido定义好的语法和规则去翻译并执行相应的代码。这种用C实现的解释器称为CPython，它是python领域性能最好，应用最广泛的一款解释器，我们在后面提到的解释器指的都是Cpython解释器。但其实解释器作为一款应用软件，完全可以采用其他语言来开发，只要能解释python这门语言的语法即可。Python解释器的一些种类如下，简单了解即可 123456789101112# JythonJPython解释器是用JAVA编写的python解释器，可以直接把Python代码编译成Java字节码并执行，它不但使基于java的项目之上嵌入python脚本成为可能，同时也可以将java程序引入到python程序之中。# IPythonIPython是基于CPython之上的一个交互式解释器，也就是说，IPython只是在交互方式上有所增强，但是执行Python代码的功能和CPython是完全一样的。这就好比很多国产浏览器虽然外观不同，但内核其实都是调用了IE。CPython用&gt;&gt;&gt;作为提示符，而IPython用In [序号]:作为提示符。# PyPyPyPy是Python开发者为了更好地Hack Python而用Python语言实现的Python解释器。PyPy提供了JIT编译器和沙盒功能，对Python代码进行动态编译（注意不是解释），因此运行速度比CPython还要快。# IronPythonIronPython和Jython类似，只不过IronPython是运行在微软.Net平台上的Python解释器，可以直接把Python代码编译成.Net的字节码。 四，第一个python程序4.1 运行python程序有两种方式方式一： 交互式模式 123456➜ Desktop python3Python 3.9.13 (v3.9.13:6de2ca5339, May 17 2022, 11:37:23)[Clang 13.0.0 (clang-1300.0.29.30)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; print(&#x27;hello world&#x27;)hello world 方式二：脚本文件 1234➜ ~ cat test.pyprint(&quot;hello world&quot;)➜ ~ python3 test.pyhello world 总结: #1、交互式模式下可以即时得到代码执行结果，调试程序十分方便 #2、若想将代码永久保存下来，则必须将代码写入文件中 #3、我们以后主要就是在代码写入文件中，偶尔需要打开交互式模式调试某段代码、验证结果 4.2 注释4.2.1 什么是注释注释就是就是对代码的解释说明，注释的内容不会被当作代码运行 4.2.2 为什么要注释增强代码的可读性 4.2.3 怎么用注释?1234567891011121314代码注释分单行和多行注释1、单行注释用#号，可以跟在代码的正上方或者正后方2、多行注释可以用三对双引号&quot;&quot;&quot; &quot;&quot;&quot;# 单行注释&quot;&quot;&quot;多行注释&quot;&quot;&quot; 4.2.4 代码注释的原则 1、不用全部加注释，只需要为自己觉得重要或不好理解的部分加注释即可 2、注释可以用中文或英文，但不要用拼音","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"计算机核心基础","slug":"jisuanjihexinjichu","date":"2023-03-16T02:16:41.000Z","updated":"2023-03-16T02:16:41.000Z","comments":true,"path":"articles/2023/03/16/jisuanjihexinjichu/","permalink":"https://kkabuzs.github.io/articles/2023/03/16/jisuanjihexinjichu/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 1、计算机核心基础特别鸣谢： — b站老男孩agen老师 一，引子 接下来一段时间，我们的目标的是：学会使用python这门编程语言来编写ATM+购物车程序，那么问题来了: 1.1 什么是语言？什么是编程语言？为何要有编程语言？ 语言其实就是人与人之间沟通的介质，如英语，汉语，俄语等。 而编程语言则是人与计算机之间沟通的介质。 那么为何要有编程语言，或者说人为何要与计算机沟通呢？这是因为在编程的世界里，计算机就好比是人的奴隶，人与计算机沟通的目的就是为了奴役计算机，让计算机按照人类的思维逻辑自发地去工作从而把人力解放出来。 此处我们可以提炼出如下两个重要的概念 1.2 什么是编程？为什么要编程？ 编程就是人把自己想命令计算机干的事用编程语言翻译出来并写到文件里（这一系列的文件就是程序），那么为什么要编程？ 小人类编程的目的就是为了让计算机按照人类的思维逻辑(程序)自发地去工作从而把人力解放出来。 综上，我们接下来的学习应该分为两个层面 为了更好地控制人类的奴隶（即计算机），我们需要学习计算机是由什么组成的、它能做什么事、它是怎样工作的（详见计算机组成原理、操作系统概述） 我们需要学习编程语言，从而把原来需要人力来完成的业务（比如ATM和购物）交给计算机去做 二，计算机组成原理2.1 什么是计算机？ 俗称电脑，即通电的大脑，电脑二字蕴含了人类对计算机的终极期望，希望它能真的像人脑一样去工作，从而解放人力。 2.2 为什么要用计算机？ 世界是由聪明的懒人统治的，任何时期，总有一群聪明的懒人想要奴隶别人。在奴隶制社会，聪明的懒人奴役的是真正的人，而人是无法不吃、不喝、不睡觉一直工作的，但是计算机作为一台机器是可以做到的，所以把计算机当奴隶是上上之选。 2.3 计算机的五大组成部分 计算机有五大组成部分，既然计算机是人的奴隶，那么计算机设计核心肯定也是在模仿真正的人，所以我们完全可以把计算机的五大组件比喻成人类的各种器官 2.3.1 控制器控制器是计算机的指挥系统，用来控制计算机其他组件的运行，相当于人类的大脑 2.3.2 运算器运算器是计算机的运算功能，用来做算术运算和逻辑运算，相当于人脑。 ps：控制器+运算器=CPU，cpu相当于人的大脑 2.3.3 存储器存储器是计算机的记忆功能，用来存取数据。 存储器主要分为内存与外存： - 内存相当于人的短期记忆。断电数据丢失 - 外存(如磁盘),相当于记事的本子，断电数据不会丢失，是用来永久保存数据的 ps：内存的存取速度要远远高于外存 2.3.4 输入设备input输入设备是计算接收外界输入数据的工具，如键盘、鼠标，相当于人的眼睛或耳朵。 2.3.5 输出设备output输出设备是计算机向外输出数据的工具，如显示器、打印机，相当于人说的话，写出的文章。 ps：存储器如内存、磁盘等既是输入设备又是输出设备，统称为IO设备 一个非常重要的基础知识:与运行程序相关的三大核心硬件 我们编写的程序一定是要运行于计算机硬件之上，而站在硬件的角度，与运行程序有关的三大核心硬件为CPU、内存、硬盘。 程序最先是存放于硬盘中的，程序的运行是先从硬盘把代码加载到内存中，然后cpu是从内存中读取指令运行。 三，操作系统概述3.1 操作系统的由来 大前提：我们编程目的就是为了奴役计算机，让计算机硬件自发地运行起来，然而硬件毕竟是”死的“，硬件的运行都是由软件支配。 倘若我们要开发一个应用程序，比如暴风音影，该软件的一个核心业务就是播放视频，开发者若要编写程序完成播放视频这个业务逻辑，必先涉及到底层硬件硬盘的基本运作（视频文件都是先存放于硬盘中），这意味着开发者在编写业务逻辑代码之前，必须先编写一个控制硬盘基本运行的控制程序，然而这仅仅只是一个开始，事实上，在编写应用程序的业务逻辑前，需要开发者编写出一套完整的控制程序用来控制所有硬件的基本运行（这要求开发者需要详细了解计算机硬件的各种控制细节，例如我们必须把CPU里面所有指令集都掌握一遍），如此，所有的开发者在开发程序时都必须依次开发两种： 编写一套完整的的控制程序，用来控制硬件的基本运行，以及把复杂的硬件的操作封装成简单的接口 基于控制程序的接口开发包含一系列业务逻辑的程序，为了与控制程序区分，可以称为应用程序，以ATM这款应用程序为例，业务逻辑有提款、转账、查询余额等 综上，对于不同公司的开发者来说，应用程序的业务逻辑各不相同，但硬件的控制程序都大致相同，为了避免所有程序员做重复劳动，以及不用再耗费精力去了解所有硬件的运行细节，有公司专门跳出来承担起控制程序的开发任务，这里所说的控制程序指的就是操作系统。 操作系统的功能就是帮我们把复杂的硬件的控制封装成简单的接口，对于开发应用程序来说只需要调用操作系统提供给我们的接口即可 3.2 系统软件与应用软件 硬件以上运行的都是软件，而软件分为两类： 1.应用软件（例如qq、word、暴风影音，我们学习python就是为了开发应用软件的） 2.操作系统，操作系统应用软件与硬件之间的一个桥梁，是协调、管理、控制计算机硬件与应用软件资源的控制程序。 3.3 计算机系统三层结构 综上，我们开发应用程序本质是在控制硬件，但是我们直接打交道的是操作系统，应用程序都是通过操作系统来间接地操作硬件的，所以一套完整的计算机系统分为三层，如下 一个非常重要的基础概念：平台 应用程序都是运行于操作系统之上，而操作系统则是运行于硬件之上的，所以承载应用程序的是一台运行有操作系统的计算机，称之为应用程序的运行平台，即：硬件 + 操作系统 == 平台 常见的平台有：windows系统+某款硬件、linux系统+某款硬件、ubuntu+某款硬件等，我们在开发应用程序时就需要考虑到应用程序的跨平台性，如果能开发出一款可以在任意平台运行的应用程序，那对于开发者来说真是极大的福音。而决定应用软件的跨平台性的关键因素往往是编程语言的选择，python恰好是一款跨平台性语言，这也是我们学习它的原因之一。","categories":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"}]},{"title":"节点加入k8s集群如何获取token等参数值","slug":"jiedianjiaruk8sjiqunruhehuodetoken","date":"2020-09-24T03:37:51.000Z","updated":"2020-09-24T03:37:51.000Z","comments":true,"path":"articles/2020/09/24/jiedianjiaruk8sjiqunruhehuodetoken/","permalink":"https://kkabuzs.github.io/articles/2020/09/24/jiedianjiaruk8sjiqunruhehuodetoken/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 节点加入k8s集群如何获取token等参数值在 kubeadm 初始话集群成功后会返回join 命令，里面有 token，discovery-token-ca-cert-hash等参数，需要记下来。 有关 token 的过期时间是24小时certificate-key 过期时间是2小时 如果是不记得，请执行以下命令获取 12341. 在master节点执行kubeadm token list获取token（注意查看是否过期）2. 如果没有--discovery-token-ca-cert-hash值，也可以通过以下命令获取openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#x27;s/^.* //&#x27; 如果是过期了，需要重新生成 123456781. 执行kubeadm token create --print-join-command，重新生成，重新生成基础的 join 命令（对于添加 master 节点还需要重新生成certificate-key，见下一步）# 如果是添加 worker 节点，不需要执行这一步，直接使用上面返回的 join 命令加入集群。2. 使用 kubeadm init phase upload-certs --experimental-upload-certs 重新生成certificate-key# 添加 master 节点：用上面第1步生成的 join 命令和第2步生成的--certificate-key 值拼接起来执行参考https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/#token-based-discovery-with-ca-pinninghttps://kubernetes.io/docs/setup/independent/high-availability/#steps-for-the-first-control-plane-node (其中的note)","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Kuberbetes","slug":"Kuberbetes","permalink":"https://kkabuzs.github.io/tags/Kuberbetes/"}]},{"title":"Docker 之离线安装","slug":"dockerzhilixiananzhuang","date":"2020-04-09T03:34:52.000Z","updated":"2020-04-09T03:34:52.000Z","comments":true,"path":"articles/2020/04/09/dockerzhilixiananzhuang/","permalink":"https://kkabuzs.github.io/articles/2020/04/09/dockerzhilixiananzhuang/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Docker 之离线安装离线安装docker-ce1234567891011121314151617181920# 根据机器架构进行选择，其中centos64是 x86_64# 这里以18.09.4版本为例# 解压tar xf docker-18.09.4.tar -C /usr/local# 软链ln -s /usr/local/docker/* /usr/bin# service和socket文件# 在 https://github.com/moby/moby/tree/master/contrib/init/systemd vim /usr/lib/systemd/system/docker.servicevim /usr/lib/systemd/system/docker.socket# 加docker组groupadd docker# 依次起服务systemctl start dockersystemctl enable docker 安装docker-compose12345678## 在有网的机器上下载docker-compose可执行文件sudo curl -L https://github.com/docker/compose/releases/download/1.17.1/docker-compose-`uname -s`-`uname -m`## copy该文件到断网机器cp to /usr/bin/docker-compose## 给可执行权限chmod +x /usr/bin/docker-compose 迁移一个镜像到其他机器123docker save mysql:5.7 &gt; mysql\\:5.7docker load &lt; mysql\\:5.7 迁移一个容器到其他机器123docker export 3ef747139cb8 &gt; test_project-v1.1.tarcat test_project-v1.1.tar | docker import - test_project:v1.1","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://kkabuzs.github.io/tags/Docker/"}]},{"title":"Nginx CORS跨域","slug":"nginxcorskuayu","date":"2019-10-15T07:31:48.000Z","updated":"2019-10-15T07:31:48.000Z","comments":true,"path":"articles/2019/10/15/nginxcorskuayu/","permalink":"https://kkabuzs.github.io/articles/2019/10/15/nginxcorskuayu/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Nginx CORS跨域L 地址返回的状态码是 400、403、404、500 的时候，跨域的资源是不会跟随返回的，也就是说，即便是 Nginx 上配置了 add_header 关键字，也不会随着内容返回而返回。 举个例子说: 12add_header Access-Control-Allow-Origin *;add_header Access-Control-Allow-Methods *; 当我们在请求对应地址的时候，理应是会返回已经配置好的头部信息，但是我们来看看最终的结果。 200 1234567891011HTTP/1.1 200 OKServer: openresty/1.11.2.2Date: Fri, 26 Jan 2018 08:46:39 GMTContent-Type: text/html; charset=UTF-8Content-Length: 558Last-Modified: Tue, 28 Mar 2017 01:13:24 GMTConnection: keep-aliveETag: &quot;58d9b8b4-22e&quot;Access-Control-Allow-Origin: *Access-Control-Allow-Methods: *Accept-Ranges: bytes 内容无误。 404 123456HTTP/1.1 404 Not FoundServer: openresty/1.11.2.2Date: Fri, 26 Jan 2018 08:47:18 GMTContent-Type: text/html; charset=UTF-8Content-Length: 175Connection: keep-alive 神奇了，这里404状态码下面居然自定义的响应头消失了。 三，原因与解决方式 留意 Nginx 文档上说的:Adds the specified field to a response header provided that the response code equals 200, 201 (1.3.10), 204, 206, 301, 302, 303, 304, 307 (1.1.16, 1.0.13), or 308 (1.13.0). The value can contain variables. 意思就是说，add_header 只会追加到以上响应状态码的响应头上面。 因为咱们的 API 有各种的状态码返回，那么其他状态码下，该怎么办? 大家留意文档上有一个参数。 1234Syntax: add_trailer name value [**always**];Default: —Context: http, server, location, if in locationThis directive appeared in version 1.13.2. 你会发现有个 [always] 参数，那么这个参数，就是让你的配置头，应用在所有的影响上面去。 将参数添加进去: 12add_header Access-Control-Allow-Origin * always;add_header Access-Control-Allow-Methods * always; 重启 nginx 服务器后重试一下. 200 1234567891011HTTP/1.1 200 OKServer: openresty/1.11.2.2Date: Fri, 26 Jan 2018 09:01:36 GMTContent-Type: text/html; charset=UTF-8Content-Length: 558Last-Modified: Tue, 28 Mar 2017 01:13:24 GMTConnection: keep-aliveETag: &quot;58d9b8b4-22e&quot;Access-Control-Allow-Origin: *Access-Control-Allow-Methods: *Accept-Ranges: bytes 200请求没变化，一切正常。 404 12345678HTTP/1.1 404 Not FoundServer: openresty/1.11.2.2Date: Fri, 26 Jan 2018 09:02:12 GMTContent-Type: text/html; charset=UTF-8Content-Length: 175Connection: keep-aliveAccess-Control-Allow-Origin: *Access-Control-Allow-Methods: * 现在 404 也正确了。我们的跨域也正是配置完成。 四，关于 OPTIONS 请求当我们前端发起跨域请求的时候，会事先发起一次 OPTIONS 请求，以用来查询该接口是否支持跨域和对应的请求方法。 在配置方面可以这么做。 123456if ($request_method = OPTIONS) &#123; add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Methods *; add_header Access-Control-Allow-Credentials true; return 204; &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@qiepian conf.d]# cat qieqian-demo.conf server &#123; listen 443 ssl; server_name xxx.xxx.cn; access_log /var/log/nginx/log/qiepian.log main; ssl_certificate /etc/nginx/ssl/xxx.crt; ssl_certificate_key /etc/nginx/ssl/xxx.key; location / &#123; if ($request_method = &#x27;OPTIONS&#x27;) &#123; add_header &#x27;Access-Control-Allow-Origin&#x27; &#x27;*&#x27; &#x27;always&#x27;; add_header &#x27;Access-Control-Allow-Credentials&#x27; &#x27;true&#x27;; add_header &#x27;Access-Control-Allow-Methods&#x27; &#x27;GET,POST,OPTIONS&#x27;; add_header &#x27;Access-Control-Allow-Headers&#x27; &#x27;Authorization,Content-Type,Accept,Origin,User-Agent,DNT,Cache-Control,X-Mx-ReqToken,X-Requested-With,token&#x27;; return 200; &#125; if ($request_method = &#x27;POST&#x27;) &#123; add_header &#x27;Access-Control-Allow-Origin&#x27; &#x27;*&#x27; &#x27;always&#x27;; add_header &#x27;Access-Control-Allow-Credentials&#x27; &#x27;true&#x27;; add_header &#x27;Access-Control-Allow-Methods&#x27; &#x27;GET,POST,OPTIONS&#x27;; add_header &#x27;Access-Control-Allow-Headers&#x27; &#x27;Authorization,Content-Type,Accept,Origin,User-Agent,DNT,Cache-Control,X-Mx-ReqToken,X-Requested-With,token&#x27;; &#125; if ($request_method = &#x27;GET&#x27;) &#123; add_header &#x27;Access-Control-Allow-Origin&#x27; &#x27;*&#x27; &#x27;always&#x27;; add_header &#x27;Access-Control-Allow-Credentials&#x27; &#x27;true&#x27;; add_header &#x27;Access-Control-Allow-Methods&#x27; &#x27;GET,POST,OPTIONS&#x27;; add_header &#x27;Access-Control-Allow-Headers&#x27; &#x27;Authorization,Content-Type,Accept,Origin,User-Agent,DNT,Cache-Control,X-Mx-ReqToken,X-Requested-With,token&#x27;; &#125; root /mountdisk/projects_tile; autoindex on; &#125;&#125;server &#123; listen 80; server_name xxx.xxx.cn; rewrite ^(.*)$ https://$host$1 permanent; &#125;","categories":[{"name":"工作随笔，问题排查","slug":"工作随笔，问题排查","permalink":"https://kkabuzs.github.io/categories/%E5%B7%A5%E4%BD%9C%E9%9A%8F%E7%AC%94%EF%BC%8C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://kkabuzs.github.io/tags/Nginx/"}]},{"title":"CentOS虚拟机断电或强制关机，再开机出现问题：Entering emergency mode. Exit the shell to continue.","slug":"centosxunijiqiangzhiduandiankaijibaocuo","date":"2019-09-17T06:14:50.000Z","updated":"2019-09-17T06:14:50.000Z","comments":true,"path":"articles/2019/09/17/centosxunijiqiangzhiduandiankaijibaocuo/","permalink":"https://kkabuzs.github.io/articles/2019/09/17/centosxunijiqiangzhiduandiankaijibaocuo/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 CentOS虚拟机断电或强制关机，再开机出现问题：Entering emergency mode. Exit the shell to continue. 找出问题出现在哪： 这里的 journalctl 是查看系统的日志信息；直接输入此命令查看，日志内容可能很多，快速翻页或者直接定位到最新的日志信息，发现有标红的，说明此处出现错误。 错误原因： 123failed to mount /sysroot. Dependency failed for Initrd root File System. Dependency failed for Reload configuration from the Real Root. 解决问题： 1输入命令：xfs_repair -v -L /dev/dm-0 -L 选项指定强制日志清零，强制xfs_repair将日志归零，即使它包含脏数据（元数据更改）。","categories":[{"name":"工作随笔，问题排查","slug":"工作随笔，问题排查","permalink":"https://kkabuzs.github.io/categories/%E5%B7%A5%E4%BD%9C%E9%9A%8F%E7%AC%94%EF%BC%8C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://kkabuzs.github.io/tags/Linux/"}]},{"title":"Hadoop集群之 CDH v5.13.1 搭建 (离线本地Parcels）","slug":"hadoopjiqunzhi-cdh5-13-1lixiandajian","date":"2019-08-30T02:09:25.000Z","updated":"2019-08-30T02:09:25.000Z","comments":true,"path":"articles/2019/08/30/hadoopjiqunzhi-cdh5-13-1lixiandajian/","permalink":"https://kkabuzs.github.io/articles/2019/08/30/hadoopjiqunzhi-cdh5-13-1lixiandajian/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Hadoop集群之 CDH v5.13.1 搭建 (离线本地Parcels）一，CDH Parcels 离线安装 官方一共给了三种方法： Installation Path A - Automated Installation by Cloudera Manager：要求所有机器都能连网，而且外国网站不太稳定。一旦失败，重装非常痛苦。 Installation Path B - Manual Installation Using Cloudera Manager Packages：设置Red Hat&#x2F;CentOS或者Debian&#x2F;Ubuntu，下载系统package安装，下载量数目众多 Installation Path C - Manual Installation Using Tarballs and Parcels安装步骤：该方法对系统侵入性最小,最大优点可实现全离线安装，而且重装什么的都非常方便。后期的集群统一包升级也非常好 在这里，优雅的我们必须使用优雅的方法，显然是第三种 1.1 本地源与Package本地源的区别 本地通过Parcel安装过程与本地通过Package安装过程完全一致，不同的是两者的本地源的配置。区别如下： Package本地源：软件包是.rpm格式的，数量通常较多，下载的时候比较麻烦。通过createrepo .的命令创建源，并要放到存放源文件主机的web服务器的根目录下。 Parcel本地源：软件包是以.parcel结尾，相当于压缩包格式的，一个系统版本对应一个，下载的时候方便。 二，使用 Parcels 离线部署 CDH2.1 环境介绍 主机名 ip 描述 hadoop1 192.168.101.116 主 hadoop2 192.168.101.49 从 hadoop3 192.168.101.229 从 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#所有节点关闭防火墙和selinux[root@hadoop1 ~]# systemctl stop firewalld[root@hadoop1 ~]# systemctl disable firewalldRemoved symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.[root@hadoop1 ~]# vim /etc/selinux/config将SELINUX=enforcing改为SELINUX=disabled[root@hadoop1 ~]# setenforce 0#配置hosts映射[root@hadoop1 ~]# vim /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.101.116 hadoop1192.168.101.49 hadoop2192.168.101.229 hadoop3#scp到两台从[root@hadoop1 ~]# scp /etc/hosts hadoop2:/etc/root@hadoop2&#x27;s password:hosts 100% 231 658.0KB/s 00:00[root@hadoop1 ~]# scp /etc/hosts hadoop3:/etc/root@hadoop3&#x27;s password:hosts 100% 231 440.1KB/s 00:00#安装jdk（所有机器）[root@hadoop1 ~]# wget https://github.com/frekele/oracle-java/releases/download/8u181-b13/jdk-8u181-linux-x64.tar.gz[root@hadoop1 ~]# tar xf jdk-8u181-linux-x64.tar.gz -C /usr/local/[root@hadoop1 ~]# ln -s /usr/local/jdk1.8.0_181 /usr/local/jdk#配置java环境变量[root@hadoop1 ~]# sed -i.ori &#x27;$a export JAVA_HOME=/usr/local/jdk\\nexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH\\nexport CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar&#x27; /etc/profile[root@hadoop1 ~]# tail -3 /etc/profileexport JAVA_HOME=/usr/local/jdkexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATHexport CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar[root@hadoop1 ~]# source /etc/profile[root@hadoop1 ~]# which java/usr/local/jdk/bin/java[root@hadoop1 ~]# java -versionjava version &quot;1.8.0_181&quot;Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode) 2.2 部署Cloudera Manager1234567891011121314# 下载二进制安装包[root@hadoop1 ~]# wget https://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.13.3_x86_64.tar.gz[root@hadoop1 ~]# lsanaconda-ks.cfg cloudera-manager-centos7-cm5.13.3_x86_64.tar.gz#先把安装包copy到其他两台机器[root@hadoop1 ~]# scp cloudera-manager-centos7-cm5.13.3_x86_64.tar.gz hadoop2:~[root@hadoop1 ~]# scp cloudera-manager-centos7-cm5.13.3_x86_64.tar.gz hadoop3:~#创建解压目录(三台都创建)[root@hadoop1 ~]# mkdir /usr/local/cloudera-manager#解压cloudera-manager-centos7-cm5.13.3_x86_64.tar.gz（三台机器）[root@hadoop1 ~]# tar xf cloudera-manager-centos7-cm5.13.3_x86_64.tar.gz -C /usr/local/cloudera-manager 2.2.1 在所有的节点上创建clodera-scm用户123456789[root@hadoop1 ~]# useradd --system --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm[root@hadoop1 ~]# id cloudera-scmuid=998(cloudera-scm) gid=996(cloudera-scm) 组=996(cloudera-scm)#参数解读：-r, --system 创建一个系统账户-M, --no-create-home 不创建用户的主目录-s, --shell SHELL 新账户的登录 shell-c, --comment COMMENT 新账户的 GECOS 字段 2.2.2 修改Cloudera Manager Agent端的配置文件(所有机器)1234[root@hadoop1 ~]# vim /usr/local/cloudera-manager/cm-5.13.3/etc/cloudera-scm-agent/config.iniserver_host=localhost改为：server_host=hadoop1 2.2.3 初始化CM Server数据库123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105#首先要有一个mysql数据库[root@hadoop1 ~]# rpm -qa | grep mysql #没有安装[root@hadoop1 ~]#[root@hadoop1 ~]# rpm -Uvh http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm#查看mysql可用安装[root@hadoop1 ~]# yum repolist enabled | grep &quot;mysql.*-community.*&quot;mysql-connectors-community/x86_64 MySQL Connectors Community 118mysql-tools-community/x86_64 MySQL Tools Community 95mysql56-community/x86_64 MySQL 5.6 Community Server 479#安装mysql 5.6[root@hadoop1 ~]# yum -y install mysql-community-server#加入开机启动，启动服务systemctl enable mysqldsystemctl start mysqld#配置mysql（设置root密码等）：[root@zhaohadoop3 ~]# mysql_secure_installationNOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MySQL SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY!In order to log into MySQL to secure it, we&#x27;ll need the currentpassword for the root user. If you&#x27;ve just installed MySQL, andyou haven&#x27;t set the root password yet, the password will be blank,so you should just press enter here.Enter current password for root (enter for none): #回车OK, successfully used password, moving on...Setting the root password ensures that nobody can log into the MySQLroot user without the proper authorisation.Set root password? [Y/n] y [设置root用户密码]New password: 123456Re-enter new password: 123456Password updated successfully!Reloading privilege tables.. ... Success!By default, a MySQL installation has an anonymous user, allowing anyoneto log into MySQL without having to have a user account created forthem. This is intended only for testing, and to make the installationgo a bit smoother. You should remove them before moving into aproduction environment.Remove anonymous users? [Y/n] y [删除匿名用户] ... Success!Normally, root should only be allowed to connect from &#x27;localhost&#x27;. Thisensures that someone cannot guess at the root password from the network.Disallow root login remotely? [Y/n] n [禁止root远程登录] ... skipping.By default, MySQL comes with a database named &#x27;test&#x27; that anyone canaccess. This is also intended only for testing, and should be removedbefore moving into a production environment.Remove test database and access to it? [Y/n] y [删除test数据库] - Dropping test database...ERROR 1008 (HY000) at line 1: Can&#x27;t drop database &#x27;test&#x27;; database doesn&#x27;t exist ... Failed! Not critical, keep moving... - Removing privileges on test database... ... Success!Reloading the privilege tables will ensure that all changes made so farwill take effect immediately.Reload privilege tables now? [Y/n] y [刷新权限] ... Success!All done! If you&#x27;ve completed all of the above steps, your MySQLinstallation should now be secure.Thanks for using MySQL!Cleaning up...#登陆数据库，创建初始化CM Server用户mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &#x27;cdh&#x27;@&#x27;192.168.101.%&#x27; IDENTIFIED BY &#x27;111111&#x27; WITH GRANT OPTION;Query OK, 0 rows affected (0.00 sec)[root@hadoop1 schema]# yum -y install mysql-connector-java#######第二种方法：把jar包拷贝到/usr/share/java下[root@hadoop1 ~]# mkdir /usr/share/java[root@hadoop1 ~]# mv mysql-connector-java.jar /usr/share/java########初始化CM Server数据库[root@hadoop1 ~]# cd /usr/local/cloudera-manager/cm-5.13.3/share/cmf/schema/[root@hadoop1 schema]# ./scm_prepare_database.sh mysql cdh -h hadoop1 -ucdh -p111111 --scm-host hadoop1 scm scm scm #其格式为：数据库类型，数据库，数据库服务器，用户名，密码，CM服务器，后面的三个scm请不要改动！JAVA_HOME=/usr/local/jdkVerifying that we can write to /usr/local/cloudera-manager/cm-5.13.3/etc/cloudera-scm-serverCreating SCM configuration file in /usr/local/cloudera-manager/cm-5.13.3/etc/cloudera-scm-serverExecuting: /usr/local/jdk/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/local/cloudera-manager/cm-5.13.3/share/cmf/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /usr/local/cloudera-manager/cm-5.13.3/etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.2019-09-02 11:23:13,182 [main] INFO com.cloudera.enterprise.dbutil.DbCommandExecutor - Successfully connected to database. #成功All done, your SCM database is configured correctly! 检查数据库内容 1234567891011121314151617181920212223242526272829303132[root@hadoop1 ~]# mysql -uroot -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 17Server version: 5.6.45 MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || cdh || mysql || performance_schema |+--------------------+4 rows in set (0.00 sec)mysql&gt; use cdh;Database changedmysql&gt; show tables;Empty set (0.00 sec)mysql&gt; quitBye 2.3 制作CDH本地源2.3.1 server创建parcel-repo目录1234[root@hadoop1 ~]# mkdir -p /opt/cloudera/parcel-repo #Server端创建Parcel-repo目录，用于存放离线CDH文件[root@hadoop1 ~]# chown cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo/ #别忘记把权限给赋给我们之前创建的cloudera-scm用户[root@hadoop1 ~]# ll -d /opt/cloudera/parcel-repo/drwxr-xr-x. 2 cloudera-scm cloudera-scm 6 9月 2 11:34 /opt/cloudera/parcel-repo/ 2.3.2 agent端创建parcels目录(49和229机器)1234[root@hadoop2 ~]# mkdir -p /opt/cloudera/parcels[root@hadoop2 ~]# chown cloudera-scm:cloudera-scm /opt/cloudera/parcels[root@hadoop2 ~]# ls -ld /opt/cloudera/parcelsdrwxr-xr-x. 2 cloudera-scm cloudera-scm 6 9月 2 13:39 /opt/cloudera/parcels 2.3.3 制作CDH本地源 paecel官方下载网址：http://archive.cloudera.com/cdh5/parcels/ 123456789101112131415[root@hadoop1 ~]# wget http://archive.cloudera.com/cdh5/parcels/5.13.3/CDH-5.13.3-1.cdh5.13.3.p0.2-el7.parcel[root@hadoop1 ~]# wget http://archive.cloudera.com/cdh5/parcels/5.13.3/CDH-5.13.3-1.cdh5.13.3.p0.2-el7.parcel.sha1[root@hadoop1 ~]# wget http://archive.cloudera.com/cdh5/parcels/5.13.3/manifest.json[root@hadoop1 ~]# mv CDH-5.13.3-1.cdh5.13.3.p0.2-el7.parcel /opt/cloudera/parcel-repo[root@hadoop1 ~]# mv CDH-5.13.3-1.cdh5.13.3.p0.2-el7.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.13.3-1.cdh5.13.3.p0.2-el7.parcel.sha #注意 这里我改名了[root@hadoop1 ~]# mv manifest.json /opt/cloudera/parcel-repo/[root@hadoop1 ~]# ll /opt/cloudera/parcel-repo/总用量 1889856-rw-r--r--. 1 root root 1935128068 4月 5 2018 CDH-5.13.3-1.cdh5.13.3.p0.2-el7.parcel-rw-r--r--. 1 root root 41 4月 5 2018 CDH-5.13.3-1.cdh5.13.3.p0.2-el7.parcel.sha-rw-r--r--. 1 root root 73766 4月 5 2018 manifest.json温馨提示： 如果你没有下载到“CDH-5.13.3-1.cdh5.13.3.p0.2-el7.parcel.sha”文件是，可以找到“manifest.json”文件中&quot;parcelName&quot;: &quot;CDH-5.13.3-1.cdh5.13.3.p0.2-el7.parcel.sha&quot;对应的&quot;hash&quot;: &quot;1dce02d58ab5c336d861ed2e62f5745e2fca5afe&quot;复制到该文件即可。这个方法也适用于其他的版本！ 2.4 启动CM的Server，Agent端2.4.1 启动Cloudera Manager Server端1234[root@hadoop1 ~]# /usr/local/cloudera-manager/cm-5.13.3/etc/init.d/cloudera-scm-server startStarting cloudera-scm-server: [ 确定 ][root@hadoop1 init.d]# /usr/local/cloudera-manager/cm-5.13.3/etc/init.d/cloudera-scm-server statuscloudera-scm-server (pid 24714) 正在运行... 我们需要观察Server的日志文件，如果出现以下内容说明启动成功 2.4.2 启动Cloudera Manager Agent端123456789101112131415[root@hadoop1 ~]# /usr/local/cloudera-manager/cm-5.13.3/etc/init.d/cloudera-scm-agent startStarting cloudera-scm-agent: [ 确定 ][root@hadoop1 ~]# /usr/local/cloudera-manager/cm-5.13.3/etc/init.d/cloudera-scm-agent statuscloudera-scm-agent (pid 24942) 正在运行...[root@hadoop2 ~]# /usr/local/cloudera-manager/cm-5.13.3/etc/init.d/cloudera-scm-agent startStarting cloudera-scm-agent: [ 确定 ][root@hadoop2 ~]# /usr/local/cloudera-manager/cm-5.13.3/etc/init.d/cloudera-scm-agent statuscloudera-scm-agent (pid 23528) 正在运行...[root@hadoop3 ~]# /usr/local/cloudera-manager/cm-5.13.3/etc/init.d/cloudera-scm-agent startStarting cloudera-scm-agent: [ 确定 ][root@hadoop3 ~]# /usr/local/cloudera-manager/cm-5.13.3/etc/init.d/cloudera-scm-agent statuscloudera-scm-agent (pid 23957) 正在运行... 2.5 访问webUI界面 我们安装CM的过程通过WebUI的安装向导来进行安装，推荐使用谷歌浏览器，不要使用容易崩溃的浏览器，这样会影响你安装进度的！如果你也出现了以下界面，恭喜你CM部署成功。 三，基于CM的WebUI部署CDH集群3.1 CM的webUI界面默认的用户名&#x2F;密码都是小写的admin 3.2 同意并继续 3.3 选择CM的免费版本 3.4 点击继续 3.5 为 CDH 群集安装指定主机 3.6 选择CDH本地版本（即我们自己下载好的版本） CDH的默认存放路径介绍 3.7 当CDH安装完毕后，点击继续 3.8 等待检查主机完成 3.9 我们可以根据下面的提示去相应的服务器做具体的操作，做完操作后点击“重新运行”以验证你是否配置成功！ 3.10 解决告警信息后，点击完成123456789101112#解决Cloudera 建议将 /proc/sys/vm/swappiness 设置为最大值 10问题[root@hadoop1 ~]# echo &#x27;vm.swappiness=10&#x27; &gt;&gt; /etc/sysctl.conf[root@hadoop1 ~]# sysctl -pvm.swappiness = 10[root@hadoop2 ~]# echo &#x27;vm.swappiness=10&#x27; &gt;&gt; /etc/sysctl.conf[root@hadoop2 ~]# sysctl -pvm.swappiness = 10[root@hadoop3 ~]# echo &#x27;vm.swappiness=10&#x27; &gt;&gt; /etc/sysctl.conf[root@hadoop4 ~]# sysctl -pvm.swappiness = 10 123456789101112131415161718192021222324#解决已启用透明大页面压缩，可能会导致重大性能问题。#hadoop1机器：[root@hadoop1 ~]# echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag[root@hadoop1 ~]# echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled[root@hadoop1 ~]# vim /etc/rc.local追加如下内容：echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled#hadoop2机器：[root@hadoop2 ~]# echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag[root@hadoop2 ~]# echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled[root@hadoop2 ~]# vim /etc/rc.local追加如下内容：echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled#hadoop3机器：[root@hadoop3 ~]# echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag[root@hadoop3 ~]# echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled[root@hadoop3 ~]# vim /etc/rc.local追加如下内容：echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled 点击页面上的重新运行： 3.11 自定义需要安装的服务 根据公司需求，选择安装相应的组件 3.12 分配角色 3.13 集群设置，数据库设置123456789101112131415161718192021[root@hadoop1 ~]# mysql -ucdh -h192.168.101.116 -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 392Server version: 5.6.45 MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt; create database hive; #创建hive库Query OK, 1 row affected (0.00 sec)mysql&gt; create database ooize; #创建ooize库Query OK, 1 row affected (0.00 sec)mysql&gt; 3.14 等待安装完成 解决办法（所有agent节点都操作）： 12[root@hadoop01 init.d]# mkdir /usr/java[root@hadoop01 init.d]# ln -s /usr/local/jdk1.8.0_181 /usr/java/default 3.15 完成安装后，点击继续 3.16 CDH部署成功界面 四，验证CHD服务是否可用4.1 验证zookeeper是否可以正常使用123456789101112131415161718192021222324252627282930313233[root@hadoop1 ~]# zookeeper-client -server hadoop1Connecting to hadoop12019-09-02 17:52:27,419 [myid:] - INFO [main:Environment@100] - Client environment:zookeeper.version=3.4.5-cdh5.13.3--1, built on 03/17/2018 11:30 GMT2019-09-02 17:52:27,422 [myid:] - INFO [main:Environment@100] - Client environment:host.name=hadoop12019-09-02 17:52:27,422 [myid:] - INFO [main:Environment@100] - Client environment:java.version=1.8.0_1812019-09-02 17:52:27,426 [myid:] - INFO [main:Environment@100] - Client environment:java.vendor=Oracle Corporation2019-09-02 17:52:27,426 [myid:] - INFO [main:Environment@100] - Client environment:java.home=/usr/local/jdk1.8.0_181/jre2019-09-02 17:52:27,426 [myid:] - INFO [main:Environment@100] - Client environment:java.class.path=/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/lib/zookeeper/bin/../build/classes:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/lib/zookeeper/bin/../build/lib/*.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/lib/zookeeper/bin/../lib/slf4j-log4j12.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/lib/zookeeper/bin/../lib/slf4j-log4j12-1.7.5.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/lib/zookeeper/bin/../lib/slf4j-api-1.7.5.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/lib/zookeeper/bin/../lib/netty-3.10.5.Final.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/lib/zookeeper/bin/../lib/log4j-1.2.16.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/lib/zookeeper/bin/../lib/jline-2.11.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/lib/zookeeper/bin/../zookeeper-3.4.5-cdh5.13.3.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/lib/zookeeper/bin/../src/java/lib/*.jar:/etc/zookeeper/conf:.:/usr/local/jdk/lib:/usr/local/jdk/lib/tools.jar:/etc/zookeeper/conf:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/bin/../lib/zookeeper/zookeeper.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/bin/../lib/zookeeper/zookeeper-3.4.5-cdh5.13.3.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/bin/../lib/zookeeper/lib/slf4j-log4j12.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/bin/../lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/bin/../lib/zookeeper/lib/slf4j-api-1.7.5.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/bin/../lib/zookeeper/lib/netty-3.10.5.Final.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/bin/../lib/zookeeper/lib/log4j-1.2.16.jar:/opt/cloudera/parcels/CDH-5.13.3-1.cdh5.13.3.p0.2/bin/../lib/zookeeper/lib/jline-2.11.jar2019-09-02 17:52:27,426 [myid:] - INFO [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib2019-09-02 17:52:27,426 [myid:] - INFO [main:Environment@100] - Client environment:java.io.tmpdir=/tmp2019-09-02 17:52:27,426 [myid:] - INFO [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;2019-09-02 17:52:27,426 [myid:] - INFO [main:Environment@100] - Client environment:os.name=Linux2019-09-02 17:52:27,426 [myid:] - INFO [main:Environment@100] - Client environment:os.arch=amd642019-09-02 17:52:27,426 [myid:] - INFO [main:Environment@100] - Client environment:os.version=3.10.0-862.el7.x86_642019-09-02 17:52:27,426 [myid:] - INFO [main:Environment@100] - Client environment:user.name=root2019-09-02 17:52:27,426 [myid:] - INFO [main:Environment@100] - Client environment:user.home=/root2019-09-02 17:52:27,427 [myid:] - INFO [main:Environment@100] - Client environment:user.dir=/root2019-09-02 17:52:27,428 [myid:] - INFO [main:ZooKeeper@438] - Initiating client connection, connectString=hadoop1 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@7aec35aWelcome to ZooKeeper!2019-09-02 17:52:27,455 [myid:] - INFO [main-SendThread(hadoop1:2181):ClientCnxn$SendThread@975] - Opening socket connection to server hadoop1/192.168.101.116:2181. Will not attempt to authenticate using SASL (unknown error)JLine support is enabled2019-09-02 17:52:27,542 [myid:] - INFO [main-SendThread(hadoop1:2181):ClientCnxn$SendThread@852] - Socket connection established, initiating session, client: /192.168.101.116:38806, server: hadoop1/192.168.101.116:21812019-09-02 17:52:27,553 [myid:] - INFO [main-SendThread(hadoop1:2181):ClientCnxn$SendThread@1235] - Session establishment complete on server hadoop1/192.168.101.116:2181, sessionid = 0x26cf151f2e30027, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: hadoop1(CONNECTED) 0] ls /[zookeeper, hbase][zk: hadoop1(CONNECTED) 1] quitQuitting...2019-09-02 17:52:47,902 [myid:] - INFO [main:ZooKeeper@684] - Session: 0x26cf151f2e30027 closed2019-09-02 17:52:47,902 [myid:] - INFO [main-EventThread:ClientCnxn$EventThread@512] - EventThread shut down 4.2 查看hdfs目录信息（注意权限，如果你想要往指定目录写数据的话，可以指定ACL）1234567891011121314151617181920212223root@hadoop1 ~]# hdfs dfs -ls -R /drwxr-xr-x - hbase hbase 0 2019-09-02 17:34 /hbasedrwxr-xr-x - hbase hbase 0 2019-09-02 17:34 /hbase/.tmpdrwxr-xr-x - hbase hbase 0 2019-09-02 17:34 /hbase/.tmp/datadrwxr-xr-x - hbase hbase 0 2019-09-02 17:34 /hbase/.tmp/data/hbasedrwxr-xr-x - hbase hbase 0 2019-09-02 17:34 /hbase/MasterProcWALs-rw-r--r-- 2 hbase hbase 0 2019-09-02 17:34 /hbase/MasterProcWALs/state-00000000000000000002.logdrwxr-xr-x - hbase hbase 0 2019-09-02 17:34 /hbase/WALsdrwxr-xr-x - hbase hbase 0 2019-09-02 17:41 /hbase/WALs/hadoop2,60020,1567416840373-rw-r--r-- 2 hbase hbase 83 2019-09-02 17:41 /hbase/WALs/hadoop2,60020,1567416840373/hadoop2%2C60020%2C1567416840373.null0.1567417311915drwxr-xr-x - hbase hbase 0 2019-09-02 17:41 /hbase/WALs/hadoop3,60020,1567416844608-rw-r--r-- 2 hbase hbase 83 2019-09-02 17:41 /hbase/WALs/hadoop3,60020,1567416844608/hadoop3%2C60020%2C1567416844608.meta.1567417309855.meta-rw-r--r-- 2 hbase hbase 83 2019-09-02 17:34 /hbase/WALs/hadoop3,60020,1567416844608/hadoop3%2C60020%2C1567416844608.null0.1567416849733drwxr-xr-x - hbase hbase 0 2019-09-02 17:34 /hbase/datadrwxr-xr-x - hbase hbase 0 2019-09-02 17:34 /hbase/data/defaultdrwxr-xr-x - hbase hbase 0 2019-09-02 17:34 /hbase/data/hbasedrwxr-xr-x - hbase hbase 0 2019-09-02 17:34 /hbase/data/hbase/metadrwxr-xr-x - hbase hbase 0 2019-09-02 17:34 /hbase/data/hbase/meta/.tabledesc-rw-r--r-- 2 hbase hbase 398 2019-09-02 17:34 /hbase/data/hbase/meta/.tabledesc/.tableinfo.0000000001drwxr-xr-x - hbase hbase 0 2019-09-02 17:34 /hbase/data/hbase/meta/.tmpdrwxr-xr-x - hbase hbase 0 2019-09-02 17:41 /hbase/data/hbase/meta/1588230740。。。省略若干。。。 4.3 查看hdfs的webUI界面 附录 123456789101112131415161718192021222324252627282930313233343536373839404142434445463、HDFS-副本不足的块，报错信息：测试 HDFS 是否具有过多副本不足块。不良 : 群集中有 8 个 副本不足的块 块。群集中共有 11 个块。百分比 副本不足的块: 72.73%。 临界阈值：40.00%。操作为此服务更改“副本不足的块监控阈值”。建议这是 HDFS 服务级运行状况测试，用于检查副本不足的块数是否未超过群集块总数的某一百分比。该运行状况测试失败可能表示 DataNode 丢失。使用 HDFS fsck 命令可确定哪些文件含有副本不足的块。可使用 副本不足的块监控阈值 HDFS 服务范围内的监控设置配置该测试。 原因：设置的副本备份数与DataNode的个数不同。dfs.replication的默认是3，也就是说副本数--块的备份数默认为3份。但是我这只有两个DataNode。所以导致了达不到目标，副本备份不足。 解决方案：设置目标备份数为2通过命令修改当前备份数点击集群-HDFS-配置，搜索dfs.replication,设置为2后保存更改。dfs.replication这个参数其实只在文件被写入dfs时起作用，虽然更改了配置文件，但是不会改变之前写入的文件的备份数。所以我们还需要步骤2，在cm0中通过命令更改备份数：这里的-R 2的数字2就对应我们的DataNode个数。su hdfshadoop fs -setrep -R 2 /","categories":[{"name":"大数据","slug":"大数据","permalink":"https://kkabuzs.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://kkabuzs.github.io/tags/Hadoop/"}]},{"title":"Hadoop集群之Spark","slug":"hadoopjiqunzhispark","date":"2019-08-26T09:53:33.000Z","updated":"2019-08-26T09:53:33.000Z","comments":true,"path":"articles/2019/08/26/hadoopjiqunzhispark/","permalink":"https://kkabuzs.github.io/articles/2019/08/26/hadoopjiqunzhispark/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Hadoop集群之Spark一，Spark概述1.1 Spark 基于内存的分布式的计算框架，是一个针对海量数据处理的非常快的通用的计算引擎(计算框架)。 1.2 特点 先进架构： 采用Scala语言编写，底层采用actormodel的akka作为通讯框架，代码十分简洁高效。基于DAG图执行引擎，减少多次计算中间结果写到HDFS的开销。建立在统一抽象的RDD之上，以基本一致的方式应对不同的大数据处理。 高效： 基于cache机制来支持需要反复迭代的计算或者多次数据共享，减少数据读取的IO开销。基于内存运算比MR要快100倍，基于硬盘的运算也比MR快10倍。 易用： 提供广泛的数据集操作类型（20+种），而MR只有两种。Spark支持Java，Python和ScalaAPI，支持交互的Python和Scala的shell 整体解决方案： Spark内存中批处理 SparkSQL交互式查询 SparkStreaming流式计算 Graphx和MLib提供的常用图计算和机器学习算法。 与Hadoop无缝链接： 可以使用Yarn作为集群管理框架，读取HDFS、Hbase等一切Hadoop数据。 1.3 核心组件 SparkCore: 核心部分 包含Spark基本功能（任务调度 内存管理 容错机制等） SparkSQL: Spark中交互式处理模块 SparkStreaming: Spark中流式数据处理的模块 SparkMLib：Spark机器学习相关模块 &#x3D;&gt; MahoutSparkGraphX: - Spark中图形计算的模块 SparkManagers：集群管理（HadoopYARN、ApacheMesos、Spark自带的单独调度器） 1.4 spark与mapreduce的比较 MapReduce: 分布式的计算框架 -&gt; Hive问题：shuffle：大文件的排序+读写磁盘+网络传输 &#x3D;&gt; 比较慢只有两种执行算子&#x2F;API: MapTask(数据转换+过滤)和ReduceTask(数据聚合) &#x3D;&#x3D;&gt; 定制化稍微有点差。不适合迭代式的计算。对于需要快速执行的产生结果的应用场景不适合。Spark：为了解决MapReduce执行慢、不适合迭代执行的问题。 Spark计算的核心思路就是将数据集缓存在内存中加快读取速度，Spark的中间结果放到内存中，一次创建数据集，可以多次迭代运算，减少IOK开销。适合运算比较多的ML和DL。 二，部署Spark2.1 环境准备 主机名 ip地址 用户名 运行组件 zhaohadoop1 192.168.101.150 hadoop DataNode、NodeManager、Hbase zhaohadoop2 192.168.101.154 hadoop DataNode、NodeManager、SecondaryNameNode、zookeeper、Hbase zhaohadoop3 192.168.101.156 hadoop NameNode、ResourceManager、zookeeper、Hbase、hive、spark zhaohadoop4 192.168.101.157 hadoop DataNode、NodeManager、zookeeper、Hbase 2.1 安装scala(所有节点) Spark官方要求 Scala 版本为 2.11.x，注意不要下错版本，我这里下了 2.11.12官方下载地址：http://www.scala-lang.org/download/github地址：https://github.com/scala/scala 123[root@zhaohadoop3 ~]# wget https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz[root@zhaohadoop3 ~]# tar xf scala-2.11.12.tgz -C /usr/local/[root@zhaohadoop3 ~]# mv /usr/local/scala-2.11.12 /usr/local/scala 2.2 配置环境变量123456789101112[root@zhaohadoop3 ~]# vim /etc/profile追加如下内容：#scalaexport SCALA_HOME=/usr/local/scalaexport PATH=$PATH:$SCALA_HOME/bin#是环境变量生效[root@zhaohadoop3 ~]# source /etc/profile[root@zhaohadoop3 ~]# scala -versionScala code runner version 2.11.12 -- Copyright 2002-2017, LAMP/EPFL 2.3 下载Spark源码包 官方网址：https://spark.apache.org/downloads.html 123[root@zhaohadoop3 ~]# wget http://mirror.bit.edu.cn/apache/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz[root@zhaohadoop3 ~]# tar xf spark-2.4.3-bin-hadoop2.7.tgz -C /usr/local/[root@zhaohadoop3 ~]# mv /usr/local/spark-2.4.3-bin-hadoop2.7 /usr/local/spark 2.4 配置Spark12345678910111213141516171819202122232425262728293031323334[root@zhaohadoop3 ~]# cd /usr/local/spark/conf/[root@zhaohadoop3 conf]# lsdocker.properties.template fairscheduler.xml.template log4j.properties.template metrics.properties.template slaves.template spark-defaults.conf.template spark-env.sh.template[root@zhaohadoop3 conf]# cp spark-env.sh.template spark-env.sh #从模版复制一份配置文件[root@zhaohadoop3 conf]# vim spark-env.sh末尾追加如下内容：export SCALA_HOME=/usr/local/scalaexport JAVA_HOME=/usr/local/jdkexport HADOOP_HOME=/usr/local/hadoopexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopSPARK_MASTER_IP=192.168.101.150SPARK_LOCAL_DIRS=/usr/local/sparkSPARK_DRIVER_MEMORY=512M变量说明JAVA_HOME：Java安装目录SCALA_HOME：Scala安装目录HADOOP_HOME：hadoop安装目录HADOOP_CONF_DIR：hadoop集群的配置文件的目录SPARK_MASTER_IP：spark集群的Master节点的ip地址SPARK_WORKER_MEMORY：每个worker节点能够最大分配给exectors的内存大小#注：在设置Worker进程的CPU个数和内存大小，要注意机器的实际硬件条件，如果配置的超过当前Worker节点的硬件条件，Worker进程会启动失败。#配置环境变量[root@zhaohadoop3 conf]# vim /etc/profile末尾追加如下内容：#sparkexport SPARK_HOME=/usr/local/sparkexport PATH=$PATH:$SPARK_HOME/bin[root@zhaohadoop3 conf]# source /etc/profile 2.5 在slaves文件下填上slave主机名12345[root@zhaohadoop3 conf]# cp slaves.template slaves[root@zhaohadoop3 conf]# vim slaveszhaohadoop1zhaohadoop2zhaohadoop4 2.6 spark目录复制到所有从节点123[root@zhaohadoop3 ~]# scp -r /usr/local/spark zhaohadoop1:/usr/local/[root@zhaohadoop3 ~]# scp -r /usr/local/spark zhaohadoop2:/usr/local/[root@zhaohadoop3 ~]# scp -r /usr/local/spark zhaohadoop1:/usr/local/ 2.7 启动Spark12345678910111213141516171819[root@zhaohadoop3 sbin]# pwd/usr/local/spark/sbin[root@zhaohadoop3 sbin]# ./start-all.shstarting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-zhaohadoop3.outzhaohadoop2: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-zhaohadoop2.outzhaohadoop1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-zhaohadoop1.outzhaohadoop4: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-zhaohadoop4.out[root@zhaohadoop3 sbin]# jps7665 RunJar19073 QuorumPeerMain19555 RunJar7350 RunJar21958 Jps21895 Master7194 RunJar18186 NameNode18476 ResourceManager 进入Spark的Web管理页面： http://master:8080","categories":[{"name":"大数据","slug":"大数据","permalink":"https://kkabuzs.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://kkabuzs.github.io/tags/Hadoop/"}]},{"title":"kubernetes calico网络之基于canel的网络策略","slug":"kubernetescalicowangluozhijiyucanceldewangluocelue","date":"2019-08-23T01:39:19.000Z","updated":"2019-08-23T01:39:19.000Z","comments":true,"path":"articles/2019/08/23/kubernetescalicowangluozhijiyucanceldewangluocelue/","permalink":"https://kkabuzs.github.io/articles/2019/08/23/kubernetescalicowangluozhijiyucanceldewangluocelue/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 kubernetes calico网络之基于canel的网络策略一，为什么要使用canel网络策略 在kubernetes集群中flannel网络插件为集群提供了pod网段一致性的功能，但是我们可能还希望在单个pod或者单个对单个pod进出站做一些类似于物理务器的iptables的网络策略限制 那么我们就需要安装calico，但是calico是基于bgp的网络协议才能提供它的网络功能，所以我们在此也仅仅是使用它的canel的网络策略 1.1 canal 通过 Egress 和Ingress来控制不同的网络策略 Egress： 表示出栈 代表pod作为一个源地址 对客户端访问进行响应 自己端口可预测Ingress：表示入栈 代表pod作为一个客户端 对别的pod的原地址进行访问 对方端口可预测 #二，部署canel calico官网：https://docs.projectcalico.org 12345可以直接从官网下载官方的安装yaml文件进行安装curl https://docs.projectcalico.org/v3.7/manifests/canal.yaml -O#此处注意 官方的文件下载下来以后需要对文件进行修改 将其里边的一个ip网段进行修改 需要与在创建etcd时添加的ip地址段一样 2.1 创建canel123456789101112131415161718192021[root@localhost yaml]# kubectl create -f canal.yaml configmap/canal-config createdcustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org createdclusterrole.rbac.authorization.k8s.io/calico-node createdclusterrole.rbac.authorization.k8s.io/flannel createdclusterrolebinding.rbac.authorization.k8s.io/canal-flannel createdclusterrolebinding.rbac.authorization.k8s.io/canal-calico createddaemonset.extensions/canal createdserviceaccount/canal created[root@localhost yaml]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcanal-5qjdm 1/2 Running 0 5scanal-rmx7c 0/2 PodInitializing 0 5s 2.2 创建pod进行策略测试12345678910111213#创建两个网络名称空间 然后进行相关测试[root@localhost yaml]# kubectl create namespace devnamespace/dev created[root@localhost yaml]# kubectl create namespace pordnamespace/pord created[root@localhost yaml]# kubectl get nsNAME STATUS AGEdefault Active 46ddev Active 16singress-nginx Active 18dkube-public Active 46dkube-system Active 46dpord Active 5s 2.3 创建规则文件12345678910111213[root@localhost yaml]# vim ingresspolicy.yaml apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: deny-all-ingressspec: podSelector: &#123;&#125; #这里标签选择器空表示对此名称空间下的所有pod生效此规则 policyTypes: - Ingress #只对入栈进行规则策略 默认什么都不加是拒绝所有入栈[root@localhost yaml]# kubectl apply -f ingresspolicy.yaml -n devnetworkpolicy.networking.k8s.io/deny-all-ingress created#规则就已经创建好了 接下来在此名称空间下创建一个pod进行访问测试 2.4 创建pod基于dev空间123456789101112131415161718192021222324[root@localhost yaml]# vim pod-dev.yamlapiVersion: v1kind: Podmetadata: labels: liang: jing name: pod-test namespace: devspec: containers: - image: 192.168.12.132/shendun/fengkongvue:v8 name: pod-fenkong ports: - name: fengkong-port containerPort: 80 imagePullSecrets: - name: registry-secret#创建此pod 查看pod[root@localhost yaml]# kubectl create -f pod-dev.yaml pod/pod-test created[root@localhost yaml]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpod-test 1/1 Running 0 6s 三，规则模板3.1 拒绝所有1234567891011121314151617181920212223apiVersion: extensions/v1beta1kind: NetworkPolicymetadata: name: deny-all-ingress namespace: dev spec: podSelector: &#123;&#125; policyTypes: #用来控制Ingress和Egress哪个生效。 - Ingress - Egress#此规则创建于名称空间 dev #podSelector pod选择器空 表示为此名称空间的所有pod#policyTypes Ingress 后边如果不加规则，表示默认拒绝所有入栈此名称空间所有pod的访问Egress 后边如果不加规则，表示默认拒绝所有pod出栈此名称空间*********************#如果不写Ingress的话，默认都可以访问#如果写上Ingress的话，默认都不能访问*********************该条规则的意思就是，拒绝所有其他名称空间的pod或者其他访问dev名称空间的所有pod 也限定此名称空间的所有pod访问其他名称空间的pod 3.2 允许所有访问1234567891011121314151617apiVersion: extensions/v1beta1kind: NetworkPolicymetadata: name: deny-all-ingress namespace: devspec: podSelector: &#123;&#125; ingress: - &#123;&#125; #入栈添加规则为空 表示默认允许所有入栈访问 egress: - &#123;&#125; #出栈添加规则为空 表示默认允许所有出栈访问 policyTypes: - Ingress - Egress #该条规则意思为 在dev名称空间中的pod，可以被所有其他名称空间pod进行入栈访问，也允许该名称空间中的所有pod对其他名称空间中的pod进行出栈访问 3.3 放行特定入栈规则1234567891011121314151617181920apiVersion: extensions/v1beta1kind: NetworkPolicymetadata: name: allow-app-flaskapp1spec: podSelector: matchLabels: app: flaskapp1 ingress: - from: - ipBlock: #指定ip网段 允许10.244.0.0/16这个网段的所有pod及机器访问此labels下的pod cidr: 10.244.0.0/16 except: #允许整个网段的同时怕排出网段中的一个ip地址 禁止访问32掩码表示单个ip - 10.244.1.19/32 ports: #可以访问标签选择器下的labels下的pod的哪个端口 - port: 80#该跳规则的意思是 该规则对标签选择器下含有app:flaskapp1的所有pod生效 访问策略是:允许整个10.244.0.0/16网段的所有机器对本标签下所有pod的80端口进行访问但是排出网段中10.244.1.19此ip地址的访问权限，不让此ip地址的机器访问 3.4 对选定的pod进行出栈全部限制12345678910111213apiVersion: extensions/v1beta1kind: NetworkPolicymetadata: name: deny-all-egress namespace: devspec: podSelector: matchLabels: app: flaskapp1 policyTypes: - Egress#对标签选择器 选定的pod进行出栈拒绝 拒绝访问其他的所有pod 只能访问pod本身自己 3.5 对特定pod进行特定的出栈限制1234567891011121314151617apiVersion: extensions/v1beta1kind: NetworkPolicymetadata: name: deny-all-egress namespace: devspec: podSelector: matchLabels: app: flaskapp1 egress: - to: - podSelector: #ju标签选择器 matchLabels: app: flaskapp1 #对标签选择器下的pod访问放行 policyTypes: - Egress#该条规则意思为 对标签选择器下的pod出栈访问进行全部限制 然后放行这些pod只能访问第二个标签选择器下的pod","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://kkabuzs.github.io/tags/kubernetes/"}]},{"title":"Prometheus 之 AlertManager 接入企业微信报警","slug":"prometheuszhialertmanagerjieruqiyeweixinbaojing","date":"2019-08-21T07:40:50.000Z","updated":"2019-08-21T07:40:50.000Z","comments":true,"path":"articles/2019/08/21/prometheuszhialertmanagerjieruqiyeweixinbaojing/","permalink":"https://kkabuzs.github.io/articles/2019/08/21/prometheuszhialertmanagerjieruqiyeweixinbaojing/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Prometheus 之 AlertManager 接入企业微信报警一，通过企业微信接收告警 Alertmanger从v0.12开始已经默认支持企业微信了，下面我们就一起体验一下。 1.1 准备工作 第1步：访问网站注册企业微信账号（不需要企业认证）。 第2步：访问应用创建第三方应用，点击创建应用按钮- &gt;填写应用信息： 1.2 详细配置 一路和我配置下来的同学，那么此时只要修改AlertManager configmap文件即可！ 12345678910111213141516171819202122232425262728293031323334353637[root@k8s-master01 prometheus]# cd alert/[root@k8s-master01 alert]# vim prometheus-alert-conf.yamlapiVersion: v1kind: ConfigMapmetadata: name: alert-config namespace: kube-systemdata: config.yml: |- global: resolve_timeout: 5m route: group_by: [&#x27;alertname&#x27;] group_wait: 30s group_interval: 5m repeat_interval: 5m receiver: &#x27;wechat&#x27; receivers: - name: &#x27;wechat&#x27; wechat_configs: - corp_id: &#x27;wwb289cff51760b25e&#x27; to_party: &#x27;1&#x27; agent_id: &#x27;1000003&#x27; api_secret: &#x27;Hu4HAoj_i6b1I6fFwraipkNFKksIr4SRLDvcpaHiCEg&#x27; send_resolved: true #配置文件详解：resolve_timeout：该参数定义了当Alertmanager持续多长时间未接收到告警后标记告警状态为resolved（已解决）group_wait：发送一组新的警报的初始等待时间,也就是初次发警报的延时group_interval：初始警报组如果已经发送，需要等待多长时间再发送同组新产生的其他报警repeat_interval：如果警报已经成功发送，间隔多长时间再重复发送corp_id：企业微信账号唯一ID，可以在我的企业中查看。to_party：需要发送的组。agent_id：第三方企业应用的ID，可以在自己创建的第三方企业应用详情页面查看。api_secret：第三方企业应用的密钥，可以在自己创建的第三方企业应用详情页面查看。send_resolved：发送报警恢复消息。 1.3 更新配置文件 注意多reload几次。 123456kubectl delete -f prometheus.configmap.yamlkubectl create -f prometheus.configmap.yamlkubectl delete -f alert/prometheus-alert-conf.yamlkubectl create -f alert/prometheus-alert-conf.yamlcurl -X POST &quot;http://10.108.79.122:9090/-/reload&quot;curl -X POST &quot;http://10.108.79.122:9093/-/reload&quot; 1.4 查看报警信息 1.5 报警信息格式化 由于默认的报警消息太不优雅了！！！鄙人实在是受不了，故此做了一下格式化： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 修改AlertManager configmap 配置文件[root@k8s-master01 alert]# vim prometheus-alert-conf.yamlapiVersion: v1kind: ConfigMapmetadata: name: alert-config namespace: kube-systemdata: config.yml: |- global: resolve_timeout: 5m templates: - /etc/alertmanager/wechat.tmpl #引用一个模版配置文件，因为我们AlertManager configmap挂载在/etc/alertmanager路径下，故此在此创建一个模版配置文件。 route: group_by: [&#x27;alertname&#x27;] group_wait: 30s group_interval: 5m repeat_interval: 5m receiver: &#x27;wechat&#x27; receivers: - name: &#x27;wechat&#x27; wechat_configs: - corp_id: &#x27;wwb289cff51760b25e&#x27; to_party: &#x27;1&#x27; agent_id: &#x27;1000003&#x27; api_secret: &#x27;Hu4HAoj_i6b1I6fFwraipkNFKksIr4SRLDvcpaHiCEg&#x27; send_resolved: true wechat.tmpl: | #模版配置文件信息如下： &#123;&#123;- define &quot;__text_alert_list&quot; -&#125;&#125; &#123;&#123;- range .Alerts.Firing -&#125;&#125; Alerts Firing: 告警级别: &#123;&#123; .Labels.severity &#125;&#125; 告警类型: &#123;&#123; .Labels.alertname &#125;&#125; 故障主机: &#123;&#123; .Labels.cluster &#125;&#125; 告警主题: &#123;&#123; .Annotations.summary &#125;&#125; 告警详情: &#123;&#123; .Annotations.description &#125;&#125; 触发时间: &#123;&#123; (.StartsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &#123;&#123; end &#125;&#125; &#123;&#123;- range .Alerts.Resolved -&#125;&#125; Alerts Resolved: 告警级别: &#123;&#123; .Labels.severity &#125;&#125; 告警类型: &#123;&#123; .Labels.alertname &#125;&#125; 故障主机: &#123;&#123; .Labels.cluster &#125;&#125; 告警主题: &#123;&#123; .Annotations.summary &#125;&#125; 触发时间: &#123;&#123; (.StartsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; 恢复时间: &#123;&#123; (.EndsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &#123;&#123; end &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123;- define &quot;wechat.default.message&quot; -&#125;&#125; &#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125; &#123;&#123; template &quot;__text_alert_list&quot; . &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125; &#123;&#123; template &quot;__text_alert_list&quot; . &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123;- end &#125;&#125; 1.6 格式化之后的报警消息查看 这样看来 优雅多啦！！ 1.7 再添加一条报警规则 为了测试 ，CPU使用率大于百分之0.01就触发报警。（最开始阈值设置为2%，但是我的CPU利用率实在是太低了，没办法） 123456789#修改rules.yml文件（添加如下内容） - alert: NodeCpuUsage expr: sum(sum by (container_name)( rate(container_cpu_usage_seconds_total&#123;image!=&quot;&quot;&#125;[1m] ) )) / count(node_cpu_seconds_total&#123;mode=&quot;system&quot;&#125;) * 100 &gt; 0.01 for: 1m labels: cpu: node annotations: summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: High NodeCpu usage detected&quot; description: &quot;&#123;&#123;$labels.instance&#125;&#125;: NodeCpu usage is above 0.01% (current value is: &#123;&#123; $value &#125;&#125;&quot; 下方直接跳转到完整配置文件展示 ###附录： 说明：上述配置文件中，有个labels字段，prometheus可根据labels进行分流报警。因为AlertManager configmap配置文件我只配置了一个报警媒介，下面我给大家展示邮件和企业微信两个媒介同时存在并报警分流的模版 123456789101112131415161718192021222324252627282930313233343536373839404142apiVersion: v1kind: ConfigMapmetadata: name: alert-config namespace: kube-systemdata: config.yml: |- global: resolve_timeout: 5m smtp_smarthost: &#x27;smtp.exmail.qq.com:465&#x27; smtp_from: &#x27;zhaoshuo@gagogroup.com&#x27; smtp_auth_username: &#x27;zhaoshuo@gagogroup.com&#x27; smtp_auth_password: &#x27;授权密码&#x27; smtp_hello: &#x27;exmail.qq.com&#x27; smtp_require_tls: false route: group_by: [&#x27;alertname&#x27;, &#x27;cluster&#x27;] group_wait: 30s group_interval: 5m repeat_interval: 5m receiver: wechat routes: - receiver: &#x27;email&#x27; group_wait: 10s match: #这里定义了匹配的标签，需要和prometheus里面的规则文件的标签一致，也就是有team: node标签的告警，通过邮件来告警。 team: node - receiver: &#x27;wechat&#x27; group_wait: 10s match: cpu: node #符合cpu: node标签实现wechat报警。 receivers: - name: &#x27;wechat&#x27; wechat_configs: - corp_id: &#x27;wwb289cff51760b25e&#x27; to_party: &#x27;1&#x27; agent_id: &#x27;1000003&#x27; api_secret: &#x27;Hu4HAoj_i6b1I6fFwraipkNFKksIr4SRLDvcpaHiCEg&#x27; send_resolved: true - name: &#x27;email&#x27; email_configs: - to: &#x27;zhaoshuo@gagogroup.com&#x27; send_resolved: true 看，下面图片里的配置文件，两个报警规则里面的labels不一样，那么这样就实现了Menory报警发送到邮件，CPU报警发送到企业微信中。 完整配置文件展示 更新配置文件 123456kubectl delete -f prometheus.configmap.yamlkubectl create -f prometheus.configmap.yamlkubectl delete -f alert/prometheus-alert-conf.yamlkubectl create -f alert/prometheus-alert-conf.yamlcurl -X POST &quot;http://10.108.79.122:9090/-/reload&quot;curl -X POST &quot;http://10.108.79.122:9093/-/reload&quot; 警报添加成功！！！ 已经发送报警 查看企业微信报警信息","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://kkabuzs.github.io/tags/Prometheus/"}]},{"title":"Prometheus 之 AlertManager 报警组件+邮件报警","slug":"prometheuszhialertmanagerbaojingzujian-youjianbaojing","date":"2019-08-20T02:23:55.000Z","updated":"2019-08-20T02:23:55.000Z","comments":true,"path":"articles/2019/08/20/prometheuszhialertmanagerbaojingzujian-youjianbaojing/","permalink":"https://kkabuzs.github.io/articles/2019/08/20/prometheuszhialertmanagerbaojingzujian-youjianbaojing/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Prometheus 之 AlertManager 报警组件+邮件报警一，AlertManager 简介 prometheus的兄弟alertmanager就是一个专门负责报警的组建，它独立于prometheus项目，自行运行并接收来自prometheus的请求，然后使用多种方式进行报警，之所以不是直接集成到prometheus中。 Prometheus包含了一个报警模块，那就是AlertManager，主要用于接受Prometheus发送的告警信息，它支持丰富的告警通知渠道，而且很容易做到告警信息进行去重，降噪，分组等，是一个前卫的告警通知系统 二，安装 AlerManager prometheus配置文件官方文档https://prometheus.io/docs/alerting/configuration/ 首先，我们需要先指定配置文件 ，这里我们还是创建一个ConfigMap资源对象 2.1 创建alert的ConfigMap12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@k8s-master01 alert]# vim prometheus-alert-conf.yamlapiVersion: v1kind: ConfigMapmetadata: name: alert-config namespace: kube-systemdata: config.yml: |- global: # 在没有报警的情况下声明为已解决的时间 resolve_timeout: 5m # 配置邮件发送信息 smtp_smarthost: &#x27;smtp.exmail.qq.com:465&#x27; smtp_from: &#x27;zhaoshuo@gagogroup.com&#x27; smtp_auth_username: &#x27;zhaoshuo@gagogroup.com&#x27; smtp_auth_password: &#x27;授权密码&#x27; smtp_hello: &#x27;exmail.qq.com&#x27; smtp_require_tls: false # 所有报警信息进入后的根路由，用来设置报警的分发策略 route: # 这里的标签列表是接收到报警信息后的重新分组标签，例如，接收到的报警信息里面有许多具有 cluster=A 和 alertname=LatncyHigh 这样的标签的报警信息将会批量被聚合到一个分组里面 group_by: [&#x27;alertname&#x27;, &#x27;cluster&#x27;] # 当一个新的报警分组被创建后，需要等待至少group_wait时间来初始化通知，这种方式可以确保您能有足够的时间为同一分组来获取多个警报，然后一起触发这个报警信息。 group_wait: 30s # 当第一个报警发送后，等待&#x27;group_interval&#x27;时间来发送新的一组报警信息。 group_interval: 5m # 如果一个报警信息已经发送成功了，等待&#x27;repeat_interval&#x27;时间来重新发送他们 repeat_interval: 5m # 默认的receiver：如果一个报警没有被一个route匹配，则发送给默认的接收器 receiver: default # 上面所有的属性都由所有子路由继承，并且可以在每个子路由上进行覆盖。 routes: - receiver: email group_wait: 10s match: #这里定义了匹配的标签，需要和prometheus里面的规则文件的标签一致，也就是有team: node标签的告警，通过邮件来告警。 team: node receivers: - name: &#x27;default&#x27; email_configs: - to: &#x27;zhaoshuo@gagogroup.com&#x27; send_resolved: true - name: &#x27;email&#x27; email_configs: - to: &#x27;zhaoshuo@gagogroup.com&#x27; send_resolved: true 创建alertmanager的配置文件 1234567891011121314[root@k8s-master01 alert]# kubectl create -f prometheus-alert-conf.yamlconfigmap/alert-config created[root@k8s-master01 alert]# kubectl get cm -n kube-systemNAME DATA AGEalert-config 1 8scoredns 1 10dextension-apiserver-authentication 6 10dkube-flannel-cfg 2 10dkube-proxy 2 10dkubeadm-config 2 10dkubelet-config-1.14 1 10dprometheus-config 1 3d18h#这里已经显示我们创建好的alert-config 2.2 在之前的prometheus pod的yaml文件中添加这个容器 这里我们将上面创建的aler-config这个configmap资源对象volume的形式挂载到&#x2F;etc&#x2F;alertmanager目录下去，然后在启动参数中指定–config.file&#x3D;&#x2F;etc&#x2F;alertmanager&#x2F;config.yml 12345678910111213141516171819202122232425262728293031- name: alermanager image: prom/alertmanager:v0.15.3 imagePullPolicy: IfNotPresent args: - &quot;--config.file=/etc/alertmanager/config.yml&quot; - &quot;--storage.path=/alertmanager/data&quot; ports: - containerPort: 9093 name: http volumeMounts: - mountPath: &quot;/etc/alertmanager&quot; name: alertcfg resources: requests: cpu: 100m memory: 256Mi limits: cpu: 200m memory: 1024MisecurityContext: runAsUser: 0volumes:- name: data persistentVolumeClaim: claimName: prometheus- configMap: name: prometheus-config name: config-volume- name: alertcfg configMap: name: alert-config 说明：在0.15版本，alertmanager的WORKDIR发生了变化，变成&#x2F;etc&#x2F;alertmanager默认情况下存储路径–storage.path是相对目录data&#x2F;，因此alertmanager会在我们上面挂载的ConfigMap中去创建这个目录，所以会报错，这里通过–storage.path参数来解决 1234#更新deployment[root@k8s-master01 prometheus]# kubectl apply -f prometheus.deploy.yamlWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl applydeployment.extensions/prometheus configured 下面为完整的yaml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: prometheus namespace: kube-system labels: app: prometheusspec: template: metadata: labels: app: prometheus spec: serviceAccountName: prometheus containers: - image: prom/prometheus:v2.4.3 name: prometheus command: - &quot;/bin/prometheus&quot; args: - &quot;--config.file=/etc/prometheus/prometheus.yml&quot; - &quot;--storage.tsdb.path=/prometheus&quot; - &quot;--storage.tsdb.retention=7d&quot; - &quot;--web.enable-admin-api&quot; # 控制对admin HTTP API的访问，其中包括删除时间序列等功能 - &quot;--web.enable-lifecycle&quot; # 支持热更新，直接执行localhost:9090/-/reload立即生效 ports: - containerPort: 9090 protocol: TCP name: http volumeMounts: - mountPath: &quot;/prometheus&quot; subPath: prometheus name: data - mountPath: &quot;/etc/prometheus&quot; name: config-volume resources: requests: cpu: 400m memory: 100Mi limits: cpu: 400m memory: 2Gi - name: alermanager image: prom/alertmanager:v0.15.3 imagePullPolicy: IfNotPresent args: - &quot;--config.file=/etc/alertmanager/config.yml&quot; - &quot;--storage.path=/alertmanager/data&quot; ports: - containerPort: 9093 name: http volumeMounts: - mountPath: &quot;/etc/alertmanager&quot; name: alertcfg resources: requests: cpu: 100m memory: 256Mi limits: cpu: 200m memory: 1024Mi securityContext: runAsUser: 0 volumes: - name: data persistentVolumeClaim: claimName: prometheus - configMap: name: prometheus-config name: config-volume - name: alertcfg configMap: name: alert-config 查看一下pod启动状态 12[root@k8s-master01 prometheus]# kubectl get pod -n kube-system |grep prometheusprometheus-5bc9b9c67c-4wcm5 2/2 Running 0 43s 2.3 修改prometheus的configmap AlertManager容器启动之后，我们还需要在Prometheus中配置下AlertManager的地址，让Prometheus能够访问AlertManager 1234alerting: alertmanagers: - static_configs: - targets: [&quot;localhost:9093&quot;] 配置截图 更新configmap配置文件 1234567[root@k8s-master01 prometheus]# kubectl delete -f prometheus.configmap.yamlconfigmap &quot;prometheus-config&quot; deleted[root@k8s-master01 prometheus]# kubectl create -f prometheus.configmap.yamlconfigmap/prometheus-config created[root@k8s-master01 prometheus]# curl -X POST &quot;http://10.108.79.122:9090/-/reload&quot;#确保更新配置没有报错（刷新比较慢可以等等） 现在prometheus alertmanager并没有告警的规则，还需要我们添加报警规则 三，Prometheus 报警规则 上面我们将prometheus和alertmanager进行了关联，但是现在并没有报警规则，所以这里还需要配置一些报警规则。让prometheus触发报警 3.1 添加报警规则123#首先在prometheus.configmap.yaml文件中添加报警规则，下面的文件就是prometheus报警的规则文件 rule_files: - /etc/prometheus/rules.yml 报警规则允许基于Prometheus表达式语言来定义报警规则条件，并在出发报警时发送给外部 我们上面已经将&#x2F;etc&#x2F;promtehus进行挂载了，所以这里只需要修改prometheus-configmap就可以了。 123456789101112131415161718192021222324252627 rules.yml: | groups: - name: abcdocker-test-rule rules: - alert: NodeMemoryUsage expr: (node_memory_MemTotal_bytes - (node_memory_MemFree_bytes+node_memory_Buffers_bytes + node_memory_Cached_bytes)) / node_memory_MemTotal_bytes * 100 &gt; 50 for: 1m labels: team: node annotations: summary: &quot;&#123;&#123; $labels.instance &#125;&#125;：High Memory Usage detected&quot; description: &quot;&#123;&#123; $labels.instance &#125;&#125;: Memory usage us avive 50% (current value is :: &#123;&#123; $value &#125;&#125;)&quot; #配置相关参数说明 rules.yml: | groups: - name: abcdocker-test-rule rules: #规则 - alert: NodeMemoryUsage #报警名称(内存报警) expr: (node_memory_MemTotal_bytes - (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes)) / node_memory_MemTotal_bytes * 100 &gt; 50 #规则表达式 for: 1m #等待1分钟执行查询条件 labels: team: node #当我们触发报警后，带有team=node的标签，并且这里走的是我们alertmanager node标签，这里对应的就是我们的email接收器 annotations: #指定另外一组标签，不会将这个标签当做我们告警的身份标示（不会在我们报警信息里操作）这里主要是用于额外的展示，例如发送给邮件里面&gt;的报警信息 summary: &quot;&#123;&#123; $labels.instance &#125;&#125;：High Memory Usage detected&quot; #label标签，instance代表节点名称 description: &quot;&#123;&#123; $labels.instance &#125;&#125;: Memory usage us avive 50% (current value is :: &#123;&#123; $value &#125;&#125;)&quot; #描述：相当于报警信息 $value代表当前值 说明一点expr所执行的命令是可以在prometheus上获取到数据的 配置截图 报警说明 本次报警大概意思是当服务器内存百分比大于80的时候，就进行报警，并且通过labels标签关联team:node (这里team&#x3D;node是在我们alertmanager里面配置的接收器，默认是default)，并且报警内容添加主机和当前内存使用率 接下来我们访问prometheus，点击alerts，就可以看到我们添加的NodeMemoryUsage 我这里将脚本改成&gt;30 (为了测试，正常值应为80或者85) 更新配置文件 12345[root@k8s-master01 prometheus]# kubectl delete -f prometheus.configmap.yamlconfigmap &quot;prometheus-config&quot; deleted[root@k8s-master01 prometheus]# kubectl create -f prometheus.configmap.yamlconfigmap/prometheus-config created[root@k8s-master01 prometheus]# curl -X POST &quot;http://10.108.79.122:9090/-/reload&quot; 等待1分钟后 prometheus进入了等待PENDING状态 当前值已经大于我们设置的30%，现在已经出发报警 邮件内容如下 3.2 alertManager Ui界面 我们可以在邮件内容中看到包含View in AlertManager的链接，这是alertmanager自带的Ui界面。我们可以使用NodePort进行访问 这里需要修改一下prometheus的service 12345678910111213141516171819[root@k8s-master01 prometheus]# vim prometeheus-svc.yamlapiVersion: v1kind: Servicemetadata: name: prometheus namespace: kube-system labels: app: prometheusspec: selector: app: prometheus type: NodePort ports: - name: web port: 9090 targetPort: http - name: alertmanager port: 9093 targetPort: 9093 如果前面也是按照我的文档操作的，可以在prometheus后面添加一个svc端口接口 123- name: alertmanager port: 9093 targetPort: 9093 12345678#更新prometheus的svc[root@k8s-master01 prometheus]# kubectl apply -f prometeheus-svc.yamlWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl applyservice/prometheus configured#我们查看一下node-port端口[root@k8s-master01 prometheus]# kubectl get svc -n kube-system |grep prometheusprometheus NodePort 10.108.79.122 &lt;none&gt; 9090:31062/TCP,9093:30798/TCP 5d2h 访问alertmanager端口为9093&#x3D;30798 (集群任一节点访问即可) 在上面的图片，我们可以看到hostname为k8s-master01一直在报警，如果不想接收这个IP报警。可以点击Slience 注意Prometheus有8小时时区问题 这时候报警匹配为k8s-master01的hostname，在2个小时内。不进行报警，我们点击创建就可以。在Comment输入提交内容就可以了 这里我们已经看不到报警内容了 点击Silences可以看到被禁用的监控","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://kkabuzs.github.io/tags/Prometheus/"}]},{"title":"Prometheus 之接入Grafana","slug":"prometheusjierugrafana","date":"2019-08-19T01:42:02.000Z","updated":"2019-08-19T01:42:02.000Z","comments":true,"path":"articles/2019/08/19/prometheusjierugrafana/","permalink":"https://kkabuzs.github.io/articles/2019/08/19/prometheusjierugrafana/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Prometheus 之接入Grafana一，Grafana介绍由于Prometheus自带的web Ui图标功能相对较弱，所以一般情况下我们会使用一个第三方的工具来展示这些数据 Grafana是一个跨平台的开源的度量分析和可视化工具，可以通过将采集的数据查询然后可视化的展示，并及时通知。 grafana 是一个可视化面包，有着非常漂亮的图片和布局展示，功能齐全的度量仪表盘和图形化编辑器，支持Graphite、Zabbix、InfluxDB、Prometheus、OpenTSDB、Elasticasearch等作为数据源，比Prometheus自带的图标展示功能强大很多，更加灵活，有丰富的插件. 1.1 部署Grafana12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[root@k8s-master01 grafana]# vim grafana_deployment.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: grafana namespace: kube-system labels: app: grafanaspec: revisionHistoryLimit: 10 template: metadata: labels: app: grafana spec: containers: - name: grafana image: grafana/grafana:5.3.4 imagePullPolicy: IfNotPresent ports: - containerPort: 3000 name: grafana env: - name: GF_SECURITY_ADMIN_USER value: admin - name: GF_SECURITY_ADMIN_PASSWORD value: admin readinessProbe: failureThreshold: 10 httpGet: path: /api/health port: 3000 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 30 livenessProbe: failureThreshold: 3 httpGet: path: /api/health port: 3000 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: limits: cpu: 300m memory: 1024Mi requests: cpu: 300m memory: 1024Mi volumeMounts: - mountPath: /var/lib/grafana subPath: grafana name: storage securityContext: fsGroup: 472 runAsUser: 472 volumes: - name: storage persistentVolumeClaim: claimName: grafana 这里使用了grafana 5.3.4的镜像，添加了监控检查、资源声明，比较重要的变量是GF_SECURITY_ADMIN_USER和GF_SECURITY_ADMIN_PASSWORD为grafana的账号和密码。 由于grafana将dashboard、插件这些数据保留在&#x2F;var&#x2F;lib&#x2F;grafana目录下，所以我们这里需要做持久化，同时要针对这个目录做挂载声明，由于5.3.4版本用户的userid和groupid都有所变化，所以这里添加了一个securityContext设置用户ID 1.2 添加pv pvc123456789101112131415161718192021222324[root@k8s-master01 ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEnfs-pv01 1Gi RWX Retain Bound default/www-web-0 5d18hnfs-pv02 2Gi RWX Retain Bound default/www-web-1 5d18hnfs-pv03 3Gi RWO Retain Available 5d18hnfs-pv04 4Gi RWX Retain Bound default/www-web-2 5d18hnfs-pv05 5Gi RWX Retain Bound kube-system/prometheus 5d18h#说明：可以看到我们还有一个可以使用的pv，故此不用创建pv了，直接创建一个pvc，用于绑定Grafana。[root@k8s-master01 ~]# mkdir grafana[root@k8s-master01 ~]# cd grafana/[root@k8s-master01 grafana]# vim grafana_pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: grafana namespace: kube-systemspec: accessModes: - ReadWriteOnce resources: requests: storage: 3Gi 1.3 创建service，使用NodePort1234567891011121314[root@k8s-master01 grafana]# vim grafana_svc.yamlapiVersion: v1kind: Servicemetadata: name: grafana namespace: kube-system labels: app: grafanaspec: type: NodePort ports: - port: 3000 selector: app: grafana 1.4 创建一个job 由于5.1 (可以选择5.1之前的docker镜像，可以避免此类错误) 版本后groupid更改，同时我们将&#x2F;var&#x2F;lib&#x2F;grafana挂载到pvc后，目录拥有者可能不是grafana用户，所以我们还需要添加一个Job用于授权目录 12345678910111213141516171819202122232425[root@k8s-master01 grafana]# vim grafana_job.yamlapiVersion: batch/v1kind: Jobmetadata: name: grafana-chown namespace: kube-systemspec: template: spec: restartPolicy: Never containers: - name: grafana-chown command: [&quot;chown&quot;, &quot;-R&quot;, &quot;472:472&quot;, &quot;/var/lib/grafana&quot;] image: busybox imagePullPolicy: IfNotPresent volumeMounts: - name: storage subPath: grafana mountPath: /var/lib/grafana volumes: - name: storage persistentVolumeClaim: claimName: grafana# 这里使用一个busybox镜像将/var/lib/grafana目录修改为权限472 1.5 创建所有资源（注意创建顺序）1234567891011121314151617181920212223242526#创建pvc[root@k8s-master01 grafana]# kubectl create -f grafana_pvc.yamlpersistentvolumeclaim/grafana created[root@k8s-master01 grafana]# kubectl get pvc -n kube-system | grep grafanagrafana Bound nfs-pv03 3Gi RWO 29s#创建job[root@k8s-master01 grafana]# kubectl create -f grafana_job.yamljob.batch/grafana-chown created[root@k8s-master01 grafana]# kubectl get job -n kube-systemNAME COMPLETIONS DURATION AGEgrafana-chown 1/1 10s 20s#创建deployment[root@k8s-master01 grafana]# kubectl create -f grafana_deployment.yamldeployment.extensions/grafana created[root@k8s-master01 grafana]# kubectl get deploy -n kube-systemNAME READY UP-TO-DATE AVAILABLE AGEcoredns 2/2 2 2 9dgrafana 1/1 1 1 4m #启动成功kube-state-metrics 1/1 1 1 2d17hprometheus 1/1 1 1 3d23h#创建service[root@k8s-master01 grafana]# kubectl create -f grafana_svc.yamlservice/grafana created 1.6 创建完成后我们打开grafana的dashboard界面12[root@k8s-master01 grafana]# kubectl get svc -n kube-system | grep gragrafana NodePort 10.101.203.108 &lt;none&gt; 3000:30123/TCP 23s 然后我们在任意集群中的节点访问端口为30123 这里的集群密码就是上面我们创建deployment里面设置的变量，我这里用户设置为admin密码admin 二，Grafana web 基本操作2.1 登陆grafana 首次登陆提示修改密码，我们不做修改。 2.2 登陆到grafana就显示到了引导界面 2.3 第一次创建grafana需要添加数据源 2.4 类型选择prometheus 这里的地址我们填写下面的urlhttp://prometheus.kube-system.svc.cluster.local:9090 这里的prometheus代表service名称 kube-system代表命名空间 这里的Access配置要说明一下: 服务器(Server)访问模式（默认）：所有请求都将从浏览器发送到 Grafana 后端的服务器，后者又将请求转发到数据源，通过这种方式可以避免一些跨域问题，其实就是在 Grafana 后端做了一次转发，需要从Grafana 后端服务器访问该 URL。 浏览器(Browser)访问模式：所有请求都将从浏览器直接发送到数据源，但是有可能会有一些跨域的限制，使用此访问模式，需要从浏览器直接访问该 URL。 检查一下数据源是否配置成功 2.5 数据源添加完毕后，接下来添加New dashboard 这里我们可以自定义模板，或者可以使用别人写好的模板 （写好的模板后面是需要我们自己修改的） grafana提供了很多模板，类似和docker镜像仓库一下。导入模板也极其简单。点击上方的Dashboard 这里面的模板都是公共的，可以免费使用 点进去任意一个模板后，我们可以看到ID，复制ID然后在返回grafana 我这里添加一个监控Kubernetes集群。显示整体群集CPU、内存、磁盘使用情况以及单个pod统计信息。 https://grafana.com/grafana/dashboards/8588 2.6 点击导入模板 在这里我们输入8588或者url，会自动跳转到配置页面 https://grafana.com/grafana/dashboards/8588 选择好数据源之后，我们在点击Import即可 这里就会将模板8588给我们导入进行 这里就会获取我们prometheus里面的数据了 2.7 现在的模板还没有进行保存，我们要点击保存一下 至此，Grafana接入Prometheus完毕！！！ 三，附录 以上模版为8588模版，首排监控的deployment的信息，哪我们想看集群的资源占用怎么办呢？可以在创建一个162模版。 3.1 grafana模板修改3.1.1Cluster memory usage (集群内存使用率) 计算方式就是(整个集群的内存-(整个集群剩余的内存以及Buffer和Cached))/整 1(sum(node_memory_MemTotal_bytes) - sum(node_memory_MemFree_bytes + node_memory_Buffers_bytes+node_memory_Cached_bytes)) / sum(node_memory_MemTotal_bytes) * 100 这里要说明一点，这里填写的是PromSQL，也就是说是可以在prometheus查询到的。 如果查询不到grafana也是会获取不到数据的 这里在prometheus是可以获取到的 3.1.2 Cluster memory usage 配置如下 (集群内存使用率)1sum(sum by (container_name)( rate(container_cpu_usage_seconds_total&#123;image!=&quot;&quot;&#125;[1m] ) )) / count(node_cpu_seconds_total&#123;mode=&quot;system&quot;&#125;) * 100 3.1.3 Cluster filesystem usage 集群文件系统使用率1(sum(node_filesystem_size_bytes&#123;device=&quot;tmpfs&quot;&#125;) - sum(node_filesystem_free_bytes&#123;device=&quot;tmpfs&quot;&#125;) ) / sum(node_filesystem_size_bytes&#123;device=&quot;tmpfs&quot;&#125;) * 100 3.1.4 配置集群中Pod cpu使用率1sum by (pod_name)(rate(container_cpu_usage_seconds_total&#123;image!=&quot;&quot;, pod_name!=&quot;&quot;&#125;[1m])) 下面显示的地方配置 1&#123;&#123; pod_name &#125;&#125; 3.1.5 集群pod 内存使用率1sort_desc(sum (container_memory_usage_bytes&#123;image!=&quot;&quot;, pod_name!=&quot;&quot;&#125;) by(pod_name)) 下面显示的名称同样也是&#123;&#123; pod_name &#125;&#125; 3.1.6 配置Pod 网络监控12345671.入口流量sort_desc(sum by (pod_name) (rate (container_network_receive_bytes_total&#123;name!=&quot;&quot;&#125;[1m]) ))2.出口流量sort_desc(sum by (pod_name) (rate (container_network_transmit_bytes_total&#123;name!=&quot;&quot;&#125;[1m]) ))#监控时间为1分钟 说明： 如要配置，配置完毕记得保存！！！！ 所有的PromSQL都是可以在prometheus获取到数据的！","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://kkabuzs.github.io/tags/Prometheus/"},{"name":"Grafana","slug":"Grafana","permalink":"https://kkabuzs.github.io/tags/Grafana/"}]},{"title":"Prometheus 监控 Kubernetes 集群节点及应用","slug":"prometheusjiankongkubernetesjiqunjieidanjiyingyong","date":"2019-08-16T01:43:40.000Z","updated":"2019-08-16T01:43:40.000Z","comments":true,"path":"articles/2019/08/16/prometheusjiankongkubernetesjiqunjieidanjiyingyong/","permalink":"https://kkabuzs.github.io/articles/2019/08/16/prometheusjiankongkubernetesjiqunjieidanjiyingyong/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Prometheus 监控 Kubernetes 集群节点及应用一，Prometheus监控Kubernetes 集群节点及应用 对于Kubernetes的集群监控一般我们需要考虑一下几方面: Kubernetes节点的监控；比如节点的cpu、load、fdisk、memory等指标 内部系统组件的状态；比如kube-scheduler、kube-controller-manager、kubedns&#x2F;coredns等组件的运行状态 编排级的metrics；比如Deployment的状态、资源请求、调度和API延迟等数据指标 1.1 监控方案Kubernetes集群的监控方案主要有以下几种方案 Heapster:Herapster是一个集群范围的监控和数据聚合工具，以Pod的形式运行在集群中 Kubelet&#x2F;cAdvisor之外，我们还可以向Heapster添加其他指标源数据，比如kube-state-metrics Heapster已经被废弃，使用metrics-server代替 cAvisor:cAdvisor是Google开源的容器资源监控和性能分析工具，它是专门为容器而生，本身也支持Docker容器，Kubernetes中，我们不需要单独去安装，cAdvisor作为kubelet内置的一部分程序可以直接使用 Kube-state-metrics:通过监听API Server生成有关资源对象的状态指标，比如Deployment、Node、Pod，需要注意的是kube-state-metrics只是简单的提供一个metrics数据，并不会存储这些指标数据，所以我们可以使用Prometheus来抓取这些数据然后存储 metrics-server:metrics-server也是一个集群范围内的资源数据局和工具，是Heapster的代替品，同样的，metrics-server也只是显示数据，并不提供数据存储服务。 不过kube-state-metrics和metrics-server之前还有很大不同的，二者主要区别如下 kube-state-metrics主要关注的是业务相关的一些元数据，比如Deployment、Pod、副本状态等 metrics-service主要关注的是资源度量API的实现，比如CPU、文件描述符、内存、请求延时等指标 二，监控集群节点 首先需要我们监控集群的节点，要监控节点其实我们已经有很多非常成熟的方案了，比如Nagios、Zabbix，甚至可以我们自己收集数据，这里我们通过prometheus来采集节点的监控指标，可以通过node_exporter获取，node_exporter就是抓取用于采集服务器节点的各种运行指标，目前node_exporter几乎支持所有常见的监控点，比如cpu、distats、loadavg、meminfo、netstat等，详细的监控列表可以参考github repo 这里使用DeamonSet控制器来部署该服务，这样每一个节点都会运行一个Pod，如果我们从集群中删除或添加节点后，也会进行自动扩展 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071[root@k8s-master01 k8s]# vim prometheus-node-exporter.yamlapiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: node-exporter namespace: kube-system labels: name: node-exporterspec: template: metadata: labels: name: node-exporter spec: hostPID: true hostIPC: true hostNetwork: true containers: - name: node-exporter image: prom/node-exporter:v0.16.0 ports: - containerPort: 9100 resources: requests: cpu: 0.15 securityContext: privileged: true args: - --path.procfs - /host/proc - --path.sysfs - /host/sys - --collector.filesystem.ignored-mount-points - &#x27;&quot;^/(sys|proc|dev|host|etc)($|/)&quot;&#x27; volumeMounts: - name: dev mountPath: /host/dev - name: proc mountPath: /host/proc - name: sys mountPath: /host/sys - name: rootfs mountPath: /rootfs tolerations: - key: &quot;node-role.kubernetes.io/master&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; volumes: - name: proc hostPath: path: /proc - name: dev hostPath: path: /dev - name: sys hostPath: path: /sys - name: rootfs hostPath: path: /#创建node-exporter并检查pod[root@k8s-master01 k8s]# kubectl create -f prometheus-node-exporter.yamldaemonset.extensions/node-exporter created[root@k8s-master01 k8s]# kubectl get pod -n kube-system -o wide|grep node-exoorter[root@k8s-master01 k8s]# kubectl get pod -n kube-system -o wide|grep node-exporter #这里我们可以看到，我们有3个节点，在所有的节点上都启动了一个对应Pod进行获取数据node-exporter-88hlw 1/1 Running 0 52s 192.168.101.61 k8s-master01 &lt;none&gt; &lt;none&gt;node-exporter-vw7v6 1/1 Running 0 52s 192.168.101.125 k8s-node02 &lt;none&gt; &lt;none&gt;node-exporter-zrdp6 1/1 Running 0 52s 192.168.101.62 k8s-node01 &lt;none&gt; &lt;none&gt; 2.1 node-exporter.yaml文件说明 由于我们要获取的数据是主机的监控指标数据，而我们的node-exporter是运行在容器中的，所以我们在Pod中需要配置一些Pod的安全策略 12345hostPID:truehostIPC:truehostNetwork:true#这三个配置主要用于主机的PID namespace、IPC namespace以及主机网络，这里需要注意的是namespace是用于容器隔离的关键技术，这里的namespace和集群中的namespace是两个完全不同的概念 另外我们还需要将主机&#x2F;dev、&#x2F;proc、&#x2F;sys这些目录挂在到容器中，这些因为我们采集的很多节点数据都是通过这些文件来获取系统信息 比如我们在执行top命令可以查看当前cpu使用情况，数据就来源于&#x2F;proc&#x2F;stat，使用free命令可以查看当前内存使用情况，其数据来源是&#x2F;proc&#x2F;meminfo文件 另外如果是使用kubeadm搭建的，同时需要监控master节点的，则需要添加下方的相应内容 123- key: &quot;node-role.kubernetes.io/master&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule 2.2 node-exporter容器相关启动参数1234567args:- --path.procfs #配置挂载宿主机（node节点）的路径- /host/proc- --path.sysfs #配置挂载宿主机（node节点）的路径- /host/sys- --collector.filesystem.ignored-mount-points- &#x27;&quot;^/(sys|proc|dev|host|etc)($|/)&quot;&#x27; 说明：在我们的yaml文件中加入了hostNetwork:true会直接将我们的宿主机的9100端口映射出来，从而不需要创建service 在我们的宿主机上就会有一个9100的端口 容器的9100---&gt;映射到宿主机9100 123456hostNetwork: truecontainers:- name: node-exporter image: prom/node-exporter:v0.16.0 ports: - containerPort: 9100 上面我们检查了Pod的运行状态都是正常的，接下来我们要查看一下Pod日志，以及node-exporter中的metrics 使用命令kubectl logs -n 命名空间 node-exporter中Pod名称检查Pod日志是否有额外报错 123456789101112131415161718192021222324252627282930313233343536[root@k8s-master01 k8s]# kubectl log -n kube-system node-exporter-88hlwlog is DEPRECATED and will be removed in a future version. Use logs instead.time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot;Starting node_exporter (version=0.16.0, branch=HEAD, revision=d42bd70f4363dced6b77d8fc311ea57b63387e4f)&quot; source=&quot;node_exporter.go:82&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot;Build context (go=go1.9.6, user=root@a67a9bc13a69, date=20180515-15:52:42)&quot; source=&quot;node_exporter.go:83&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot;Enabled collectors:&quot; source=&quot;node_exporter.go:90&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - arp&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - bcache&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - bonding&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - conntrack&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - cpu&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - diskstats&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - edac&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - entropy&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - filefd&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - filesystem&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - hwmon&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - infiniband&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - ipvs&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - loadavg&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - mdadm&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - meminfo&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - netdev&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - netstat&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - nfs&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - nfsd&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - sockstat&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - stat&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - textfile&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - time&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - timex&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - uname&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - vmstat&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - wifi&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - xfs&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot; - zfs&quot; source=&quot;node_exporter.go:97&quot;time=&quot;2019-08-16T03:09:18Z&quot; level=info msg=&quot;Listening on :9100&quot; source=&quot;node_exporter.go:111&quot; 1234567891011121314151617181920212223242526272829303132333435#############################################接下来，我们在任意集群节点curl 9100/metrics#############################################[root@k8s-master01 k8s]# curl 127.0.0.1:9100/metrics# HELP go_gc_duration_seconds A summary of the GC invocation durations.# TYPE go_gc_duration_seconds summarygo_gc_duration_seconds&#123;quantile=&quot;0&quot;&#125; 0go_gc_duration_seconds&#123;quantile=&quot;0.25&quot;&#125; 0go_gc_duration_seconds&#123;quantile=&quot;0.5&quot;&#125; 0go_gc_duration_seconds&#123;quantile=&quot;0.75&quot;&#125; 0go_gc_duration_seconds&#123;quantile=&quot;1&quot;&#125; 0go_gc_duration_seconds_sum 0go_gc_duration_seconds_count 0# HELP go_goroutines Number of goroutines that currently exist.# TYPE go_goroutines gaugego_goroutines 6# HELP go_info Information about the Go environment.# TYPE go_info gaugego_info&#123;version=&quot;go1.9.6&quot;&#125; 1# HELP go_memstats_alloc_bytes Number of bytes allocated and still in use.# TYPE go_memstats_alloc_bytes gaugego_memstats_alloc_bytes 1.385208e+06···省略若干···promhttp_metric_handler_requests_in_flight 1# HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code.# TYPE promhttp_metric_handler_requests_total counterpromhttp_metric_handler_requests_total&#123;code=&quot;200&quot;&#125; 0promhttp_metric_handler_requests_total&#123;code=&quot;500&quot;&#125; 0promhttp_metric_handler_requests_total&#123;code=&quot;503&quot;&#125; 0说明：只要metrics可以获取到数据说明node-exporter没有问题 2.3 服务发现 我们这里三个节点都运行了node-exporter程序，如果我们通过一个Server来将数据收集在一起，用静态的方式配置到prometheus就会显示一条数据，我们得自己在指标中过滤每个节点的数据，配置比较麻烦。 这里就采用服务发现 在Kubernetes下，Prometheus通过Kubernetes API基础，目前主要支持5种服务发现，分别是node、Server、Pod、Endpoints、Ingress 1234567#需要我们在Prometheus配置文件中，添加如下三行 - job_name: &#x27;kubernetes-node&#x27; kubernetes_sd_configs: - role: node#通过制定Kubernetes_sd_config的模式为node，prometheus就会自动从Kubernetes中发现所有的node节点并作为当前job监控的目标实例，发现的节点/metrics接口是默认的kubelet的HTTP接口 完整版configmap如下： 123456789101112131415161718192021222324252627282930313233apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: kube-systemdata: prometheus.yml: | global: scrape_interval: 15s scrape_timeout: 15s scrape_configs: - job_name: &#x27;prometheus&#x27; static_configs: - targets: [&#x27;localhost:9090&#x27;] - job_name: &#x27;redis&#x27; static_configs: - targets: [&#x27;redis.zhaoshuo.svc.cluster.local:9121&#x27;] - job_name: &#x27;kubernetes-node&#x27; #新添加行 kubernetes_sd_configs: - role: node#接下来我们更新配置文件[root@k8s-master01 prometheus]# kubectl delete -f prometheus.configmap.yamlconfigmap &quot;prometheus-config&quot; deleted[root@k8s-master01 prometheus]# kubectl create -f prometheus.configmap.yamlconfigmap/prometheus-config created[root@k8s-master01 prometheus]# kubectl get svc -n kube-system |grep prometheusprometheus NodePort 10.108.79.122 &lt;none&gt; 9090:31062/TCP 23h#热更新刷新配置（可能稍微需要等待一小会）[root@k8s-master01 prometheus]# curl -X POST &quot;http://10.108.79.122:9090/-/reload&quot; 此时进行web访问： 现在我们可以看到已经获取到我们的Node节点的IP，但是由于metrics监听的端口是10250而并不是我们设置的9100，所以提示我们节点属于Down的状态 这里我们就需要使用Prometheus提供的relabel_configs中的replace能力了，relabel可以在Prometheus采集数据之前，通过Target实例的Metadata信息，动态重新写入Label的值。除此之外，我们还能根据Target实例的Metadata信息选择是否采集或者忽略该Target实例。这里使用__address__标签替换10250端口为9100 12345678910#这里使用正则进行替换端口，添加到configmap中 - job_name: &#x27;kubernetes-node&#x27; kubernetes_sd_configs: - role: node relabel_configs: #新增字段 - source_labels: [__address__] regex: &#x27;(.*):10250&#x27; replacement: &#x27;$&#123;1&#125;:9100&#x27; target_label: __address__ action: replace 完整版如下： 123456789101112131415161718192021222324252627282930313233343536apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: kube-systemdata: prometheus.yml: | global: scrape_interval: 15s scrape_timeout: 15s scrape_configs: - job_name: &#x27;prometheus&#x27; static_configs: - targets: [&#x27;localhost:9090&#x27;] - job_name: &#x27;redis&#x27; static_configs: - targets: [&#x27;redis.zhaoshuo.svc.cluster.local:9121&#x27;] - job_name: &#x27;kubernetes-node&#x27; kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__address__] regex: &#x27;(.*):10250&#x27; replacement: &#x27;$&#123;1&#125;:9100&#x27; target_label: __address__ action: replace#接下来我们更新一下配置[root@k8s-master01 prometheus]# kubectl delete -f prometheus.configmap.yamlconfigmap &quot;prometheus-config&quot; deleted[root@k8s-master01 prometheus]# kubectl create -f prometheus.configmap.yamlconfigmap/prometheus-config created[root@k8s-master01 prometheus]# kubectl get svc -n kube-system |grep prometheusprometheus NodePort 10.108.79.122 &lt;none&gt; 9090:31062/TCP 23h[root@k8s-master01 prometheus]# curl -X POST &quot;http://10.108.79.122:9090/-/reload&quot; 此时集群显示正常 目前状态已经正常，但是还有一个问题就是我们的采集数据只显示了IP地址，对于我们监控分组分类不是很方便，这里可以通过labelmap这个属性来将Kubernetes的Label标签添加为Prometheus的指标标签 此处为可选配置 1234567891011- job_name: &#x27;kubernetes-node&#x27; kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__address__] regex: &#x27;(.*):10250&#x27; replacement: &#x27;$&#123;1&#125;:9100&#x27; target_label: __address__ action: replace - action: labelmap #新增labelmap action regex: __meta_kubernetes_node_label_(.+) 添加了一个action为labelmap，正则表达式是__meta_kubernetes_node(.+)的配置，这里的意思就是表达式中匹配的数据也添加到指标数据的Label标签中去。 完整配置文件 12345678910111213141516171819202122232425262728293031323334353637apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: kube-systemdata: prometheus.yml: | global: scrape_interval: 15s scrape_timeout: 15s scrape_configs: - job_name: &#x27;prometheus&#x27; static_configs: - targets: [&#x27;localhost:9090&#x27;] - job_name: &#x27;redis&#x27; static_configs: - targets: [&#x27;redis.zhaoshuo.svc.cluster.local:9121&#x27;] - job_name: &#x27;kubernetes-node&#x27; kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__address__] regex: &#x27;(.*):10250&#x27; replacement: &#x27;$&#123;1&#125;:9100&#x27; target_label: __address__ action: replace - action: labelmap regex: __meta_kubernetes_node_label_(.+) #更新配置[root@k8s-master01 prometheus]# kubectl delete -f prometheus.configmap.yamlconfigmap &quot;prometheus-config&quot; deleted[root@k8s-master01 prometheus]# kubectl create -f prometheus.configmap.yamlconfigmap/prometheus-config created[root@k8s-master01 prometheus]# kubectl get svc -n kube-system |grep prometheusprometheus NodePort 10.108.79.122 &lt;none&gt; 9090:31062/TCP 26h[root@k8s-master01 prometheus]# curl -X POST &quot;http://10.108.79.122:9090/-/reload&quot; 实际上就是获取我们的标签 12345[root@k8s-master01 prometheus]# kubectl get nodes --show-labelsNAME STATUS ROLES AGE VERSION LABELSk8s-master01 Ready master 7d2h v1.14.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master01,kubernetes.io/os=linux,node-role.kubernetes.io/master=k8s-node01 Ready &lt;none&gt; 7d2h v1.14.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node01,kubernetes.io/os=linuxk8s-node02 Ready &lt;none&gt; 7d2h v1.14.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node02,kubernetes.io/os=linux 对于Kubernetes_sd_configs下面可用的元标签如下 __meta_kubernetes_node_name: 节点对象的名称 _meta_kubernetes_node_label: 节点对象中的每个标签 _meta_kubernetes_node_annotation: 来自节点对象的每个注释 _meta_kubernetes_node_address: 每个节点地址类型的第一个地址(如果存在) 关于kubernetes_sd_configs更多信息可以查看官方文档: https://prometheus.io/docs/prometheus/latest/configuration/configuration/ 2.4 查看图形 三，容器监控 cAdvisor是一个容器资源监控工具，包括容器的内存，CPU，网络IO，资源IO等资源，同时提供了一个Web页面用于查看容器的实时运行状态。 cAvisor已经内置在了kubelet组件之中，所以我们不需要单独去安装，cAdvisor的数据路径为&#x2F;api&#x2F;v1&#x2F;nodes&#x2F;&#x2F;proxy&#x2F;metrics action 使用labelkeep或者labeldrop则可以对Target标签进行过滤，仅保留符合过滤条件的标签 12345678910111213141516- job_name: &#x27;kubernetes-cadvisor&#x27; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor 完整版配置信息如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: kube-systemdata: prometheus.yml: | global: scrape_interval: 15s scrape_timeout: 15s scrape_configs: - job_name: &#x27;prometheus&#x27; static_configs: - targets: [&#x27;localhost:9090&#x27;] - job_name: &#x27;redis&#x27; static_configs: - targets: [&#x27;redis.zhaoshuo.svc.cluster.local:9121&#x27;] - job_name: &#x27;kubernetes-node&#x27; kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__address__] regex: &#x27;(.*):10250&#x27; replacement: &#x27;$&#123;1&#125;:9100&#x27; target_label: __address__ action: replace - action: labelmap regex: __meta_kubernetes_node_label_(.+) - job_name: &#x27;kubernetes-cadvisor&#x27; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor这里稍微说一下:tls_config配置的证书地址是每个Pod连接apiserver所使用的地址，基本上写死了。并且我们在配置文件添加了一个labelmap标签。在最下面使用了一个正则替换了cAdvisor的一个metrics地址证书是我们Pod启动的时候kubelet给pod注入的一个证书，所有的pod启动的时候都会有一个ca证书注入进来如要想要访问apiserver的信息，还需要配置一个token_file 修改完成之后，我们需要更新configmap并且使用curl进行热更新(过程比较慢，需要等待会 1234kubectl delete -f prometheus.configmap.yamlkubectl create -f prometheus.configmap.yamlcurl -X POST &quot;http://10.108.79.122:9090/-/reload&quot;#curl可以多刷几次 现在我们可以到Graph路径下面查询容器的相关数据 这里演示查询集群中所有Pod的CPU使用情况，查询指标container_cpu_usage_seconds_total并且查询1分钟之内的数据 123#这里演示一下使用函数rate和不使用函数的一个过滤功能container_cpu_usage_seconds_total&#123;image!=&quot;&quot;,pod_name!=&quot;&quot;&#125;rate(container_cpu_usage_seconds_total&#123;image!=&quot;&quot;,pod_name!=&quot;&quot;&#125;[1m]) 执行下方命令，过滤1分钟内的数据 rate(container_cpu_usage_seconds_total&#123;image!=&quot;&quot;,pod_name!=&quot;&quot;&#125;[1m]) 123还可以使用sum函数,pod在1分钟内的使用率，同时将pod名称打印出来sum by (pod_name)(rate(container_cpu_usage_seconds_total&#123;image!=&quot;&quot;, pod_name!=&quot;&quot;&#125;[1m] )) 四，Api-Service 监控 apiserver作为Kubernetes最核心的组件，它的监控也是非常有必要的，对于apiserver的监控，我们可以直接通过kubernetes的service来获取 123[root@k8s-master01 prometheus]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 7d3h 上面的service是我们集群的apiserver内部的service的地址，要自动发现service类型的服务，需要使用role为Endpoints的kubernetes_sd_configs (自动发现)，我们只需要在configmap里面在添加Endpoints类型的服务发现 123- job_name: &#x27;kubernetes-apiserver&#x27; kubernetes_sd_configs: - role: endpoints 完整版configmap如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: kube-systemdata: prometheus.yml: | global: scrape_interval: 15s scrape_timeout: 15s scrape_configs: - job_name: &#x27;prometheus&#x27; static_configs: - targets: [&#x27;localhost:9090&#x27;] - job_name: &#x27;redis&#x27; static_configs: - targets: [&#x27;redis.zhaoshuo.svc.cluster.local:9121&#x27;] - job_name: &#x27;kubernetes-node&#x27; kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__address__] regex: &#x27;(.*):10250&#x27; replacement: &#x27;$&#123;1&#125;:9100&#x27; target_label: __address__ action: replace - action: labelmap regex: __meta_kubernetes_node_label_(.+) - job_name: &#x27;kubernetes-cadvisor&#x27; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor - job_name: &#x27;kubernetes-apiserver&#x27; kubernetes_sd_configs: - role: endpoints#刷新配置文件[root@k8s-master01 prometheus]# kubectl delete -f prometheus.configmap.yamlconfigmap &quot;prometheus-config&quot; deleted[root@k8s-master01 prometheus]# kubectl create -f prometheus.configmap.yamlconfigmap/prometheus-config created[root@k8s-master01 prometheus]# curl -X POST &quot;http://10.108.79.122:9090/-/reload&quot; 更新完成后，我们可以看到kubernetes-apiserver下面出现了很多实例，这是因为我们这里使用的Endpoints类型的服务发现，所以prometheus把所有的Endpoints服务都抓取过来了，同样的我们要监控的kubernetes也在列表中。 这里我们使用keep动作，将符合配置的保留下来，例如我们过滤default命名空间下服务名称为kubernetes的元数据，这里可以根据__meta_kubernetes_namespace和__mate_kubertnetes_service_name2个元数据进行relabel 12345678910111213141516 - job_name: &#x27;kubernetes-apiservers&#x27; kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https#参数解释action: keep #保留哪些标签regex: default;kubernetes;https #匹配namespace下的default命名空间下的kubernetes service 最后https协议可以通过`kubectl describe svc kubernetes`查看到 完整版配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: kube-systemdata: prometheus.yml: | global: scrape_interval: 15s scrape_timeout: 15s scrape_configs: - job_name: &#x27;prometheus&#x27; static_configs: - targets: [&#x27;localhost:9090&#x27;] - job_name: &#x27;redis&#x27; static_configs: - targets: [&#x27;redis.zhaoshuo.svc.cluster.local:9121&#x27;] - job_name: &#x27;kubernetes-node&#x27; kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__address__] regex: &#x27;(.*):10250&#x27; replacement: &#x27;$&#123;1&#125;:9100&#x27; target_label: __address__ action: replace - action: labelmap regex: __meta_kubernetes_node_label_(.+) - job_name: &#x27;kubernetes-cadvisor&#x27; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor - job_name: &#x27;kubernetes-apiserver&#x27; kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https #刷新配置[root@k8s-master01 prometheus]# kubectl delete -f prometheus.configmap.yamlconfigmap &quot;prometheus-config&quot; deleted[root@k8s-master01 prometheus]# kubectl create -f prometheus.configmap.yamlconfigmap/prometheus-config created[root@k8s-master01 prometheus]# curl -X POST &quot;http://10.108.79.122:9090/-/reload&quot; 前往Greph上查看采集到的数据 123sum(rate(apiserver_request_count[1m]))#这里使用的promql里面的rate和sun函数，意思是apiserver在1分钟内请求的数 如果我们要监控其他系统组件，比如kube-controller-manager、kube-scheduler的话就需要单独手动创建service，因为apiserver服务默认在default，而其他组件在kube-steam这个namespace下。其中kube-sheduler的指标数据端口为10251，kube-controller-manager对应端口为10252 五，Service 监控 apiserver实际上是一种特殊的Service，现在配置一个专门发现普通类型的Service 这里我们对service进行过滤，只有在service配置了prometheus.io/scrape: &quot;true&quot;过滤出来 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 - job_name: &#x27;kubernetes-service-endpoints&#x27; kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name############################################################ #Serivce自动发现参数说明 （并不是所有创建的service都可以被prometheus发现）#1.参数解释relabel_configs:-source_labels:[__meta_kubernetes_service_annotation_prometheus_io_scrape]action: keep regex: true 保留标签source_labels: [__meta_kubernetes_service_annotation_prometheus_io_cheme]这行配置代表我们只去筛选有__meta_kubernetes_service_annotation_prometheus_io_scrape的service，只有添加了这个声明才可以自动发现其他service#2.参数解释 - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2#指定一个抓取的端口，有的service可能有多个端口（比如之前的redis）。默认使用的是我们添加是使用kubernetes_service端口#3.参数解释 - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?)#这里如果是https证书类型，我们还需要在添加证书和token############################################################ 继续重复步骤，刷新配置 12345[root@k8s-master01 prometheus]# kubectl delete -f prometheus.configmap.yamlconfigmap &quot;prometheus-config&quot; deleted[root@k8s-master01 prometheus]# kubectl create -f prometheus.configmap.yamlconfigmap/prometheus-config created[root@k8s-master01 prometheus]# curl -X POST &quot;http://10.108.79.122:9090/-/reload&quot; 我们可以看到这里的服务的core DNS,为什么那么多service只有coreDNS可以被收集到呢？ 上面也说了，我们有过滤条件，只有复合条件的才进行过滤 core DNS serviceYaml 文件包含true参数，所以会被匹配到 1234567891011121314151617181920212223242526272829303132333435363738[root@k8s-master01 prometheus]# kubectl get svc -n kube-system kube-dns -o yamlapiVersion: v1kind: Servicemetadata: annotations: prometheus.io/port: &quot;9153&quot; #符合过滤条件 prometheus.io/scrape: &quot;true&quot; #符合过滤条件 creationTimestamp: &quot;2019-08-09T03:28:02Z&quot; labels: k8s-app: kube-dns kubernetes.io/cluster-service: &quot;true&quot; kubernetes.io/name: KubeDNS name: kube-dns namespace: kube-system resourceVersion: &quot;205&quot; selfLink: /api/v1/namespaces/kube-system/services/kube-dns uid: b1c27878-ba55-11e9-b09b-005056a6c65dspec: clusterIP: 10.96.0.10 ports: - name: dns port: 53 protocol: UDP targetPort: 53 - name: dns-tcp port: 53 protocol: TCP targetPort: 53 - name: metrics port: 9153 protocol: TCP targetPort: 9153 selector: k8s-app: kube-dns sessionAffinity: None type: ClusterIPstatus: loadBalancer: &#123;&#125; 案例：之前我们配置了一个Redis的一个exporter，我们通过redis进行暴露监控 我们在之前的Redis上添加prometheus.io&#x2F;scrape&#x3D;true 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#修改后如下：[root@k8s-master01 prometheus]# vim prometheus-redis-exporter.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: redis namespace: zhaoshuo annotations: prometheus.io/scrape: &quot;true&quot; prometheus.io/port: &quot;9121&quot;spec: template: metadata: labels: app: redis spec: containers: - name: redis image: redis:4 resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 - name: redis-exporter image: oliver006/redis_exporter:latest resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 9121---kind: ServiceapiVersion: v1metadata: name: redis namespace: zhaoshuo annotations: prometheus.io/scrape: &quot;true&quot; prometheus.io/port: &quot;9121&quot;spec: selector: app: redis ports: - name: redis port: 6379 targetPort: 6379 - name: prom port: 9121 targetPort: 9121 刷新Redis的Service配置 12345[root@k8s-master01 prometheus]# kubectl apply -f prometheus-redis-exporter.yamlWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl applydeployment.extensions/redis configuredWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl applyservice/redis configured 六，kube-state-metrics kube-state-metrics是一个简单的服务，它监听Kubernetes API服务器并生成相关指标数据，它不单关注单个Kubernetes组件的运行情况，而是关注内部各种对象的运行状况 在K8s集群上Pod、DaemonSet、Deployment、Job、CronJob等各种资源对象的状态也需要监控，这些指标主要来自于apiserver和kubelet中集成的cAvisor，但是并没有具体的各种资源对象的状态指标。对于Prometheus来说，当然是需要引入新的exporter来暴露这些指标，Kubernetes提供了一个kube-state-metrics kube-state-metrics已经给出了在Kubernetes部署的文件，我们直接将代码Clone到集群中执行yaml文件即可 将kube-state-metrics部署在kubernetes上之后，会发现kubernetes集群中的prometheus会在kube-state-metrics这个job下自动发现kube-state-metrics，并开始拉去metrics，这是因为部署kube-state-metrics的manifest定义文件kube-state-metrics-server.yaml对Service的定义包含prometheus.io&#x2F;scrape: ‘true’这样的一个annotation。因此kube-state-metrics的endpoint可以被Prometheus自动发现 关于kube-state-metrics暴露所有监控指标可以参考kube-state-metrics的文档https://github.com/kubernetes/kube-state-metrics/tree/master/docs 123456789101112131415161718192021#克隆kube-state-metrics代码[root@k8s-master01 prometheus]# git clone https://github.com/kubernetes/kube-state-metrics.git正克隆到 &#x27;kube-state-metrics&#x27;...remote: Enumerating objects: 19, done.remote: Counting objects: 100% (19/19), done.remote: Compressing objects: 100% (16/16), done.remote: Total 15136 (delta 4), reused 6 (delta 1), pack-reused 15117接收对象中: 100% (15136/15136), 13.09 MiB | 69.00 KiB/s, done.处理 delta 中: 100% (9374/9374), done.#部署kube-state-metrics[root@k8s-master01 prometheus]# cd kube-state-metrics/kubernetes/[root@k8s-master01 kubernetes]# lskube-state-metrics-cluster-role-binding.yaml kube-state-metrics-cluster-role.yaml kube-state-metrics-deployment.yaml kube-state-metrics-service-account.yaml kube-state-metrics-service.yaml[root@k8s-master01 kubernetes]# kubectl create -f ./clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics createdclusterrole.rbac.authorization.k8s.io/kube-state-metrics createddeployment.apps/kube-state-metrics createdserviceaccount/kube-state-metrics createdservice/kube-state-metrics created 可以通过Graph进行测试一下 1234官方文档给出了一个例子是按照Pod内存使用情况进行绘图sum(kube_pod_container_resource_requests_memory_bytes) by (namespace, pod, node) * on (pod) group_left() (sum(kube_pod_status_phase&#123;phase=&quot;Running&quot;&#125;) by (pod, namespace) == 1)","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://kkabuzs.github.io/tags/Prometheus/"}]},{"title":"Prometheus Exporter 监控 Redis","slug":"prometheusjiankongredis","date":"2019-08-15T06:08:44.000Z","updated":"2019-08-15T06:08:44.000Z","comments":true,"path":"articles/2019/08/15/prometheusjiankongredis/","permalink":"https://kkabuzs.github.io/articles/2019/08/15/prometheusjiankongredis/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Prometheus Exporter 监控 Redis一，Exporter简介 Redis应用没有自带的/metrics接口，我们就需要利用exporter服务来为prometheus提供指标数据了。Prometheus官方为许多应用提供了exporter应用 Prometheus已经成为云原生应用监控行业的标准，在很多流行的监控系统中都已经实现了Prometheus的监控接口，例如etcd、Kubernetes、CoreDNS等，他们可以直接被Prometheus监控，但是大多数监控对象都没办法直接提供监控接口，主要原因有: 很多系统在Prometheus诞生前很多年就已经发布，例如MySQL、Redis等 它们本身不支持HTTP接口，例如对于硬件性能指标，操作系统并没有原生的HTTP接口可以获取； 考虑到安全性、稳定性及代码耦合等因素的影响 在这个背景之下，exporter诞生，exporter是一个采集监控数据并通过Prometheus监控规范对外提供数据的组件。除了官方实现的exporter如Node exporter、HAProxy exporter、Mysql exporter，还有很多第三方的如Redis exporter和Rabbitmq exporter 这些exporter主要通过被监控对象提供的监控相关的接口获取监控数据，这些接口主要通过以下方式对外提供服务。 HTTP&#x2F;HTTPS方式。例如Rabbitmq exporter通过Rabbitmq的HTTPS接口获取监控数据 TCP方式。例如Redis exporter通过Redis提供的系统监控相关命令获取监控指标，MySQL server exporter 通过MySQL开放的监控相关的表获取监控指标 本地文件方式。 例如Node exporter通过读取proc文件系统下的文件，计算得出整个操作系统状态 标准协议方式。例如IPMI exporter通过IPMI协议获取硬件相关信息。这些exporter将不同规范和格式的监控指标进行转化，输出prometheus能够识别的监控数据格式，从而极大扩展prometheus采集数据的能力 二，使用exporter 监控Redis Redis应用没有自带的&#x2F;metrics接口，我们就需要利用exporter服务来为prometheus提供指标数据了。Prometheus官方为许多应用提供了exporter应用 我们这次使用Reedis exporter进行演示，对于这类应用，我们一般会以 sidecar 的形式和主应用部署在同一个 Pod 中，比如我们这里来部署一个 redis 应用，并用 redis-exporter 的方式来采集监控数据供 Prometheus 使用 2.1 创建一个Redis项目，包含2个pod123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@k8s-master01 prometheus]# vim prometheus-redis-exporter.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: redis namespace: zhaoshuospec: template: metadata: labels: app: redis spec: containers: - name: redis image: redis:4 resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 - name: redis-exporter image: oliver006/redis_exporter:latest resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 9121---kind: ServiceapiVersion: v1metadata: name: redis namespace: zhaoshuospec: selector: app: redis ports: - name: redis port: 6379 targetPort: 6379 - name: prom port: 9121 targetPort: 9121 在命名空间中，我们将命名空间指定到zhaoshuo 123456789101112131415161718#创建命名空间[root@k8s-master01 prometheus]# kubectl create namespace zhaoshuonamespace/zhaoshuo created#创建redis项目[root@k8s-master01 prometheus]# kubectl create -f prometheus-redis-exporter.yamldeployment.extensions/redis createdservice/redis created#查看redis pod是否正常[root@k8s-master01 prometheus]# kubectl get pod -n zhaoshuoNAME READY STATUS RESTARTS AGEredis-66844d86b7-vks29 2/2 Running 0 2m50s#查看svc是否正常[root@k8s-master01 prometheus]# kubectl get svc -n zhaoshuoNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEredis ClusterIP 10.107.60.195 &lt;none&gt; 6379/TCP,9121/TCP 3m22s 创建完毕之后，在redis-exporter里面已经包含了metrics，我们访问一下service的metrics接口即可 1234567891011121314151617181920212223[root@k8s-master01 prometheus]# curl 10.107.60.195:9121/metrics# HELP exporter_scrape_duration_seconds Duration of scrape by the exporter# TYPE exporter_scrape_duration_seconds summaryexporter_scrape_duration_seconds&#123;quantile=&quot;0.5&quot;&#125; NaNexporter_scrape_duration_seconds&#123;quantile=&quot;0.9&quot;&#125; NaNexporter_scrape_duration_seconds&#123;quantile=&quot;0.99&quot;&#125; NaNexporter_scrape_duration_seconds_sum 0exporter_scrape_duration_seconds_count 0# HELP go_gc_duration_seconds A summary of the GC invocation durations.# TYPE go_gc_duration_seconds summarygo_gc_duration_seconds&#123;quantile=&quot;0&quot;&#125; 0go_gc_duration_seconds&#123;quantile=&quot;0.25&quot;&#125; 0go_gc_duration_seconds&#123;quantile=&quot;0.5&quot;&#125; 0go_gc_duration_seconds&#123;quantile=&quot;0.75&quot;&#125; 0go_gc_duration_seconds&#123;quantile=&quot;1&quot;&#125; 0···省略若干···# TYPE redis_up gaugeredis_up 1# HELP redis_uptime_in_seconds uptime_in_seconds metric# TYPE redis_uptime_in_seconds gaugeredis_uptime_in_seconds 432 2.2 Redis创建完毕，还需要修改prometheus配置文件(ConfigMap)123456789101112131415161718192021222324252627282930#添加如下 - job_name: &#x27;redis&#x27; static_configs: - targets: [&#x27;redis.zhaoshuo.svc.cluster.local:9121&#x27;]#job_name 是显示在prometheus监控项的名称#redis 为service名称#zhaoshuo为命名空间（如果和prometheus在一个命名空间下可以不写）#svc.cluster.local固定格式#9121为service端口#完整版如下apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: kube-systemdata: prometheus.yml: | global: scrape_interval: 15s scrape_timeout: 15s scrape_configs: - job_name: &#x27;prometheus&#x27; static_configs: - targets: [&#x27;localhost:9090&#x27;] - job_name: &#x27;redis&#x27; static_configs: - targets: [&#x27;redis.zhaoshuo.svc.cluster.local:9121&#x27;] 更新configmap文件 1234567891011[root@k8s-master01 prometheus]# kubectl delete -f prometheus.configmap.yamlconfigmap &quot;prometheus-config&quot; deleted[root@k8s-master01 prometheus]# kubectl create -f prometheus.configmap.yamlconfigmap/prometheus-config created#查看service是否正常[root@k8s-master01 prometheus]# kubectl get svc -n kube-system |grep prometheusprometheus NodePort 10.108.79.122 &lt;none&gt; 9090:31062/TCP 3h52m#我们需要通过curl命令，对prometheus进行热更新[root@k8s-master01 prometheus]# curl -X POST &quot;http://10.108.79.122:9090/-/reload&quot; 2.3 更新完毕后，在prometheus Web Ui界面就可以看到展示结果","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://kkabuzs.github.io/tags/Prometheus/"}]},{"title":"Prometheus 之持久化安装","slug":"prometheuszhichijiuhuaanzhuang","date":"2019-08-14T07:04:08.000Z","updated":"2019-08-14T07:04:08.000Z","comments":true,"path":"articles/2019/08/14/prometheuszhichijiuhuaanzhuang/","permalink":"https://kkabuzs.github.io/articles/2019/08/14/prometheuszhichijiuhuaanzhuang/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Prometheus 之持久化安装一，简介 在早期的版本中 Kubernetes 提供了 heapster、influxDB、grafana 的组合来监控系统，所以我们可以在 Dashboard 中看到 heapster 提供的一些图表信息，在后续的版本中会陆续移除掉 heapster，现在更加流行的监控工具是 prometheus，prometheus 是 Google 内部监控报警系统的开源版本，是 Google SRE 思想在其内部不断完善的产物，它的存在是为了更快和高效的发现问题，快速的接入速度，简单灵活的配置都很好的解决了这一切，而且是已经毕业的 CNCF 项目。 二，集群部署Promethues prometheus的方式有很多，为了兼容k8s环境，我们将prometheus搭建在k8s里，除了使用docker镜像的方式安装，还可以使用二进制的方式进行安装，支持mac、Linux、windows 2.1 创建Prometheus ConfigMap1234567891011121314151617181920212223242526272829[root@k8s-master01 ~]# mkdir prometheus &amp;&amp; cd prometheus[root@k8s-master01 promethues]# cat &gt;&gt; prometheus.configmap.yaml &lt;&lt;EOFapiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: kube-systemdata: prometheus.yml: | global: scrape_interval: 15s scrape_timeout: 15s scrape_configs: - job_name: &#x27;prometheus&#x27; static_configs: - targets: [&#x27;localhost:9090&#x27;]EOF# 配置文件解释（这里的configmap实际上就是prometheus的配置）上面包含了3个模块global、rule_files和scrape_configs其中global模块控制Prometheus Server的全局配置scrape_interval:表示prometheus抓取指标数据的频率，默认是15s，我们可以覆盖这个值evaluation_interval:用来控制评估规则的频率，prometheus使用规则产生新的时间序列数据或者产生警报rule_files模块制定了规则所在的位置，prometheus可以根据这个配置加载规则，用于生产新的时间序列数据或者报警信息，当前我们没有配置任何规则，后期会添加scrape_configs用于控制prometheus监控哪些资源。由于prometheus通过http的方式来暴露它本身的监控数据，prometheus也能够监控本身的健康情况。在默认的配置有一个单独的job，叫做prometheus，它采集prometheus服务本身的时间序列数据。这个job包含了一个单独的、静态配置的目标；监听localhost上的9090端口。prometheus默认会通过目标的/metrics路径采集metrics。所以，默认的job通过URL：http://localhost:9090/metrics采集metrics。收集到时间序列包含prometheus服务本身的状态和性能。如果我们还有其他的资源需要监控，可以直接配置在该模块下即可 创建该资源对象： 12[root@k8s-master01 promethues]# kubectl create -f prometheus.configmap.yamlconfigmap/prometheus-config created 配置文件创建完成，如果以后我们有新的资源需要被监控，我们只需要将ConfigMap对象更新即可 2.2 创建rbac认证 这里还需要创建rbac认证，因为prometheus需要访问k8s集群内部的资源 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@k8s-master01 prometheus]# vim prometheus-rbac.yamlapiVersion: v1kind: ServiceAccountmetadata: name: prometheus namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: prometheusrules:- apiGroups: - &quot;&quot; resources: - nodes - services - endpoints - pods - nodes/proxy verbs: - get - list - watch- apiGroups: - &quot;&quot; resources: - configmaps - nodes/metrics verbs: - get- nonResourceURLs: - /metrics verbs: - get---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: prometheusroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheussubjects:- kind: ServiceAccount name: prometheus namespace: kube-system 由于我们要获取的资源，在每一个namespace下面都有可能存在，所以我们这里使用的是ClusterRole的资源对象，nonResourceURLs是用来对非资源型metrics进行操作的权限声明 创建rbac文件 1234[root@k8s-master01 prometheus]# kubectl create -f prometheus-rbac.yamlserviceaccount/prometheus createdclusterrole.rbac.authorization.k8s.io/prometheus createdclusterrolebinding.rbac.authorization.k8s.io/prometheus created 2.3 创建pv pvc123456789101112131415161718192021222324252627282930313233343536#由于我之前已经准备好了pv，故此直接创建pvc即可。[root@k8s-master01 prometheus]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEnfs-pv01 1Gi RWX Retain Bound default/www-web-0 44hnfs-pv02 2Gi RWX Retain Bound default/www-web-1 44hnfs-pv03 3Gi RWO Retain Available 44hnfs-pv04 4Gi RWX Retain Bound default/www-web-2 44hnfs-pv05 5Gi RWX Retain Available 44h##创建pvc[root@k8s-master01 prometheus]# vim prometheus-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: prometheus namespace: kube-systemspec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi[root@k8s-master01 prometheus]# kubectl create -f prometheus-pvc.yaml #创建pvcpersistentvolumeclaim/prometheus created[root@k8s-master01 prometheus]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEnfs-pv01 1Gi RWX Retain Bound default/www-web-0 44hnfs-pv02 2Gi RWX Retain Bound default/www-web-1 44hnfs-pv03 3Gi RWO Retain Available 44hnfs-pv04 4Gi RWX Retain Bound default/www-web-2 44hnfs-pv05 5Gi RWX Retain Bound kube-system/prometheus 44h[root@k8s-master01 prometheus]# kubectl get pvc -n kube-systemNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEprometheus Bound nfs-pv05 5Gi RWX 29s 2.4 创建Prometheus Pod 资源12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@k8s-master01 prometheus]# vim prometheus.deploy.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: prometheus namespace: kube-system labels: app: prometheusspec: template: metadata: labels: app: prometheus spec: serviceAccountName: prometheus containers: - image: prom/prometheus:v2.4.3 name: prometheus command: - &quot;/bin/prometheus&quot; args: - &quot;--config.file=/etc/prometheus/prometheus.yml&quot; - &quot;--storage.tsdb.path=/prometheus&quot; - &quot;--storage.tsdb.retention=30d&quot; - &quot;--web.enable-admin-api&quot; # 控制对admin HTTP API的访问，其中包括删除时间序列等功能 - &quot;--web.enable-lifecycle&quot; # 支持热更新，直接执行localhost:9090/-/reload立即生效 ports: - containerPort: 9090 protocol: TCP name: http volumeMounts: - mountPath: &quot;/prometheus&quot; subPath: prometheus name: data - mountPath: &quot;/etc/prometheus&quot; name: config-volume resources: requests: cpu: 100m memory: 512Mi limits: cpu: 100m memory: 512Mi securityContext: runAsUser: 0 volumes: - name: data persistentVolumeClaim: claimName: prometheus - configMap: name: prometheus-config name: config-volume 创建prometheus.deploy.yaml，运行prometheus服务 1234[root@k8s-master01 prometheus]# kubectl create -f prometheus.deploy.yamldeployment.extensions/prometheus created[root@k8s-master01 prometheus]# kubectl get pod -n kube-system |grep prometheusprometheus-59895659-wnxw7 1/1 Running 0 18s 2.5 为 Prometheus 创建 Service 现在我们prometheus服务状态是已经正常了，但是我们在浏览器是无法访问prometheus的 webui服务。那么我们还需要创建一个service 12345678910111213141516[root@k8s-master01 prometheus]# vim prometeheus-svc.yamlapiVersion: v1kind: Servicemetadata: name: prometheus namespace: kube-system labels: app: prometheusspec: selector: app: prometheus type: NodePort ports: - name: web port: 9090 targetPort: http 为了方便测试，我这里使用的是NodePort，我们也可以创建一个Ingress对象使用域名访问 12345#创建service[root@k8s-master01 prometheus]# kubectl create -f prometeheus-svc.yamlservice/prometheus created[root@k8s-master01 prometheus]# kubectl get svc -n kube-system |grep prometheusprometheus NodePort 10.108.79.122 &lt;none&gt; 9090:31062/TCP 5s 2.6 Web访问Prometheus这里定义的端口为31062,我们直接在浏览器上任意节点输入node ip+端口即可 查看一下当前监控规则 默认prometheus会监控自己Status--&gt;Targets 查看一下数据，是否收集到数据 比如我们这里就选择scrape_duration_seconds这个指标，然后点击Execute，如果这个时候没有查询到任何数据，我们可以切换到Graph这个 tab 下面重新选择下时间，选择到当前的时间点，重新执行，就可以看到类似于下面的图表数据了： 一定要设置好时间，否则不出图","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://kkabuzs.github.io/tags/Prometheus/"}]},{"title":"Prometheus 之原理介绍","slug":"prometheuszhiyuanlijieshao","date":"2019-08-13T09:27:55.000Z","updated":"2019-08-13T09:27:55.000Z","comments":true,"path":"articles/2019/08/13/prometheuszhiyuanlijieshao/","permalink":"https://kkabuzs.github.io/articles/2019/08/13/prometheuszhiyuanlijieshao/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Prometheus 之原理介绍一，Prometheus简介Prometheus由Go语言编写而成，采用Pull方式获取监控信息，并提供了多维度的数据模型和灵活的查询接口。Prometheus不仅可以通过静态文件配置监控对象，还支持自动发现机制，能通过Kubernetes、Consl、DNS等多种方式动态获取监控对象。在数据采集方面，借助Go语音的高并发特性，单机Prometheus可以采取数百个节点的监控数据；在数据存储方面，随着本地时序数据库的不断优化，单机Prometheus每秒可以采集一千万个指标，如果需要存储大量的历史监控数据，则还支持远程存储。 Prometheus是由SoundCloud开发的开源监控系统的开源版本。2016年，由Google发起的Linux基金会(Cloud Native Computing Foundation,CNCF)将Prometheus纳入其第二大开源项目。Prometheus在开源社区也十分活跃 1.1 Prometheus优缺点 提供多维度数据模型和灵活的查询方式，通过将监控指标关联多个tag，来将监控数据进行任意维度的组合，并且提供简单的PromQL查询方式，还提供HTTP查询接口，可以很方便地结合Grafana等GUI组件展示数据 在不依赖外部存储的情况下，支持服务器节点的本地存储，通过Prometheus自带的时序数据库，可以完成每秒千万级的数据存储；不仅如此，在保存大量历史数据的场景中，Prometheus可以对接第三方时序数据库和OpenTSDB等。 定义了开放指标数据标准，以基于HTTP的Pull方式采集时序数据，只有实现了Prometheus监控数据才可以被Prometheus采集、汇总、并支持Push方式向中间网关推送时序列数据，能更加灵活地应对多种监控场景 支持通过静态文件配置和动态发现机制发现监控对象，自动完成数据采集。Prometheus目前已经支持Kubernetes、etcd、Consul等多种服务发现机制 易于维护，可以通过二进制文件直接启动，并且提供了容器化部署镜像。 支持数据的分区采样和联邦部署，支持大规模集群监控 1.2 Prometheus 架构 Prometheus的基本原理是通过HTTP周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口并符合Prometheus定义的数据格式，就可以介入Prometheus监控 Prometheus Server负载定时在目标上抓取metrics(指标)数据，每个抓取目标都需要暴露一个HTTP服务接口用于Prometheus定时抓取。这种调用被监控对象获取监控数据的方式被称为Pull(拉)。Pull方式体现了Prometheus独特的设计哲学与大多数采用Push(推)方式的监控不同 Pull方式的优势是能够自动进行上游监控和水平监控，配置更少，更容易扩展，更灵活，更容易实现高可用。简单来说就是Pull方式可以降低耦合。由于在推送系统中很容易出现因为向监控系统推送数据失败而导致被监控系统瘫痪的问题。所以通过Pull方式，被采集端无需感知监控系统的存在，完全独立于监控系统之外，这样数据的采集完全由监控系统控制 1.3 Prometheus支持两种Pull方式采集数据 通过配置文件、文本等进行静态配置 支持Zookeeper、Consul、Kubernetes等方式进行动态发现，例如对Kuernetes的动态发现，Prometheus使用Kubernetes的API查询和监控容器信息的变化，动态更新监控对象，这样容器的创建和删除都可以被Prometheus感知 Storage通过一定的规则清理和整理数据，并把得到的结果从年初到新的时间序列中，这里存储的方式有两种 本地存储。通过Prometheus自带的时序数据库将数据库数据保存在本地磁盘。但是本地存储的容量毕竟有限，建议不要保存超过一个月的数据 另一种是远程存储，适用于存储大量监控数据。通过中间层的适配器的转发，目前Prometheus支持OpenTsdb、InfluxDB、Elasticsearch等后端存储，通过适配器实现Prometheus存储的remote write和remote read接口，便可以接入Prometheus作为远程存储使用。 Prometheus通过PromQL和其他API可视化地展示收集的数据。Prometheus支持多种方式的图标可视化，例如Grafana、自带的PromDash及自身提供的模板引擎等。Prometheus还提供HTTP API查询方法，自定义所需要的输出 Prometheus通过Pull方式拉取数据，但某些现有系统是通过Push方式实现的，为了接入这些系统，Prometheus提供了对PushGateway的支持，这些系统主动推送metrics到PushGateway，而Prometheus只是定时去Gateway上抓取数据 AlertManager是独立于Prometheus的一个组件，在出发了预先设置在Prometheus中的高级规则后，Prometheus便会推送告警信息到AlertManager。AlertManager提供了十分灵活的告警方式，可以通过邮件、slack或者钉钉等途径推送。并且AlertManager支持高可用部署，为了解决多个AlertManager重复告警的问题，引用了Gossip，在多个AlertManager直接通过Gossip同步告警信息 1.4 Promethues 特征Prometheus 相比于其他传统监控工具主要由以下几个特点： 具有由metric名称和键值对标示的时间序列数据的多位数据模型 有一个灵活的查询语言promQL 不依赖分布式存储，只和本地磁盘有关 通过HTTP的服务拉取时间序列数据 也支持推送的方式来添加时间序列数据 支持通过服务发现和静态配置发现目标 多种图形和仪表盘支持 1.5 Prometheus 组件 Prometheus由多个组件组成，但是其中许多组件是可选的 Prometheus Server 用于抓取指标、存储时间序列数据 exporter 暴露指标让任务抓取 Pushgateway push的方式将指标数据推送到网关 alertmanager 处理报警的报警组件 adhoc 用于数据查询 大多数Prometheus组件都是使用go编写的，因此很容易构建和部署静态的二进制文件","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://kkabuzs.github.io/tags/Prometheus/"}]},{"title":"Kubernetes之StatefulSet（有状态）","slug":"kuberneteszhiStatefulSet","date":"2019-08-12T08:04:27.000Z","updated":"2019-08-12T08:04:27.000Z","comments":true,"path":"articles/2019/08/12/kuberneteszhiStatefulSet/","permalink":"https://kkabuzs.github.io/articles/2019/08/12/kuberneteszhiStatefulSet/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Kubernetes之StatefulSet（有状态）一，StatefulSet概述 statefulset是kubernetes提供的管理有状态应用的负载管理控制器API在pod管理的基础上保证pod的顺序和一致性与部署一样statefulset也是使用容器的spec来创建pod与之不同的是statefulset创建的pod在生命周期中会保持持久的标记比如pod name 1.1 前提 Kubernetes集群的版本 &gt;&#x3D;1.5 安装好DNS集群插件，版本 &gt;&#x3D;15 1.2 特点 StatefulSet(1.5版本之前叫做PetSet)为什么适合有状态的程序，因为它相比于Deployment有以下特点： 稳定的，唯一的网络标识，可以用来发现集群内部的其他成员。比如StatefulSet的名字叫kafka,那么第一个起来的Pet叫kafka-0,第二个叫 kafk-1,依次类推。 稳定的持久化存储：通过Kubernetes的PV&#x2F;PVC或者外部存储(预先提供的)来实现 启动或关闭时保证有序：优雅的部署和伸缩性: 操作第n个pod时，前n-1个pod已经是运行且准备好的状态。 有序的，优雅的删除和终止操作:从 n, n-1, … 1, 0 这样的顺序删除 上述提到的“稳定”指的是Pod在多次重新调度时保持稳定，即存储，DNS名称，hostname都是跟Pod绑定到一起的，跟Pod被调度到哪个节点没关系。所以Zookeeper, Etcd 或 Elasticsearch这类需要稳定的集群成员的应用时，就可以用StatefulSet。通过查询无头服务域名的A记录，就可以得到集群内成员的域名信息。 1.3 限制 StatefulSet也有一些限制： Pod的存储必须是通过 PersistentVolume Provisioner基于 storeage类来提供，或者是管理员预先提供的外部存储 删除或者缩容不会删除跟StatefulSet相关的卷，这是为了保证数据的安全 StatefulSet现在需要一个无头服务(Headless Service)来负责生成Pods的唯一网络标示，需要开发人员创建这个服务 对StatefulSet的升级是一个手工的过程 1.4 无头服务 要定义一个服务(Service)为无头服务(Headless Service)，需要把Service定义中的ClusterIP配置项设置为空: spec.clusterIP:None。和普通Service相比，Headless Service没有ClusterIP(所以没有负载均衡),它会给一个集群内部的每个成员提供一个唯一的DNS域名来作为每个成员的网络标识，集群内部成员之间使用域名通信。无头服务管理的域名是如下的格式：$(service_name).$(k8s_namespace).svc.cluster.local。其中的 “cluster.local”是集群的域名,除非做了配置，否则集群域名默认就是cluster.local。StatefulSet下创建的每个Pod，得到一个对应的DNS子域名，格式如下：$(podname).$(governing_service_domain),这里 governing_service_domain是由StatefulSet中定义的serviceName来决定。举例子，无头服务管理的kafka的域名是：kafka.test.svc.cluster.local, 创建的Pod得到的子域名是 kafka-1.kafka.test.svc.cluster.local。注意这里提到的域名，都是由kuber-dns组件管理的集群内部使用的域名，可以通过命令来查询： 123456[root@gago-dev-k8s-master ~]# kubectl exec -it aws-dev-adminx-v4-5c5d8f4dfc-67j5v -n api nslookup alarmServer: 10.96.0.10Address: 10.96.0.10#53Name: alarm.api.svc.cluster.localAddress: 10.105.135.71 而普通Service情况下，Pod名字后面是随机数，需要通过Service来做负载均衡。 当一个StatefulSet挂掉，新创建的StatefulSet会被赋予跟原来的Pod一样的名字，通过这个名字来匹配到原来的存储，实现了状态保存。因为上文提到了，每个Pod的标识附着在Pod上，无论pod被重新调度到了哪里。 二，创建简单的statefuleset控制器pod12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@k8s-master01 ~]# vim nginx-statsfulset.yamlapiVersion: v1kind: Service #需要创建一个无头服务关联pod 然后statefulset通过这个管理它的后端podmetadata: name: nginx labels: app: nginxspec: ports: - port: 80 name: web clusterIP: None selector: app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata: name: webspec: selector: matchLabels: app: nginx #staefulset的标签选择器必须基于模板定义的 serviceName: &quot;nginx&quot; replicas: 3 # by default is 1 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: nginx ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html #将下边的存储挂载到容器的哪里 volumeClaimTemplates: #创建挂载pv的pvc模板 - metadata: name: www spec: accessModes: [ &quot;ReadWriteMany&quot; ] #访问方式 resources: requests: storage: 1Gi #需要大小[root@k8s-master01 ~]# kubectl create -f nginx-statsfulset.yaml#####################################################################yaml语法检查方法[root@k8s-master01 configmap]# kubectl create -f pod-config.yaml --dry-run #--dry-run 检查yaml文件是否有错误。pod/pod-test created (dry run) #代表无错误#################################################################### 2.1 查看创建的pod123456789101112131415161718192021222324252627[root@k8s-master01 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEpod-test 1/1 Running 0 24hweb-0 1/1 Running 0 69sweb-1 1/1 Running 0 58sweb-2 1/1 Running 0 33s#查看pv绑定情况[root@k8s-master01 ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEnfs-pv01 1Gi RWX Retain Bound default/www-web-0 78mnfs-pv02 2Gi RWX Retain Bound default/www-web-1 78mnfs-pv03 3Gi RWO Retain Available 78mnfs-pv04 4Gi RWX Retain Bound default/www-web-2 78mnfs-pv05 5Gi RWX Retain Available#查看pvc的创建[root@k8s-master01 ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEwww-web-0 Bound nfs-pv01 1Gi RWX 4m55swww-web-1 Bound nfs-pv02 2Gi RWX 4m44swww-web-2 Bound nfs-pv04 4Gi RWX 4m19s#查看statefulset的创建[root@k8s-master01 ~]# kubectl get stsNAME READY AGEweb 3/3 5m14s 它也可以通过scale进行扩容缩容亦可以通过set-image实现版本更换它进行创建的时候是顺序创建先创建第一个再创建第二个依次类推那么如果是删除镜像或者版本迭代它是逆序的先进行最后一个然后倒数第二个依次类推的进行","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kkabuzs.github.io/tags/Kubernetes/"}]},{"title":"Kubernetes之ConfigMap","slug":"kuberneteszhiconfigmap","date":"2019-08-01T02:14:21.000Z","updated":"2019-08-01T02:14:21.000Z","comments":true,"path":"articles/2019/08/01/kuberneteszhiconfigmap/","permalink":"https://kkabuzs.github.io/articles/2019/08/01/kuberneteszhiconfigmap/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Kubernetes之ConfigMap一，目的 把应用的代码和配置分开，通过配置configmap管理pod，一种统一的集群配置管理方案。ConcigMap API资源提供了将配置数据注入容器的方式，同时保持容器是不知道Kubernetes的。ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制等对象。 二，基本原理 ConfigMap是存储通用的配置变量的.ConfigMap有点儿像一个统一的配置文件，使用户可以将分布式系统中用于不同模块的环境变量统一到一个对象中管理;而它与配置文件的区别在于它是存在集群的“环境”中的，并且支持K8S集群中所有通用的操作调用方式。 而资源的使用者可以通过ConfigMap来存储这个资源的配置，这样需要访问这个资源的应用就可以同通过ConfigMap来引用这个资源。相当通过创建Configmap封装资源配置。 configmap以一个或者多个键：值的形式保存在K8S系统中供应用使用，既可以用于表示一个变量的值（eg.apploglevel：信息），也可以用于表示一个完整配置文件的内容（例如， ：server.xml &#x3D; &lt;？xml …&gt; …） 可以通过yaml配置文件或者直接用kubectl create configmap命令行的方式来创建ConfigMap。 简单来说： configmap的作用就是让配置信息与应用程序镜像解耦配置信息可以通过configmap注入到应用程序容器中，在configmap中所有的数据以键值的方式保存 三，ConfigMap3.1 创建configmap12345678910111213141516171819202122232425262728293031323334#直接给定键值的方式创建 还可以指定文件作为键然后里边的内容作为值[root@k8s-master01 configmap]# vim configmap.yamlapiVersion: v1kind: ConfigMapmetadata: labels: config: map1 name: configmap1 namespace: defaultdata: nginx_port: &quot;8080&quot; server_name: www.zhaoshuo.com [root@k8s-master01 configmap]# kubectl create -f configmap.yamlconfigmap/configmap1 created[root@k8s-master01 configmap]# kubectl get cmNAME DATA AGEconfigmap1 2 9s[root@k8s-master01 configmap]# kubectl describe cm configmap1Name: configmap1Namespace: defaultLabels: config=map1Annotations: &lt;none&gt;Data====nginx_port:----8080server_name:----www.zhaoshuo.comEvents: &lt;none&gt;#那么这个直接赋予键值的configmap就创建完了 3.2 创建以文件为键值的configmap12345678910111213141516171819202122232425262728293031323334先创建一个要引用的文件 比如nginx配置文件[root@k8s-master01 configmap]# vim www.confserver &#123; server_name www.zhaoshuo.com; listen 90; root html/zhao/ ; root index.html;&#125;#在创建以文件为键值的configmap时 如果我们不指定键名称那么它默认以文件名作为键 文件内容作为值[root@k8s-master01 configmap]# kubectl create configmap config-file --from-file=./www.confconfigmap/config-file created[root@k8s-master01 configmap]# kubectl get cmNAME DATA AGEconfig-file 1 110sconfigmap1 2 22m[root@k8s-master01 configmap]# kubectl describe cm config-fileName: config-fileNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Data====www.conf:----server &#123; server_name www.zhaoshuo.com; listen 90; root html/zhao/ ; root index.html;&#125;Events: &lt;none&gt; 3.3 为pod传递环境变量引用configmap 当使用环境变量的方式将配置注入镜像时只在镜像启动时获取那么中途配置发生改变镜像内的环境变量不会根据configmap的改变而发生改变 123456789101112131415161718192021222324252627282930[root@k8s-master01 configmap]#[root@k8s-master01 configmap]# vim pod-config.yamlapiVersion: v1kind: Podmetadata: labels: zhao: shuo name: pod-test namespace: defaultspec: containers: - image: nginx name: pod-nginx ports: - name: port containerPort: 80 env: #使用env的方式将configmap中的key注入到镜像 - name: nginx_server_port #此处注入了两个key valueFrom: configMapKeyRef: name: configmap1 #这个名称为你创建的configmap的名字 key: nginx_port #这个就是定义的key - name: nginx_server_name valueFrom: configMapKeyRef: name: configmap1 key: server_name[root@k8s-master01 configmap]# kubectl create -f pod-config.yamlpod/pod-test created 编辑configmap中的密钥发生改变查看pod中的环境变量是否改变 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#此前我们创建的configmap中的key与值[root@k8s-master01 configmap]# kubectl describe cm configmap1Name: configmap1Namespace: defaultLabels: config=map1Annotations: &lt;none&gt;Data====nginx_port:----8080server_name:----www.zhaoshuo.comEvents: &lt;none&gt;#先查看我们已经注入的configmap中的key[root@k8s-master01 configmap]# kubectl exec -it pod-test -- /bin/sh# printenvKUBERNETES_SERVICE_PORT=443KUBERNETES_PORT=tcp://10.96.0.1:443HOSTNAME=pod-testHOME=/rootPKG_RELEASE=1~busterTERM=xtermKUBERNETES_PORT_443_TCP_ADDR=10.96.0.1NGINX_VERSION=1.17.2PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binKUBERNETES_PORT_443_TCP_PORT=443NJS_VERSION=0.3.3KUBERNETES_PORT_443_TCP_PROTO=tcpnginx_server_port=8080 #这是我们configmap中以环境变量的方式注入的keynginx_server_name=www.zhaoshuo.com #这是我们configmap中以环境变量的方式注入的keyKUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443KUBERNETES_SERVICE_PORT_HTTPS=443KUBERNETES_SERVICE_HOST=10.96.0.1PWD=/#编辑configmap 然后查看pod中的环境变量是否也会发生改变[root@k8s-master01 configmap]#[root@k8s-master01 configmap]# vim configmap.yamlapiVersion: v1kind: ConfigMapmetadata: labels: config: map1 name: configmap1 namespace: defaultdata: nginx_port: &quot;9090&quot; server_name: www.zhaoshuo.com[root@k8s-master01 configmap]# kubectl apply -f configmap.yamlWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply #警告可以忽略configmap/configmap1 configured#查看configmap1中的端口改为9090 接下来查看pod是否发生变化[root@k8s-master01 configmap]# kubectl describe cm configmap1Name: configmap1Namespace: defaultLabels: config=map1Annotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;nginx_port&quot;:&quot;9090&quot;,&quot;server_name&quot;:&quot;www.zhaoshuo.com&quot;&#125;,&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;labels&quot;:...Data====nginx_port:----9090server_name:----www.zhaoshuo.comEvents: &lt;none&gt;[root@k8s-master01 configmap]# kubectl exec -it pod-test -- /bin/sh# printenvKUBERNETES_SERVICE_PORT=443KUBERNETES_PORT=tcp://10.96.0.1:443HOSTNAME=pod-testHOME=/rootPKG_RELEASE=1~busterTERM=xtermKUBERNETES_PORT_443_TCP_ADDR=10.96.0.1NGINX_VERSION=1.17.2PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binKUBERNETES_PORT_443_TCP_PORT=443NJS_VERSION=0.3.3KUBERNETES_PORT_443_TCP_PROTO=tcpnginx_server_port=8080 #没有发生变化nginx_server_name=www.zhaoshuo.comKUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443KUBERNETES_SERVICE_PORT_HTTPS=443KUBERNETES_SERVICE_HOST=10.96.0.1PWD=/ 3.4 为pod创建卷的方式注入configmap配置 使用文件作为键值，挂载的方式将配置注入pod可以实现配置中心的配置发生改变那么镜像内也发生改变 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374[root@k8s-master01 configmap]# vim pod-config.yamlapiVersion: v1kind: Podmetadata: labels: zhao: shuo name: pod-test namespace: defaultspec: containers: - image: nginx name: pod-nginx ports: - name: port containerPort: 80 volumeMounts: - name: nginxconf #下边的卷名称 mountPath: /usr/local/nginx/conf/conf.d #指定要挂载到哪里 如果没有自动创建 volumes: - name: nginxconf #创建卷名称 configMap: #类型为configMap name: config-file #将这个以文件为key的configmap信息挂载进去 [root@k8s-master01 configmap]# kubectl create -f pod-config.yamlpod/pod-test created[root@k8s-master01 configmap]# kubectl get podNAME READY STATUS RESTARTS AGEpod-test 1/1 Running 0 11s[root@k8s-master01 configmap]# kubectl exec -it pod-test -- /bin/sh# cat /usr/local/nginx/conf/conf.d/www.confserver &#123; server_name www.zhaoshuo.com; listen 90; root html/zhao/ ; root index.html;&#125;#已经挂载进来了,那我们现在改变外边configmap中的信息查看此文件是否会改变[root@k8s-master01 configmap]# kubectl edit cm config-file# Please edit the object below. Lines beginning with a &#x27;#&#x27; will be ignored,# and an empty file will abort the edit. If an error occurs while saving this file will be# reopened with the relevant failures.#apiVersion: v1data: www.conf: |+ server &#123; server_name www.zhaoshuo.com; listen 10; #将监听的端口90改为10 root html/zhao/ ; root index.html; &#125;kind: ConfigMapmetadata: creationTimestamp: &quot;2019-08-12T07:22:24Z&quot; name: config-file namespace: default resourceVersion: &quot;396933&quot; selfLink: /api/v1/namespaces/default/configmaps/config-file uid: eeab0d4c-bcd1-11e9-b09b-005056a6c65d #再次进入容器查看[root@k8s-master01 configmap]# kubectl exec -it pod-test -- /bin/sh# cat /usr/local/nginx/conf/conf.d/www.confserver &#123; server_name www.zhaoshuo.com; listen 10; #端口已经被发生改变 root html/zhao/ ; root index.html;&#125;#至此configmap完毕","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kkabuzs.github.io/tags/Kubernetes/"}]},{"title":"win 10 装机之无法安装到gpt分区形式磁盘","slug":"win10zhuangjizhiwufaanzhuanggptxingshicipan","date":"2019-07-22T06:02:34.000Z","updated":"2019-07-22T06:02:34.000Z","comments":true,"path":"articles/2019/07/22/win10zhuangjizhiwufaanzhuanggptxingshicipan/","permalink":"https://kkabuzs.github.io/articles/2019/07/22/win10zhuangjizhiwufaanzhuanggptxingshicipan/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 win 10 装机之无法安装到gpt分区形式磁盘 解决办法如下 12345678910111213141516171819在系统提示无法安装的那一步，按住“shift+f10”，呼出“cmd”命令符；输入：diskpart，回车进入diskpart。输入：list disk，回车显示磁盘信息输入：select disk 0，回车选择第0个磁盘（电脑的硬盘编号是从0开始的）输入：clean，回车删除磁盘分区&amp;格式化输入：convert mbr，回车将当前磁盘分区设置为Mbr形式输入：create partition primary size = xxx，回车创建主分区大小（MB）输入：format fs=ntfs quick，回车格式化磁盘为ntfs输入：exit，回车退出diskpart输入：exit，回车退出cmd","categories":[{"name":"工作随笔，问题排查","slug":"工作随笔，问题排查","permalink":"https://kkabuzs.github.io/categories/%E5%B7%A5%E4%BD%9C%E9%9A%8F%E7%AC%94%EF%BC%8C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"}],"tags":[]},{"title":"Hadoop集群之Zookeeper","slug":"hadoopjiqunzhizookeeper","date":"2019-07-18T01:32:02.000Z","updated":"2019-07-18T01:32:02.000Z","comments":true,"path":"articles/2019/07/18/hadoopjiqunzhizookeeper/","permalink":"https://kkabuzs.github.io/articles/2019/07/18/hadoopjiqunzhizookeeper/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Hadoop集群之Zookeeper一，Zookeeper概述 ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。 ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 二，基本概念2.1 集群角色 在zookeeper集群中，有以下三种角色： Leader（领导者）：领导者负责进行和发起投票决议，更新系统状态。 Follower（跟随者）：用于接收客户请求并向客户端返回结果，在选主过程中参与投票。 Observer（观察者）：可以接收客户端连接，将写请求转发给leader节点，但Observer不参加投票过程，只同步leader状态，Observer的目的是为了扩展系统，提高读取速度。 一个ZooKeeper集群同一时刻只会有一个Leader，其他都是Follower或Observer。 2.2 特性 最终一致性：client不论连接到哪个Server，展示给它都是同一个视图，这是zookeeper最重要的性能。 可靠性：具有简单、健壮、良好的性能，如果消息m被到一台服务器接受，那么它将被所有的服务器接受。 实时性：Zookeeper保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效的信息。但由于网络延时等原因，Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。 等待无关（wait-free）：慢的或者失效的client不得干预快速的client的请求，使得每个client都能有效的等待。 原子性：更新只能成功或者失败，没有中间状态。 顺序性：包括全局有序和偏序两种：全局有序是指如果在一台服务器上消息a在消息b前发布，则在所有Server上消息a都将在消息b前被发布；偏序是指如果一个消息b在消息a后被同一个发送者发布，a必将排在b前面。 三，工作原理 Zookeeper的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。 为了保证事务的顺序一致性，zookeeper采用了递增的事务id号（zxid）来标识事务。所有的提议（proposal）都在被提出的时候加上了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个新的epoch，标识当前属于那个leader的统治时期。低32位用于递增计数。 每个Server在工作过程中有三种状态： LOOKING： 当前Server不知道leader是谁，正在搜寻LEADING： 当前Server即为选举出来的leaderFOLLOWING： leader已经选举出来，当前Server与之同步 3.1 选主流程 当leader崩溃或者leader失去大多数的follower，这时候zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的Server都恢复到一个正确的状态。Zk的选举算法有两种：一种是基于basic paxos实现的，另外一种是基于fast paxos算法实现的。系统默认的选举算法为fast paxos。先介绍basic paxos流程： 选举线程由当前Server发起选举的线程担任，其主要功能是对投票结果进行统计，并选出推荐的Server； 选举线程首先向所有Server发起一次询问(包括自己)； 选举线程收到回复后，验证是否是自己发起的询问(验证zxid是否一致)，然后获取对方的id(myid)，并存储到当前询问对象列表中，最后获取对方提议的leader相关信息(id,zxid)，并将这些信息存储到当次选举的投票记录表中； 收到所有Server回复以后，就计算出zxid最大的那个Server，并将这个Server相关信息设置成下一次要投票的Server； 线程将当前zxid最大的Server设置为当前Server要推荐的Leader，如果此时获胜的Server获得n&#x2F;2 + 1的Server票数， 设置当前推荐的leader为获胜的Server，将根据获胜的Server相关信息设置自己的状态，否则，继续这个过程，直到leader被选举出来。 通过流程分析我们可以得出：要使Leader获得多数Server的支持，则Server总数必须是奇数2n+1，且存活的Server的数目不得少于n+1. 每个Server启动后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的server还会从磁盘快照中恢复数据和会话信息，zk会记录事务日志并定期进行快照，方便在恢复时进行状态恢复。选主的具体流程图如下所示： fast paxos流程是在选举过程中，某Server首先向所有Server提议自己要成为leader，当其它Server收到提议以后，解决epoch和zxid的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息，重复这个流程，最后一定能选举出Leader。其流程图如下所示： 3.2 同步流程 选完leader以后，zk就进入状态同步过程。 leader等待server连接； Follower连接leader，将最大的zxid发送给leader； Leader根据follower的zxid确定同步点； 完成同步后通知follower 已经成为uptodate状态； Follower收到uptodate消息后，又可以重新接受client的请求进行服务了。 流程图如下所示： 3.3 工作流程3.3.1 Leader工作流程 Leader主要有三个功能： 恢复数据； 维持与Learner的心跳，接收Learner请求并判断Learner的请求消息类型； Learner的消息类型主要有PING消息、REQUEST消息、ACK消息、REVALIDATE消息，根据不同的消息类型，进行不同的处理。 PING消息是指Learner的心跳信息；REQUEST消息是Follower发送的提议信息，包括写请求及同步请求；ACK消息是Follower的对提议的回复，超过半数的Follower通过，则commit该提议；REVALIDATE消息是用来延长SESSION有效时间。 Leader的工作流程简图如下所示，在实际实现中，流程要比下图复杂得多，启动了三个线程来实现功能。 3.3.2 Follower工作流程 Follower主要有四个功能： 向Leader发送请求（PING消息、REQUEST消息、ACK消息、REVALIDATE消息）； 接收Leader消息并进行处理； 接收Client的请求，如果为写请求，发送给Leader进行投票； 返回Client结果。 Follower的消息循环处理如下几种来自Leader的消息： PING消息： 心跳消息； PROPOSAL消息：Leader发起的提案，要求Follower投票； COMMIT消息：服务器端最新一次提案的信息； UPTODATE消息：表明同步完成； REVALIDATE消息：根据Leader的REVALIDATE结果，关闭待revalidate的session还是允许其接受消息； SYNC消息：返回SYNC结果到客户端，这个消息最初由客户端发起，用来强制得到最新的更新。 Follower的工作流程简图如下所示，在实际实现中，Follower是通过5个线程来实现功能的。 说明：对于observer的流程不再叙述，observer流程和Follower的唯一不同的地方就是observer不会参加leader发起的投票。 三，zookeeper集群部署 下载地址：https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/ 主机名 ip地址 用户名 运行组件 zhaohadoop1 192.168.101.150 hadoop DataNode、NodeManager zhaohadoop2 192.168.101.154 hadoop DataNode、NodeManager、SecondaryNameNode、zookeeper zhaohadoop3 192.168.101.156 hadoop NameNode、ResourceManager、zookeeper zhaohadoop4 192.168.101.157 hadoop DataNode、NodeManager、zookeeper 3.1 下载zookeeper源码包123456789101112131415[root@zhaohadoop2 ~]# wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz#前提先安装好jdk[root@zhaohadoop2 ~]# tar xf zookeeper-3.4.14.tar.gz -C /usr/local/[root@zhaohadoop2 ~]# mv /usr/local/zookeeper-3.4.14 /usr/local/zookeeper[root@zhaohadoop2 ~]# cd /usr/local/zookeeper/[root@zhaohadoop2 zookeeper]# lsbin dist-maven lib pom.xml src zookeeper-3.4.14.jar.md5 zookeeper-contrib zookeeper-jutebuild.xml ivysettings.xml LICENSE.txt README.md zookeeper-3.4.14.jar zookeeper-3.4.14.jar.sha1 zookeeper-docs zookeeper-recipesconf ivy.xml NOTICE.txt README_packaging.txt zookeeper-3.4.14.jar.asc zookeeper-client zookeeper-it zookeeper-server#在每个zookeeper目录下创建一个data目录。在data目录下创建一个myid文件，文件名就叫做“myid”。内容就是每个实例的id。例如1、2、3。[root@zhaohadoop2 zookeeper]# mkdir data[root@zhaohadoop2 zookeeper]# echo &quot;1&quot; &gt; data/myid 3.2 修改配置文件zoo.cfg12345678910111213141516171819202122232425262728293031323334353637383940[root@zhaohadoop2 zookeeper]# cd conf/[root@zhaohadoop2 conf]# lsconfiguration.xsl log4j.properties zoo_sample.cfg[root@zhaohadoop2 conf]# cp zoo_sample.cfg zoo.cfg[root@zhaohadoop2 conf]# vim zoo.cfg#修改成如下：# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just# example sakes.dataDir=/usr/local/zookeeper/data# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the# administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1server.1=192.168.101.154:2881:3881server.2=192.168.101.156:2881:3881server.3=192.168.101.157:2881:3881说明：server后面的数字代表节点id。IP后面的端口分别为zookeeper内部通讯端口和投票选举端口。 三台都配置好，注意myid，不要冲突，1为主。 3.3 启动zookeeper12345678910111213141516171819202122232425262728293031323334353637383940414243[root@zhaohadoop2 zookeeper]# ./bin/zkServer.sh start#查看状态：[root@zhaohadoop2 zookeeper]# ./bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgError contacting service. It is probably not running. #报错提示可能未运行。别急 往下看：#查看端口：[root@zhaohadoop2 zookeeper]# netstat -antup | grep 2181tcp6 0 0 :::2181 :::* LISTEN 3309/java #端口已经被监听#启动第二个zookeeper，第三个zookeeper[root@zhaohadoop3 zookeeper]# ./bin/zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED[root@zhaohadoop4 zookeeper]# ./bin/zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED#再来查看zookeeper运行状态第一台：[root@zhaohadoop2 zookeeper]# ./bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgMode: follower第二台：[root@zhaohadoop3 zookeeper]# ./bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgMode: leader第三台：[root@zhaohadoop4 zookeeper]# ./bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgMode: follower#说明：显示无问题，leader为第二台机器。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://kkabuzs.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://kkabuzs.github.io/tags/Hadoop/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://kkabuzs.github.io/tags/Zookeeper/"}]},{"title":"Hadoop集群之Hbase","slug":"hadoopjiqunzhihbase","date":"2019-07-11T08:11:25.000Z","updated":"2019-07-11T08:11:25.000Z","comments":true,"path":"articles/2019/07/11/hadoopjiqunzhihbase/","permalink":"https://kkabuzs.github.io/articles/2019/07/11/hadoopjiqunzhihbase/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Hadoop集群之Hbase一，概述 HBase是一个构建在HDFS上的分布式列存储系统； HBase是基于Google BigTable模型开发的，典型的key&#x2F;value系统； HBase是Apache Hadoop生态系统中的重要一员，主要用于海量结构化数据存储； 从逻辑上讲，HBase将数据按照表、行和列进行存储。 与hadoop一样，Hbase目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加计算和存储能力。 1.1 Hbase表的特点 **大：**一个表可以有数十亿行，上百万列； **无模式：**每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列； **面向列：**面向列（族）的存储和权限控制，列（族）独立检索； **稀疏：**空（null）列并不占用存储空间，表可以设计的非常稀疏； **数据多版本：**每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳； **数据类型单一：**Hbase中的数据都是字符串，没有类型。 1.2 Hbase逻辑视图 1.3 Hbase基本概念 RowKey：是Byte array，是表中每条记录的“主键”，方便快速查找，Rowkey的设计非常重要。 Column Family：列族，拥有一个名称(string)，包含一个或者多个相关列 Column：属于某一个columnfamily，familyName:columnName，每条记录可动态添加 Version Number：类型为Long，默认值是系统时间戳，可由用户自定义 Value(Cell)：Byte array ###1.4 Hbase基本组件说明 Client 包含访问Hbase的接口，并维护cache来加快对Hbase的访问，比如region的位置信息。 Master 为Region server分配region 负责Region server的负载均衡 发现失效的Region server并重新分配其上的Region 管理用户对table的增删改查操作 Region server Region server维护region，处理对这些region的IO请求。 Region server负责切分在运行中变得过大的region。 Zookeeper的作用 通过选举，保证任何时候，集群中只有一个master，master和Region server启动时会向zookeeper注册。 存贮所有Region的寻址入口 实时监控Region server的上线和下线信息。并实时通知给Master 存储HBase的schema和table元数据 默认情况下，HBase管理ZooKeeper实例，比如，启动或者停止ZooKeeper Zookeeper的引入使得Master不再是单点故障 二，Hbase部署搭建2.1 环境准备 主机名 ip地址 用户名 运行组件 zhaohadoop1 192.168.101.150 hadoop DataNode、NodeManager、Hbase zhaohadoop2 192.168.101.154 hadoop DataNode、NodeManager、SecondaryNameNode、zookeeper、Hbase zhaohadoop3 192.168.101.156 hadoop NameNode、ResourceManager、zookeeper、Hbase zhaohadoop4 192.168.101.157 hadoop DataNode、NodeManager、zookeeper、Hbase 2.2 安装Hbase 下载地址：https://mirrors.tuna.tsinghua.edu.cn/apache/hbase/ 1234567891011121314151617#首先在hadoop-master安装配置好之后，在复制到从节点[root@zhaohadoop1 ~]# wget https://mirrors.tuna.tsinghua.edu.cn/apache/hbase/2.2.0/hbase-2.2.0-bin.tar.gz[root@zhaohadoop1 ~]# tar xf hbase-2.2.0-bin.tar.gz -C /usr/local/[root@zhaohadoop1 ~]# mv /usr/local/hbase-2.2.0 /usr/local/hbase[root@zhaohadoop1 ~]# vim /etc/profile#添加如下两行：export HBASE_HOME=/usr/local/hbaseexport PATH=$HBASE_HOME/bin:$PATH#使立即生效[root@zhaohadoop1 ~]# source /etc/profile[root@zhaohadoop1 ~]# cd /usr/local/hbase/conf/[root@zhaohadoop1 conf]# lshadoop-metrics2-hbase.properties hbase-env.cmd hbase-env.sh hbase-policy.xml hbase-site.xml log4j.properties regionservers[root@zhaohadoop1 conf]# cp hbase-env.sh&#123;,.bak&#125; 修改hbase-env.sh文件 1234[root@zhaohadoop1 conf]# vim hbase-env.sh#修改如下内容：export JAVA_HOME=/usr/local/jdk/export HBASE_MANAGES_ZK=false 修改hbase-site.xml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@zhaohadoop1 conf]# cp hbase-site.xml&#123;,.bak&#125;&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--/** * * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * &quot;License&quot;); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */--&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;zhaohadoop2:2181,zhaohadoop3:2181,zhaohadoop4:2181&lt;/value&gt; &lt;description&gt;The directory shared by RegionServers. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://zhaohadoop1:9000/hbase&lt;/value&gt; &lt;description&gt;The directory shared by RegionServers. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;The mode the cluster will be in. Possible values are false: standalone and pseudo-distributed setups with managed ZooKeeper true: fully-distributed with unmanaged ZooKeeper Quorum (see hbase-env.sh) &lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; hbase.zookeeper.quorum： 这个参数是用来设置zookeeper服务列表，每个服务器之间使用使用逗号分隔，2181是zookeeper默认端口号，你可以自行根据你的端口号添加，默认的端口号加不加都无所谓。hbase.rootdir: 这个参数是用来设置RegionServer 的共享目录，用来存放HBase数据。特别需要注意的是 hbase.rootdir 里面的 HDFS 地址是要跟 Hadoop 的 core-site.xml 里面的 fs.defaultFS 的 HDFS 的 IP 地址或者域名、端口必须一致。hbase.cluster.distributed： HBase 的运行模式。为 false 表示单机模式，为 true 表示分布式模式。 编辑regionservers文件，配置从服务器，去掉 localhost，添加 slaves 节点 1234[root@zhaohadoop1 conf]# vim regionserverszhaohadoop2zhaohadoop3zhaohadoop4 把 hadoop 的 hdfs-site.xml 复制一份到 hbase 的 conf 目录下 123456[root@zhaohadoop1 conf]# cp /usr/local/hadoop/etc/hadoop/hdfs-site.xml /usr/local/hbase/conf/#将配置好的 habase 分发到其它节点对应的路径下[root@zhaohadoop1 conf]# scp -r /usr/local/hbase root@192.168.101.154:/usr/local/[root@zhaohadoop1 conf]# scp -r /usr/local/hbase root@192.168.101.156:/usr/local/[root@zhaohadoop1 conf]# scp -r /usr/local/hbase root@192.168.101.157:/usr/local/ 三台从节点配好环境变量 12345678#zhaohadoop2节点，zhaohadoop3节点，zhaohadoop4节点：[root@zhaohadoop2 ~]# vim /etc/profile#追加如下内容export HBASE_HOME=/usr/local/hbaseexport PATH=$HBASE_HOME/bin:$PATH#立即生效：[root@zhaohadoop2 ~]# source /etc/profile 2.3 启动Hbase123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#做好ssh免密：[root@zhaohadoop1 hbase]# ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:LU5bIl22f3fmLR7yQZT25KgEGSP563LRlSNVYBiEXzY root@zhaohadoop1The key&#x27;s randomart image is:+---[RSA 2048]----+| ..oooooo.|| ...+..E. || .=. ++o.|| . +.ooo+= || . S +o.oo.o|| + =oo.o || o. .+ + +|| . o +.*o|| o .o.o|+----[SHA256]-----+[root@zhaohadoop1 hbase]# ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.101.154/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@192.168.101.154&#x27;s password:Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &#x27;192.168.101.154&#x27;&quot;and check to make sure that only the key(s) you wanted were added.[root@zhaohadoop1 hbase]# ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.101.156/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@192.168.101.156&#x27;s password:Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &#x27;192.168.101.156&#x27;&quot;and check to make sure that only the key(s) you wanted were added.[root@zhaohadoop1 hbase]# ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.101.157/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@192.168.101.157&#x27;s password:Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &#x27;192.168.101.157&#x27;&quot;and check to make sure that only the key(s) you wanted were added. 123456789101112131415161718192021222324252627282930313233343536373839404142434445#启动服务：[root@zhaohadoop1 hbase]# ./bin/start-hbase.shSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hbase/lib/client-facing-thirdparty/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]running master, logging to /usr/local/hbase/logs/hbase-root-master-zhaohadoop1.outSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hbase/lib/client-facing-thirdparty/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]zhaohadoop2: regionserver running as process 27254. Stop it first.zhaohadoop3: regionserver running as process 3619. Stop it first.zhaohadoop4: regionserver running as process 4569. Stop it first.#查看四台机器jps命令：[root@zhaohadoop1 hbase]# jps2018 NameNode2298 ResourceManager25245 HMaster #成功25439 Jps[root@zhaohadoop2 ~]# jps2134 SecondaryNameNode27254 HRegionServer2215 NodeManager2029 DataNode3309 QuorumPeerMain28015 Jps[root@zhaohadoop3 ~]# jps3619 HRegionServer7189 QuorumPeerMain4248 Jps1948 NodeManager1838 DataNode[root@zhaohadoop4 ~]# jps1873 DataNode5298 Jps6837 QuorumPeerMain4569 HRegionServer1983 NodeManager","categories":[{"name":"大数据","slug":"大数据","permalink":"https://kkabuzs.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://kkabuzs.github.io/tags/Hadoop/"}]},{"title":"Hadoop集群之Hive","slug":"hadoopjiqunzhihive","date":"2019-07-09T08:13:44.000Z","updated":"2019-07-09T08:13:44.000Z","comments":true,"path":"articles/2019/07/09/hadoopjiqunzhihive/","permalink":"https://kkabuzs.github.io/articles/2019/07/09/hadoopjiqunzhihive/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Hadoop集群之Hive一，Hive概述1.1 数据仓库的概念 数据仓库（Data Warehouse）是一个面向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史变化（Time Variant）的数据集合，用于支持管理决策。 传统数据仓库面临的挑战： 无法满足快速增长的海量数据存储需求。 无法有效处理不同类型的数据。 计算和处理能力不足。 1.2 Hive简介 Hive是一个构建于Hadoop顶层的数据仓库工具，可以查询和管理PB级别的分布式数据。 支持大规模数据存储、分析，具有良好的可扩展性 某种程度上可以看作是用户编程接口，本身不存储和处理数据。 依赖分布式文件系统HDFS存储数据。 依赖分布式并行计算模型MapReduce处理数据。 定义了简单的类似SQL 的查询语言——HiveQL。 用户可以通过编写的HiveQL语句运行MapReduce任务。 可以很容易把原来构建在关系数据库上的数据仓库应用程序移植到Hadoop平台上。 是一个可以提供有效、合理、直观组织和使用数据的分析工具。 Hive具有的特点非常适用于数据仓库。 （1）采用批处理方式处理海量数据 Hive需要把HiveQL语句转换成MapReduce任务进行运行。 数据仓库存储的是静态数据，对静态数据的分析适合采用批处理方式，不需要快速响应给出结果，而且数据本身也不会频繁变化。 （2）提供适合数据仓库操作的工具 Hive本身提供了一系列对数据进行提取、转换、加载（ETL）的工具，可以存储、查询和分析存储在Hadoop中的大规模数据。 这些工具能够很好地满足数据仓库各种应用场景。 （3）支持MapReduce，Tez，Spark等多种计算引擎。 （4）可以直接访问HDFS文件以及HBase。 （5）易用易编程。 1.3 Hive与Hadoop生态系统中其他组件的关系 Hive依赖于HDFS 存储数据 Hive依赖于MapReduce 处理数据 在某些场景下Pig可以作为Hive的替代工具 HBase 提供数据的实时访问 1.4 Hive的优缺点Hive的优点： 高可靠、高容错：HiveServer采用集群模式。双MetaStor。超时重试机制。 类SQL：类似SQL语法，内置大量函数。 可扩展：自定义存储格式，自定义函数。 多接口：Beeline，JDBC，ODBC，Python，Thrift。 Hive的缺点： 延迟较高：默认MR为执行引擎，MR延迟较高。 不支持雾化视图：Hive支持普通视图，不支持雾化视图。Hive不能再视图上更新、插入、删除数据。 不适用OLTP：暂不支持列级别的数据添加、更新、删除操作。 暂不支持存储过程：当前版本不支持存储过程，只能通过UDF来实现一些逻辑处理。 二，Hive部署安装环境介绍 主机名 ip地址 用户名 运行组件 zhaohadoop1 192.168.101.150 hadoop DataNode、NodeManager zhaohadoop2 192.168.101.154 hadoop DataNode、NodeManager、SecondaryNameNode zhaohadoop3 192.168.101.156 hadoop NameNode、ResourceManager、Hive zhaohadoop4 192.168.101.157 hadoop DataNode、NodeManager 2.1 下载hive软件包 下载地址：https://mirrors.tuna.tsinghua.edu.cn/apache/hive/（采用清华的镜像库） 1234#安装hive2.3.5版本[root@zhaohadoop3 ~]# wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-2.3.5/apache-hive-2.3.5-bin.tar.gz[root@zhaohadoop3 ~]# lsanaconda-ks.cfg apache-hive-2.3.5-bin.tar.gz hadoop-2.9.2.tar.gz jdk-8u181-linux-x64.tar.gz 2.2 解压并配置环境变量1234567891011[root@zhaohadoop3 ~]# tar xf apache-hive-2.3.5-bin.tar.gz -C /usr/local/[root@zhaohadoop3 ~]# mv /usr/local/apache-hive-2.3.5-bin /usr/local/hive#配置环境变量[root@zhaohadoop3 ~]# vim /etc/profile在最下面追加：#hiveexport HIVE_HOME=/usr/local/hiveexport PATH=$PATH:$HIVE_HOME/bin[root@zhaohadoop3 ~]# source /etc/profile 2.3 配置Hive文件2.3.1 修改hive-env.sh12345678[root@zhaohadoop3 ~]# cd /usr/local/hive/conf/[root@zhaohadoop3 conf]# cp hive-env.sh.template hive-env.sh[root@zhaohadoop3 conf]# vim hive-env.sh# HADOOP_HOME=$&#123;bin&#125;/../../hadoop# export HIVE_CONF_DIR=#改为：HADOOP_HOME=/usr/local/hadoopexport HIVE_CONF_DIR=/usr/local/hive/conf 2.3.2 修改hive-log4j.properties 修改hive的log存放日志到&#x2F;usr&#x2F;local&#x2F;hive&#x2F;logs 12345[root@zhaohadoop1 conf]# cp hive-log4j2.properties.template hive-log4j2.properties[root@zhaohadoop3 conf]# vim hive-log4j2.propertiesproperty.hive.log.dir = $&#123;sys:java.io.tmpdir&#125;/$&#123;sys:user.name&#125;#改为：property.hive.log.dir = /usr/local/hive/logs 2.3.3 运行hive1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889[root@zhaohadoop3 conf]# /usr/local/hadoop/bin/hdfs dfs -ls -R /drwx-wx-wx - root supergroup 0 2019-07-10 14:14 /tmpdrwx-wx-wx - root supergroup 0 2019-07-10 14:14 /tmp/hivedrwx------ - root supergroup 0 2019-07-10 14:14 /tmp/hive/root[root@zhaohadoop3 conf]# /usr/local/hadoop/bin/hadoop fs -mkdir -p /user/hive/warehouse[root@zhaohadoop3 conf]# /usr/local/hadoop/bin/hadoop fs -chmod g+w /user/hive/warehouse[root@zhaohadoop3 conf]# /usr/local/hadoop/bin/hdfs dfs -ls -R /drwx-wx-wx - root supergroup 0 2019-07-10 14:14 /tmpdrwx-wx-wx - root supergroup 0 2019-07-10 14:14 /tmp/hivedrwx------ - root supergroup 0 2019-07-10 14:14 /tmp/hive/rootdrwxr-xr-x - root supergroup 0 2019-07-17 09:40 /userdrwxr-xr-x - root supergroup 0 2019-07-17 09:40 /user/hivedrwxrwxr-x - root supergroup 0 2019-07-17 09:40 /user/hive/warehouse#修改配置文件：[root@zhaohadoop3 conf]# cp hive-default.xml.template hive-stze.xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.--&gt;&lt;configuration&gt; &lt;!-- WARNING!!! This file is auto generated for documentation purposes ONLY! --&gt; &lt;!-- WARNING!!! Any changes you make to this file will be ignored by Hive. --&gt; &lt;!-- WARNING!!! You must make your changes in hive-site.xml instead. --&gt; &lt;!-- Hive Execution Parameters --&gt; &lt;property&gt; &lt;name&gt;system:java.io.tmpdir&lt;/name&gt; &lt;value&gt;/tmp/hive/java&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;system:user.name&lt;/name&gt; &lt;value&gt;$&#123;user.name&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.schema.autoCreateAll&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; #开启生成元数据表 &lt;/property&gt;&lt;/configuration&gt;#初始化元数据：[root@zhaohadoop3 hive]# $HIVE_HOME/bin/schematool -dbType derby -initSchemaSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Metastore connection URL: jdbc:derby:;databaseName=metastore_db;create=trueMetastore Connection Driver : org.apache.derby.jdbc.EmbeddedDriverMetastore connection User: APPStarting metastore schema initialization to 2.3.0Initialization script hive-schema-2.3.0.derby.sqlInitialization script completedschemaTool completed#运行hive：[root@zhaohadoop3 hive]# ./bin/hivewhich: no hbase in (/usr/local/jdk/bin:/usr/local/jdk/jre/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop/sbin:/usr/local/hadoop/bin:/usr/local/hive/bin:/root/bin)SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j2.properties Async: trueHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.hive&gt; show databases; #默认有个default库OKdefaultTime taken: 4.938 seconds, Fetched: 1 row(s)hive&gt; use default;OKTime taken: 0.03 secondshive&gt; show tables;OKTime taken: 0.037 secondshive&gt; 2.4 使用MySQL作为hive的元数据库2.4.1 下载MySQL 5.6 JDBC包 mysql5.6下载地址：http://ftp.iij.ad.jp/pub/db/mysql/Downloads/MySQL-5.6/mysql JDBC 下载地址：https://dev.mysql.com/downloads/connector/j/postgresql JDBC下载地址：下载地址：https://jdbc.postgresql.org/download.html 12#下载mysql的JDBC[root@zhaohadoop3 ~]# wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.16.tar.gz 2.4.2 安装 MySQL123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161#查看系统中是否已经安装了MySQL[root@zhaohadoop3 ~]# rpm -qa | grep mysql #没有安装#安装mysql 5.6[root@zhaohadoop3 ~]# rpm -Uvh http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm#查看mysql可用安装[root@zhaohadoop3 ~]# yum repolist enabled | grep &quot;mysql.*-community.*&quot;mysql-connectors-community/x86_64 MySQL Connectors Community 108mysql-tools-community/x86_64 MySQL Tools Community 90mysql56-community/x86_64 MySQL 5.6 Community Server 463#安装mysql 5.6[root@zhaohadoop3 ~]# yum -y install mysql-community-server#加入开机启动，启动服务[root@zhaohadoop3 ~]# systemctl enable mysqld[root@zhaohadoop3 ~]# systemctl start mysqld#配置mysql（设置root密码等）：[root@zhaohadoop3 ~]# mysql_secure_installationNOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MySQL SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY!In order to log into MySQL to secure it, we&#x27;ll need the currentpassword for the root user. If you&#x27;ve just installed MySQL, andyou haven&#x27;t set the root password yet, the password will be blank,so you should just press enter here.Enter current password for root (enter for none):OK, successfully used password, moving on...Setting the root password ensures that nobody can log into the MySQLroot user without the proper authorisation.Set root password? [Y/n] y [设置root用户密码]New password:Re-enter new password:Password updated successfully!Reloading privilege tables.. ... Success!By default, a MySQL installation has an anonymous user, allowing anyoneto log into MySQL without having to have a user account created forthem. This is intended only for testing, and to make the installationgo a bit smoother. You should remove them before moving into aproduction environment.Remove anonymous users? [Y/n] y [删除匿名用户] ... Success!Normally, root should only be allowed to connect from &#x27;localhost&#x27;. Thisensures that someone cannot guess at the root password from the network.Disallow root login remotely? [Y/n] n [禁止root远程登录] ... skipping.By default, MySQL comes with a database named &#x27;test&#x27; that anyone canaccess. This is also intended only for testing, and should be removedbefore moving into a production environment.Remove test database and access to it? [Y/n] y [删除test数据库] - Dropping test database...ERROR 1008 (HY000) at line 1: Can&#x27;t drop database &#x27;test&#x27;; database doesn&#x27;t exist ... Failed! Not critical, keep moving... - Removing privileges on test database... ... Success!Reloading the privilege tables will ensure that all changes made so farwill take effect immediately.Reload privilege tables now? [Y/n] y [刷新权限] ... Success!All done! If you&#x27;ve completed all of the above steps, your MySQLinstallation should now be secure.Thanks for using MySQL!Cleaning up...#配置完毕，测试mysql[root@zhaohadoop3 ~]# mysql -uroot -p123456Warning: Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 12Server version: 5.6.44 MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema |+--------------------+3 rows in set (0.00 sec) #mysql 搭建完毕#设置任意机器都可以远程登陆mysqlmysql&gt; use mysql;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; select user,host,password from user;+------+-------------+-------------------------------------------+| user | host | password |+------+-------------+-------------------------------------------+| root | localhost | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 || root | zhaohadoop1 | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 || root | 127.0.0.1 | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 || root | ::1 | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 |+------+-------------+-------------------------------------------+4 rows in set (0.00 sec)mysql&gt; update user set host=&#x27;%&#x27; where user=&#x27;root&#x27; and host=&#x27;localhost&#x27;;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select user,host,password from user;+------+-------------+-------------------------------------------+| user | host | password |+------+-------------+-------------------------------------------+| root | % | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 || root | zhaohadoop1 | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 || root | 127.0.0.1 | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 || root | ::1 | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 |+------+-------------+-------------------------------------------+4 rows in set (0.00 sec)#删除无用账号mysql&gt; delete from user where user=&#x27;root&#x27; and host=&#x27;zhaohadoop3&#x27;;Query OK, 1 row affected (0.00 sec)mysql&gt; delete from user where user=&#x27;root&#x27; and host=&#x27;127.0.0.1&#x27;;Query OK, 1 row affected (0.00 sec)mysql&gt; delete from user where user=&#x27;root&#x27; and host=&#x27;::1&#x27;;Query OK, 1 row affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt; select user,host,password from user;+------+------+-------------------------------------------+| user | host | password |+------+------+-------------------------------------------+| root | % | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 |+------+------+-------------------------------------------+1 row in set (0.00 sec) 2.4.3 修改hive-site.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475[root@zhaohadoop3 conf]# cp hive-default.xml.template hive-site.xml[root@zhaohadoop3 conf]# vim hive-site.xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.--&gt;&lt;configuration&gt; &lt;!-- WARNING!!! This file is auto generated for documentation purposes ONLY! --&gt; &lt;!-- WARNING!!! Any changes you make to this file will be ignored by Hive. --&gt; &lt;!-- WARNING!!! You must make your changes in hive-site.xml instead. --&gt; &lt;!-- Hive Execution Parameters --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.101.156:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;beeline.hs2.connection.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;beeline.hs2.connection.password&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;beeline.hs2.connection.hosts&lt;/name&gt; &lt;value&gt;localhost:10000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.client.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against thrift client&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.client.password&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;description&gt;Password to use against thrift client&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 2.4.4 将mysql JDBC解压并拷贝到hive的lib路径12345678[root@zhaohadoop3 ~]# lsanaconda-ks.cfg hadoop-2.9.2.tar.gz MySQL-client-5.6.44-1.el7.x86_64.rpm MySQL-server-5.6.44-1.el7.x86_64.rpmapache-hive-2.3.5-bin.tar.gz jdk-8u181-linux-x64.tar.gz mysql-connector-java-8.0.16.tar.gz postgresql-9.4.1212.jar[root@zhaohadoop3 ~]# tar xf mysql-connector-java-8.0.16.tar.gz[root@zhaohadoop3 ~]# cd mysql-connector-java-8.0.16[root@zhaohadoop3 mysql-connector-java-8.0.16]# lsbuild.xml CHANGES INFO_BIN INFO_SRC LICENSE mysql-connector-java-8.0.16.jar README src[root@zhaohadoop3 mysql-connector-java-8.0.16]# cp mysql-connector-java-8.0.16.jar /usr/local/hive/lib/ 2.5 启动hive并查看数据库的变化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124[root@zhaohadoop3 hive]# $HIVE_HOME/bin/schematool -dbType mysql -initSchema #初始化元数据SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Metastore connection URL: jdbc:mysql://192.168.101.156:3306/metastore?createDatabaseIfNotExist=trueMetastore Connection Driver : com.mysql.cj.jdbc.DriverMetastore connection User: rootStarting metastore schema initialization to 2.3.0Initialization script hive-schema-2.3.0.mysql.sqlInitialization script completedschemaTool completed[root@zhaohadoop3 hive]# ./bin/hive #运行hivewhich: no hbase in (/usr/local/jdk/bin:/usr/local/jdk/jre/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/hadoop/sbin:/usr/local/hadoop/bin:/usr/local/hive/bin:/root/bin)SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j2.properties Async: trueHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.hive&gt; show databases;OKdefaultTime taken: 4.174 seconds, Fetched: 1 row(s)#查看数据库的变化[root@zhaohadoop3 ~]# mysql -uroot -p123456Warning: Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 50Server version: 5.6.44 MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || metastore | #数据库已经被创建| mysql || performance_schema |+--------------------+4 rows in set (0.01 sec)mysql&gt; use metastore ;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables; #数据表已经生成+---------------------------+| Tables_in_metastore |+---------------------------+| AUX_TABLE || BUCKETING_COLS || CDS || COLUMNS_V2 || COMPACTION_QUEUE || COMPLETED_COMPACTIONS || COMPLETED_TXN_COMPONENTS || DATABASE_PARAMS || DBS || DB_PRIVS || DELEGATION_TOKENS || FUNCS || FUNC_RU || GLOBAL_PRIVS || HIVE_LOCKS || IDXS || INDEX_PARAMS || KEY_CONSTRAINTS || MASTER_KEYS || NEXT_COMPACTION_QUEUE_ID || NEXT_LOCK_ID || NEXT_TXN_ID || NOTIFICATION_LOG || NOTIFICATION_SEQUENCE || NUCLEUS_TABLES || PARTITIONS || PARTITION_EVENTS || PARTITION_KEYS || PARTITION_KEY_VALS || PARTITION_PARAMS || PART_COL_PRIVS || PART_COL_STATS || PART_PRIVS || ROLES || ROLE_MAP || SDS || SD_PARAMS || SEQUENCE_TABLE || SERDES || SERDE_PARAMS || SKEWED_COL_NAMES || SKEWED_COL_VALUE_LOC_MAP || SKEWED_STRING_LIST || SKEWED_STRING_LIST_VALUES || SKEWED_VALUES || SORT_COLS || TABLE_PARAMS || TAB_COL_STATS || TBLS || TBL_COL_PRIVS || TBL_PRIVS || TXNS || TXN_COMPONENTS || TYPES || TYPE_FIELDS || VERSION || WRITE_SET |+---------------------------+57 rows in set (0.00 sec)","categories":[{"name":"大数据","slug":"大数据","permalink":"https://kkabuzs.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://kkabuzs.github.io/tags/Hadoop/"}]},{"title":"CentOS 7部署Hadoop集群（完全分布式）","slug":"centos7bushuhadoopjiqun","date":"2019-07-08T03:22:28.000Z","updated":"2019-07-08T03:22:28.000Z","comments":true,"path":"articles/2019/07/08/centos7bushuhadoopjiqun/","permalink":"https://kkabuzs.github.io/articles/2019/07/08/centos7bushuhadoopjiqun/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 CentOS 7部署Hadoop集群（完全分布式）测试环境 Linux版本： CentOS 7 64位Hadoop版本： hadoop-2.9.2Java版本： jdk-8u181-linux-x64 一，集群服务器节点与进程 Hadoop中的HDFS和YARN都是主从结构，主从结构中的主节点和从节点有多重概念方式： 主节点 从节点 master slave 管理者 工作者 leader followe Hadoop集群中各个角色的名称： 服务 主节点 从节点 HDFS NameNode DataNode YARN ResourceManager NodeManager 1.1 HDFSNameNode 主Master，整个Hadoop集群只能有一个，管理HDFS文件系统的命名空间，维护元数据信息，管理副本的配置和信息（默认三个副本），处理客户端读写请求。 DataNode Slave 工作节点，集群一般会启动多个，负责存储数据块和数据块校验，执行客户端的读写请求，通过心跳机制定期向NameNode汇报运行状态和本地所有块的列表信息，在集群启动时DataNode项NameNode提供存储Block块的列表信息。 1.2 YARNResourceManager 整个集群只有一个Master，Slave可以有多个，支持高可用，处理客户端Client请求，启动／管理／监控ApplicationMaster，监控NodeManager，资源的分配和调度。 NodeManager 每个节点只有一个，一般与Data Node部署在同一台机器上且一一对应，定时向Resource Manager汇报本机资源的使用状况，处理来自Resource Manager的作业请求，为作业分配Container，处理来自Application Master的请求，启动和停止Container 1.3 为增加集群的容灾性，将对SecondaryNameNode进行配置 **SecondaryNameNode：**备份所有数据分布情况，当Namenode服务器宕机（日常所说的死机）时，可通过该服务器来恢复数据。 1.4 集群部署规划 主机名 ip地址 用户名 运行组件 zhaohadoop1 192.168.101.150 hadoop DataNode、NodeManager zhaohadoop2 192.168.101.154 hadoop DataNode、NodeManager、SecondaryNameNode zhaohadoop3 192.168.101.156 hadoop NameNode、ResourceManager zhaohadoop4 192.168.101.157 hadoop DataNode、NodeManager 1.5 系统信息1234[root@zhaohadoop3 ~]# cat /etc/redhat-releaseCentOS Linux release 7.5.1804 (Core)[root@zhaohadoop1 ~]# uname -r3.10.0-862.el7.x86_64 二，配置Java环境2.1 下载所需安装包 jdk下载地址：https://github.com/frekele/oracle-java/releaseshadoop下载地址：http://mirror.bit.edu.cn/apache/hadoop/common/ 123#安装包已经下载完毕[root@zhaohadoop3 ~]# lshadoop-2.9.2.tar.gz jdk-8u181-linux-x64.tar.gz 2.2 安装java jdk（所有机器）12345678910111213141516[root@zhaohadoop3 ~]# tar xf jdk-8u181-linux-x64.tar.gz -C /usr/local/[root@zhaohadoop3 ~]# ln -s /usr/local/jdk1.8.0_181 /usr/local/jdk#配置java环境变量[root@zhaohadoop3 ~]# sed -i.ori &#x27;$a export JAVA_HOME=/usr/local/jdk\\nexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH\\nexport CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar&#x27; /etc/profile[root@zhaohadoop1 ~]# tail -3 /etc/profileexport JAVA_HOME=/usr/local/jdkexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATHexport CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar[root@zhaohadoop1 ~]# source /etc/profile[root@zhaohadoop1 ~]# which java/usr/local/jdk/bin/java[root@zhaohadoop1 ~]# java -versionjava version &quot;1.8.0_181&quot;Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode) 三，安装Hadoop3.1 解压tar包（所有机器）12[root@zhaohadoop3 ~]# tar xf hadoop-2.9.2.tar.gz -C /usr/local/[root@zhaohadoop3 ~]# mv /usr/local/hadoop-2.9.2 /usr/local/hadoop ###3.2 配置hadoop环境变量（所有机器） 123456789101112131415161718192021#配置hadoop环境变量[root@zhaohadoop3 ~]# vim /etc/profile#追加以下两行内容export HADOOP_HOME=/usr/local/hadoopexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin[root@zhaohadoop3 ~]# source /etc/profile #即时生效#关闭防火墙和selinux[root@zhaohadoop3 ~]# systemctl stop firewalld[root@zhaohadoop3 ~]# systemctl disable firewalldRemoved symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.[root@zhaohadoop3 ~]# vim /etc/selinux/configSELINUX=enforcing#改为：SELINUX=disabled[root@zhaohadoop1 ~]# setenforce 0[root@zhaohadoop1 ~]# reboot 3.3 修改hosts文件并配置ssh免密码登陆（所有机器）12345678910111213141516171819202122232425262728293031323334353637#修改hosts文件[root@zhaohadoop1 ~]# vim /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.101.150 zhaohadoop1192.168.101.154 zhaohadoop2192.168.101.156 zhaohadoop3192.168.101.157 zhaohadoop4#配置ssh免密码（四台都做）[root@zhaohadoop3 ~]$ ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):Created directory &#x27;/root/.ssh&#x27;.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:jmmRonhMA5TtED+EmDQTrlanZm3HP09WLccEQatgcH0 hadoop@zhaohadoop1The key&#x27;s randomart image is:+---[RSA 2048]----+|oO*. . .. .+. ||=+=. o . Eo ||..o+ . o .. . ||....= ... . . + ||..o+.oooS . o + ||.+oo...=. . o ||. + + .o o || . . = || . |+----[SHA256]-----+[root@zhaohadoop3 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.101.150[root@zhaohadoop3 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.101.154[root@zhaohadoop3 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.101.156[root@zhaohadoop3 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.101.157 3.4 修改Hadoop配置文件 均使用hadoop用户操作 123456[root@zhaohadoop3 ~]$ cd /usr/local/hadoop/etc/hadoop/#修改hadoop-env.sh[root@zhaohadoop3 hadoop]$ vim hadoop-env.sh#export JAVA_HOME=$&#123;JAVA_HOME&#125; #注释原配置export JAVA_HOME=/usr/local/jdk #增加新配置 3.4.1 修改core-site.xml文件123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@zhaohadoop3 hadoop]$ vim core-site.xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;指定hadoop运行时产生文件的存储路径&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://zhaohadoop3:9000&lt;/value&gt; &lt;description&gt;hdfs namenode的通信地址,通信端口&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.users&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3.4.2 修改hdfs-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@zhaohadoop3 hadoop]$ vim hdfs-site.xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;!-- 该文件指定与HDFS相关的配置信息。 需要修改HDFS默认的块的副本属性，因为HDFS默认情况下每个数据块保存3个副本，而在伪分布式模式下运行时，由于只有一个数据节点，所以需要将副本个数改为1；否则Hadoop程序会报错。 --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;description&gt;指定HDFS存储数据的副本数目，默认情况下是3份&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoopdata/namenode&lt;/value&gt; &lt;description&gt;namenode存放数据的目录&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoopdata/datanode&lt;/value&gt; &lt;description&gt;datanode存放block块的目录&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.http.address&lt;/name&gt; &lt;value&gt;zhaohadoop2:50090&lt;/value&gt; &lt;description&gt;secondarynamenode 运行节点的信息，和 namenode 不同节点&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;关闭权限验证&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 3.4.3 修改mapred-site.xml &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop文件夹中并没有mapred-site.xml文件，但提供了模板mapred-site.xml.template，将其复制一份重命名为mapred-site.xml 即可 123456789101112131415161718192021222324252627282930[root@zhaohadoop3 hadoop]$ cp mapred-site.xml.template mapred-site.xml[root@zhaohadoop3 hadoop]$ vim mapred-site.xml&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;!-- 在该配置文件中指定与MapReduce作业相关的配置属性，需要指定JobTracker运行的主机地址--&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;description&gt;指定mapreduce运行在yarn上&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 3.4.4 修改yarn-site.xml12345678910111213141516171819202122232425262728293031[root@zhaohadoop3 hadoop]$ vim yarn-site.xml&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;zhaohadoop3&lt;/value&gt; &lt;description&gt;yarn总管理器的IPC通讯地址&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;description&gt;mapreduce执行shuffle时获取数据的方式&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 3.4.5 修改slaves文件1234[root@zhaohadoop3 hadoop]$ vim slaveszhaohadoop1zhaohadoop2zhaohadoop4 3.4.6 拷贝配置文件到其他机器123[root@zhaohadoop3 hadoop]$ scp -r /usr/local/hadoop/etc/hadoop/* zhaohadoop1:/usr/local/hadoop/etc/hadoop[root@zhaohadoop3 hadoop]$ scp -r /usr/local/hadoop/etc/hadoop/* zhaohadoop2:/usr/local/hadoop/etc/hadoop[root@zhaohadoop3 hadoop]$ scp -r /usr/local/hadoop/etc/hadoop/* zhaohadoop4:/usr/local/hadoop/etc/hadoop 四，HDFS初始化 Hadoop配置完后，在zhaohadoop3上格式化namenode 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[root@zhaohadoop3 hadoop]$ hdfs namenode -format************************************************************/19/07/08 17:14:03 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]19/07/08 17:14:03 INFO namenode.NameNode: createNameNode [-format]Formatting using clusterid: CID-483fe437-8b37-4415-be1a-709cc42576d219/07/08 17:14:03 INFO namenode.FSEditLog: Edit logging is async:true19/07/08 17:14:03 INFO namenode.FSNamesystem: KeyProvider: null19/07/08 17:14:03 INFO namenode.FSNamesystem: fsLock is fair: true19/07/08 17:14:03 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false19/07/08 17:14:03 INFO namenode.FSNamesystem: fsOwner = hadoop (auth:SIMPLE)19/07/08 17:14:03 INFO namenode.FSNamesystem: supergroup = supergroup19/07/08 17:14:03 INFO namenode.FSNamesystem: isPermissionEnabled = false19/07/08 17:14:03 INFO namenode.FSNamesystem: HA Enabled: false19/07/08 17:14:03 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling19/07/08 17:14:03 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=100019/07/08 17:14:03 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true19/07/08 17:14:03 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.00019/07/08 17:14:03 INFO blockmanagement.BlockManager: The block deletion will start around 2019 Jul 08 17:14:0319/07/08 17:14:03 INFO util.GSet: Computing capacity for map BlocksMap19/07/08 17:14:03 INFO util.GSet: VM type = 64-bit19/07/08 17:14:03 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB19/07/08 17:14:03 INFO util.GSet: capacity = 2^21 = 2097152 entries19/07/08 17:14:03 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false19/07/08 17:14:03 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS19/07/08 17:14:03 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS19/07/08 17:14:03 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999000012874603319/07/08 17:14:03 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 019/07/08 17:14:03 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 3000019/07/08 17:14:03 INFO blockmanagement.BlockManager: defaultReplication = 319/07/08 17:14:03 INFO blockmanagement.BlockManager: maxReplication = 51219/07/08 17:14:03 INFO blockmanagement.BlockManager: minReplication = 119/07/08 17:14:03 INFO blockmanagement.BlockManager: maxReplicationStreams = 219/07/08 17:14:03 INFO blockmanagement.BlockManager: replicationRecheckInterval = 300019/07/08 17:14:03 INFO blockmanagement.BlockManager: encryptDataTransfer = false19/07/08 17:14:03 INFO blockmanagement.BlockManager: maxNumBlocksToLog = 100019/07/08 17:14:03 INFO namenode.FSNamesystem: Append Enabled: true19/07/08 17:14:03 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=1677721519/07/08 17:14:03 INFO util.GSet: Computing capacity for map INodeMap19/07/08 17:14:03 INFO util.GSet: VM type = 64-bit19/07/08 17:14:03 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB19/07/08 17:14:03 INFO util.GSet: capacity = 2^20 = 1048576 entries19/07/08 17:14:03 INFO namenode.FSDirectory: ACLs enabled? false19/07/08 17:14:03 INFO namenode.FSDirectory: XAttrs enabled? true19/07/08 17:14:03 INFO namenode.NameNode: Caching file names occurring more than 10 times19/07/08 17:14:03 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false19/07/08 17:14:03 INFO util.GSet: Computing capacity for map cachedBlocks19/07/08 17:14:03 INFO util.GSet: VM type = 64-bit19/07/08 17:14:03 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB19/07/08 17:14:03 INFO util.GSet: capacity = 2^18 = 262144 entries19/07/08 17:14:03 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 1019/07/08 17:14:03 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 1019/07/08 17:14:03 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,2519/07/08 17:14:03 INFO namenode.FSNamesystem: Retry cache on namenode is enabled19/07/08 17:14:03 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis19/07/08 17:14:03 INFO util.GSet: Computing capacity for map NameNodeRetryCache19/07/08 17:14:03 INFO util.GSet: VM type = 64-bit19/07/08 17:14:03 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB19/07/08 17:14:03 INFO util.GSet: capacity = 2^15 = 32768 entries19/07/08 17:14:03 INFO namenode.FSImage: Allocated new BlockPoolId: BP-365528711-192.168.101.150-156257724395219/07/08 17:14:03 INFO common.Storage: Storage directory /usr/local/hadoop/hadoopdata/namenode has been successfully formatted.19/07/08 17:14:03 INFO namenode.FSImageFormatProtobuf: Saving image file /usr/local/hadoop/hadoopdata/namenode/current/fsimage.ckpt_0000000000000000000 using no compression19/07/08 17:14:04 INFO namenode.FSImageFormatProtobuf: Image file /usr/local/hadoop/hadoopdata/namenode/current/fsimage.ckpt_0000000000000000000 of size 325 bytes saved in 0 seconds .19/07/08 17:14:04 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 019/07/08 17:14:04 INFO namenode.NameNode: SHUTDOWN_MSG:/************************************************************SHUTDOWN_MSG: Shutting down NameNode at zhaohadoop3/192.168.101.156************************************************************/#格式化成功！！ 4.1 启动Hadoop1234567891011121314151617181920#启动HDFS(注意：不管在集群中的那个节点都可以)[root@zhaohadoop3 hadoop]$ pwd/usr/local/hadoop/etc/hadoop[root@zhaohadoop3 hadoop]$ cd /usr/local/hadoop/sbin/[root@zhaohadoop3 sbin]$ start-dfs.sh可看到如下信息：Starting namenodes on [zhaohadoop3]The authenticity of host &#x27;zhaohadoop3 (192.168.101.156)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:KK9Y/ZvfUvjLWVE0bK1MKrWypjpRLYbx2EyToUNJMCI.ECDSA key fingerprint is MD5:e9:31:13:fd:78:ae:ac:d0:97:6e:76:6d:ba:ed:00:8f.Are you sure you want to continue connecting (yes/no)? yeszhaohadoop3: Warning: Permanently added &#x27;zhaohadoop3&#x27; (ECDSA) to the list of known hosts.zhaohadoop3: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-zhaohadoop3.outzhaohadoop2: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-zhaohadoop2.outzhaohadoop4: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-zhaohadoop4.outzhaohadoop1: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-zhaohadoop1.outStarting secondary namenodes [zhaohadoop2]zhaohadoop2: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-secondarynamenode-zhaohadoop2.out 启动YARN 注意：只能在主节点中进行启动 123456789[root@zhaohadoop3 sbin]$ start-yarn.sh可看到如下信息：starting yarn daemonsstarting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-resourcemanager-zhaohadoop3.outzhaohadoop4: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-zhaohadoop4.outzhaohadoop2: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-zhaohadoop2.outzhaohadoop1: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-zhaohadoop3.out 4.2 查看服务器进程123456789101112131415161718192021222324#zhaohadoop3：[hadoop@zhaohadoop1 sbin]$ jps2018 NameNode2569 Jps2298 ResourceManager#zhaohadoop2：[hadoop@zhaohadoop2 ~]$ jps2134 SecondaryNameNode2358 Jps2215 NodeManager2029 DataNode#zhaohadoop1：[hadoop@zhaohadoop3 ~]$ jps2069 Jps1948 NodeManager1838 DataNode#zhaohadoop4：[hadoop@zhaohadoop4 ~]$ jps1873 DataNode2105 Jps1983 NodeManager 可以看到与我们集群规划所分配的进程是一致的 ###4.3 启动HDFS和YARN的web管理界面 HDFS : http://192.168.101.150:50070 YARN : http://192.168.101.150:8088 HDFS界面 YARN界面","categories":[{"name":"大数据","slug":"大数据","permalink":"https://kkabuzs.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://kkabuzs.github.io/tags/Hadoop/"}]},{"title":"k8s apiserver证书轮换更新（kubeadm）","slug":"k8s-apiserver-zhengshulunhuangengxin-kubeadm","date":"2019-07-04T03:10:18.000Z","updated":"2019-07-04T03:10:18.000Z","comments":true,"path":"articles/2019/07/04/k8s-apiserver-zhengshulunhuangengxin-kubeadm/","permalink":"https://kkabuzs.github.io/articles/2019/07/04/k8s-apiserver-zhengshulunhuangengxin-kubeadm/","excerpt":"","text":"一，确认kubeadm版本 无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 如果使用的事kubeadm 1.8之前的版本，那么需要手动更新证书。或者重建集群。或者升级为1.8版本之后，即可使用命令轮换更新。 12345#kubeadm更新方法示例：$ sudo curl -sSL https://dl.k8s.io/release/v1.8.15/bin/linux/amd64/kubeadm &gt; ./kubeadm.1.8.15$ chmod a+rx kubeadm.1.8.15$ sudo mv /usr/bin/kubeadm /usr/bin/kubeadm.1.7$ sudo mv kubeadm.1.8.15 /usr/bin/kubeadm 二，更新证书流程 因我的kubeadm版本为1.9，故不需要更新。 2.1 备份旧的apiserver，apiserver-kubelet-client和前端代理客户端证书和密钥。123456$ sudo mv /etc/kubernetes/pki/apiserver.key /etc/kubernetes/pki/apiserver.key.old$ sudo mv /etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver.crt.old$ sudo mv /etc/kubernetes/pki/apiserver-kubelet-client.crt /etc/kubernetes/pki/apiserver-kubelet-client.crt.old$ sudo mv /etc/kubernetes/pki/apiserver-kubelet-client.key /etc/kubernetes/pki/apiserver-kubelet-client.key.old$ sudo mv /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/pki/front-proxy-client.crt.old$ sudo mv /etc/kubernetes/pki/front-proxy-client.key /etc/kubernetes/pki/front-proxy-client.key.old 2.2 生成新的apiserver，apiserver-kubelet-client和front-proxy-client证书和密钥。123$ sudo kubeadm alpha phase certs apiserver --apiserver-advertise-address=&lt;IP address of your master server&gt;$ sudo kubeadm alpha phase certs apiserver-kubelet-client$ sudo kubeadm alpha phase certs front-proxy-client 2.3 备份旧配置文件1234$ sudo mv /etc/kubernetes/admin.conf /etc/kubernetes/admin.conf.old$ sudo mv /etc/kubernetes/kubelet.conf /etc/kubernetes/kubelet.conf.old$ sudo mv /etc/kubernetes/controller-manager.conf /etc/kubernetes/controller-manager.conf.old$ sudo mv /etc/kubernetes/scheduler.conf /etc/kubernetes/scheduler.conf.old 2.4 生成新的配置文件。 这里有一个重要的注意事项。如果您在AWS上，则需要–node-name在此请求中显式传递参数。否则，您将收到如下错误：Unable to register node “ip-10-0-8-141.ec2.internal” with API server: nodes “ip-10-0-8-141.ec2.internal” is forbidden: node ip-10-0-8-141 cannot modify node ip-10-0-8-141.ec2.internal在您的日志中sudo journalctl -u kubelet –all | tail，主节点将报告Not Ready您运行时的状态kubectl get nodes。请务必替换传入的值–apiserver-advertise-address并–node-name使用正确的环境值。 123456#虽然我环境为aws，但是改了静态主机名，所以并不需要指定--node-name，下面为:$ sudo kubeadm alpha phase kubeconfig all --apiserver-advertise-address 10.0.8.141 --node-name ip-10-0-8-141.ec2.internal[kubeconfig] Wrote KubeConfig file to disk: &quot;admin.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;kubelet.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;controller-manager.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;scheduler.conf&quot; 2.5 确保您kubectl正在寻找配置文件的正确位置12345$ mv .kube/config .kube/config.old$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config$ sudo chmod 777 $HOME/.kube/config$ export KUBECONFIG=.kube/config 2.6 重新启动主节点1reboot 2.7 重新连接到主节点并获取令牌，并验证主节点是否“就绪”。将令牌复制到剪贴板。您将在下一步中使用它12345$ kubectl get nodes#说明：若从节点失联，可使用token重新把从节点加入到集群中#获取token命令：$ kubeadm token list 如果获取不到token，可以重新创建一个 1$ kubeadm token create 2.8 SSH到每个从属节点并重新连接到主节点 如果master节点kubeadm进行了升级，那么从节点kubeadm工具也应升级。 1$ sudo kubeadm join --token=&lt;token from step 8&gt; &lt;ip of master node&gt;:&lt;port used 6443 is the default&gt; --node-name &lt;should be the same one as from step 5&gt; 2.9 对每个连接节点重复步骤2.8。从主节点，您可以验证所有从属节点是否已连接并准备好1$ kubectl get nodes 第二种方式（上一种有些参数已经弃用）查看证书过期时间1kubeadm alpha certs check-expiration 续订过期的证书123kubeadm alpha certs renew apiserverkubeadm alpha certs renew apiserver-kubelet-clientkubeadm alpha certs renew front-proxy-client 接下来生成新kubeconfig文件12345kubeadm alpha kubeconfig user --client-name kubernetes-admin --org system:masters &gt; /etc/kubernetes/admin.confkubeadm alpha kubeconfig user --client-name system:kube-controller-manager &gt; /etc/kubernetes/controller-manager.conf# instead of $(hostname) you may need to pass the name of the master node as in &quot;/etc/kubernetes/kubelet.conf&quot; file.kubeadm alpha kubeconfig user --client-name system:node:$(hostname) --org system:nodes &gt; /etc/kubernetes/kubelet.conf kubeadm alpha kubeconfig user --client-name system:kube-scheduler &gt; /etc/kubernetes/scheduler.conf 复制新kubernetes-admin kubeconfig文件：1cp /etc/kubernetes/admin.conf ~/.kube/config 最后，你需要重新启动：kube-apiserver，kube-controller-manager和kube-scheduler。您可以使用以下命令或仅重新启动主节点：123sudo kill -s SIGHUP $(pidof kube-apiserver)sudo kill -s SIGHUP $(pidof kube-controller-manager)sudo kill -s SIGHUP $(pidof kube-scheduler)","categories":[{"name":"工作随笔，问题排查","slug":"工作随笔，问题排查","permalink":"https://kkabuzs.github.io/categories/%E5%B7%A5%E4%BD%9C%E9%9A%8F%E7%AC%94%EF%BC%8C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kkabuzs.github.io/tags/Kubernetes/"}]},{"title":"Centos 7 Hadoop单节点快速构建","slug":"Centos-7-Hadoop-danjiediankuaisugoujian","date":"2019-07-02T06:02:01.000Z","updated":"2019-07-02T06:02:01.000Z","comments":true,"path":"articles/2019/07/02/Centos-7-Hadoop-danjiediankuaisugoujian/","permalink":"https://kkabuzs.github.io/articles/2019/07/02/Centos-7-Hadoop-danjiediankuaisugoujian/","excerpt":"","text":"一，快速构建Hadoop单节点 无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 环境介绍 主机名 ip 描述 hadoop1 192.168.101.83 hadoop部署 1.1 下载所需安装包 jdk下载地址：https://github.com/frekele/oracle-java/releaseshadoop下载地址：http://mirror.bit.edu.cn/apache/hadoop/common/ 123#安装包已经下载完毕[root@hadoop1 ~]# lshadoop-2.9.2.tar.gz jdk-8u181-linux-x64.tar.gz 1.2 安装java jdk12345678910111213141516[root@hadoop1 ~]# tar xf jdk-8u181-linux-x64.tar.gz -C /usr/local/[root@hadoop1 ~]# ln -s /usr/local/jdk1.8.0_181 /usr/local/jdk#配置java环境变量[root@hadoop1 ~]# sed -i.ori &#x27;$a export JAVA_HOME=/usr/local/jdk\\nexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH\\nexport CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar&#x27; /etc/profile[root@hadoop1 ~]# tail -3 /etc/profileexport JAVA_HOME=/usr/local/jdkexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATHexport CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar[root@hadoop1 ~]# source /etc/profile[root@hadoop1 ~]# which java/usr/local/jdk/bin/java[root@hadoop1 ~]# java -versionjava version &quot;1.8.0_181&quot;Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode) 1.3 安装hadoop1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#解压软件包[root@hadoop1 ~]# tar xf hadoop-2.9.2.tar.gz -C /usr/local/[root@hadoop1 ~]# mv /usr/local/hadoop-2.9.2 /usr/local/hadoop#创建用户[root@hadoop1 ~]# useradd hadoop[root@hadoop1 ~]# passwd hadoop更改用户 hadoop 的密码 。新的 密码：无效的密码： 密码少于 8 个字符重新输入新的 密码：passwd：所有的身份验证令牌已经成功更新。[root@hadoop1 ~]# chown -R hadoop:hadoop /usr/local/hadoop[root@hadoop1 ~]# su - hadoop#根据实际情况修改环境变量[hadoop@hadoop1 ~]$ cd /usr/local/hadoop/[hadoop@hadoop1 ~]$ cat .bash_profile# .bash_profile# Get the aliases and functionsif [ -f ~/.bashrc ]; then . ~/.bashrcfi# User specific environment and startup programsPATH=$PATH:$HOME/.local/bin:$HOME/binexport PATH#添加下面内容export JAVA_HOME=/usr/local/jdkexport PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jarexport HADOOP_HOME=/usr/local/hadoopexport HADOOP_INSTALL=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin[hadoop@hadoop1 hadoop]$ source ~/.bash_profile#编辑hadoop-env.sh文件[hadoop@hadoop1 hadoop]$ cd $HADOOP_HOME/etc/hadoop[hadoop@hadoop1 hadoop]$ lscapacity-scheduler.xml hadoop-env.cmd hadoop-policy.xml httpfs-signature.secret kms-log4j.properties mapred-env.sh ssl-client.xml.example yarn-site.xmlconfiguration.xsl hadoop-env.sh hdfs-site.xml httpfs-site.xml kms-site.xml mapred-queues.xml.template ssl-server.xml.examplecontainer-executor.cfg hadoop-metrics2.properties httpfs-env.sh kms-acls.xml log4j.properties mapred-site.xml.template yarn-env.cmdcore-site.xml hadoop-metrics.properties httpfs-log4j.properties kms-env.sh mapred-env.cmd slaves yarn-env.sh[hadoop@hadoop1 hadoop]$ pwd/usr/local/hadoop/etc/hadoop[hadoop@hadoop1 hadoop]$ vim hadoop-env.sh#找到如下一行export JAVA_HOME=$&#123;JAVA_HOME&#125;#改为export JAVA_HOME=/usr/local/jdk #路径为java jdk安装位置 123456789101112#编辑core-site.xml文件：[hadoop@hadoop1 hadoop]$ vim core-site.xml&lt;configuration&gt;&lt;/configuration&gt;#改为&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt;&lt;value&gt;hdfs://localhost:9001&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 1234567891011121314151617181920#编辑hdfs-site.xml文件：[hadoop@hadoop1 hadoop]$ vim hdfs-site.xml#configuration中的内容：&lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt;&lt;value&gt;file:///usr/local/hadoop/hadoopdata/namenode&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt;&lt;value&gt;file:///usr/local/hadoop/hadoopdata/datanode&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 创建目录： 123456789101112131415[hadoop@hadoop1 hadoop]$ mkdir -p /usr/local/hadoop/hadoopdata/namenode[hadoop@hadoop1 hadoop]$ mkdir -p /usr/local/hadoop/hadoopdata/datanode#创建mapred-site.xml文件：[hadoop@hadoop1 hadoop]$ cp mapred-site.xml.template mapred-site.xml#编辑mapred-site.xml文件：[hadoop@hadoop1 hadoop]$ vim mapred-site.xml#configuration中的内容：&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 1234567891011#编辑yarn-site.xml文件：[hadoop@hadoop1 hadoop]$ vim yarn-site.xml#configuration中的内容：&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 上面配置了Hadoop单节点. 1.4 初始化HDFS文件系统1234567891011121314151617[hadoop@hadoop1 hadoop]$ hdfs namenode -format19/07/02 11:23:39 INFO namenode.NameNode: STARTUP_MSG:/************************************************************STARTUP_MSG: Starting NameNodeSTARTUP_MSG: host = k8s-master/192.168.101.83STARTUP_MSG: args = [-format]STARTUP_MSG: version = 2.9.2...省略若干内容...19/07/02 11:23:39 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1662259864-192.168.101.83-156203781974819/07/02 11:23:39 INFO common.Storage: Storage directory /usr/local/hadoop/hadoopdata/namenode has been successfully formatted.19/07/02 11:23:39 INFO namenode.FSImageFormatProtobuf: Saving image file /usr/local/hadoop/hadoopdata/namenode/current/fsimage.ckpt_0000000000000000000 using no compression19/07/02 11:23:39 INFO namenode.FSImageFormatProtobuf: Image file /usr/local/hadoop/hadoopdata/namenode/current/fsimage.ckpt_0000000000000000000 of size 325 bytes saved in 0 seconds .19/07/02 11:23:39 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 019/07/02 11:23:39 INFO namenode.NameNode: SHUTDOWN_MSG:/************************************************************SHUTDOWN_MSG: Shutting down NameNode at k8s-master/192.168.101.83************************************************************/ 1.5 Hadoop相关的执行脚本123456789101112131415161718192021222324252627282930[hadoop@hadoop1 hadoop]$ lsbin etc hadoopdata include lib libexec LICENSE.txt NOTICE.txt README.txt sbin share[hadoop@hadoop1 hadoop]$ cd sbin/[hadoop@hadoop1 sbin]$ lsdistribute-exclude.sh hadoop-daemons.sh httpfs.sh refresh-namenodes.sh start-all.sh start-dfs.sh start-yarn.sh stop-balancer.sh stop-secure-dns.sh yarn-daemon.shFederationStateStore hdfs-config.cmd kms.sh slaves.sh start-balancer.sh start-secure-dns.sh stop-all.cmd stop-dfs.cmd stop-yarn.cmd yarn-daemons.shhadoop-daemon.sh hdfs-config.sh mr-jobhistory-daemon.sh start-all.cmd start-dfs.cmd start-yarn.cmd stop-all.sh stop-dfs.sh stop-yarn.sh[hadoop@hadoop1 sbin]$ start-dfs.sh #执行脚本，因没有配置ssh免密码，所以多次输入密码19/07/02 11:25:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableStarting namenodes on [localhost]The authenticity of host &#x27;localhost (::1)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:/Vg2mVBOSm3b6SxWyslY0VYOTIqFafATNeHzSlaECpM.ECDSA key fingerprint is MD5:c6:bf:1e:3f:8d:c5:39:f4:57:3e:49:6b:0f:aa:ba:c0.Are you sure you want to continue connecting (yes/no)? yeslocalhost: Warning: Permanently added &#x27;localhost&#x27; (ECDSA) to the list of known hosts.hadoop@localhost&#x27;s password:localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-hadoop1.outhadoop@localhost&#x27;s password:localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-hadoop1.outStarting secondary namenodes [0.0.0.0]The authenticity of host &#x27;0.0.0.0 (0.0.0.0)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:/Vg2mVBOSm3b6SxWyslY0VYOTIqFafATNeHzSlaECpM.ECDSA key fingerprint is MD5:c6:bf:1e:3f:8d:c5:39:f4:57:3e:49:6b:0f:aa:ba:c0.Are you sure you want to continue connecting (yes/no)? yes0.0.0.0: Warning: Permanently added &#x27;0.0.0.0&#x27; (ECDSA) to the list of known hosts.hadoop@0.0.0.0&#x27;s password:0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-secondarynamenode-hadoop1.out19/07/02 11:26:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 12345[hadoop@hadoop1 sbin]$ start-yarn.sh #执行脚本starting yarn daemonsstarting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-resourcemanager-hadoop1.outhadoop@localhost&#x27;s password:localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-hadoop1.out 12345678#查看进程[hadoop@hadoop1 sbin]$ jps736 DataNode1136 ResourceManager1555 Jps582 NameNode1430 NodeManager967 SecondaryNameNode 二，测试 默认情况下，Hadoop namenode服务使用50070端口： Hadoop All Applications，访问： 查看NodeManager信息：","categories":[{"name":"大数据","slug":"大数据","permalink":"https://kkabuzs.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://kkabuzs.github.io/tags/Hadoop/"}]},{"title":"CentOS 7 使用kubeadm部署kubernetes 1.14.1集群","slug":"centos7shiyongkubeadmbushukubernetes1-14-1jiqun","date":"2019-07-01T06:25:05.000Z","updated":"2019-07-01T06:25:05.000Z","comments":true,"path":"articles/2019/07/01/centos7shiyongkubeadmbushukubernetes1-14-1jiqun/","permalink":"https://kkabuzs.github.io/articles/2019/07/01/centos7shiyongkubeadmbushukubernetes1-14-1jiqun/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 CentOS 7 使用kubeadm部署kubernetes 1.14.1集群一，主机清单 主机名 ip 描述 版本 gago-k8s-master 192.168.8.77 master节点 v1.14.1 gago-k8s-node1 192.168.8.174 node节点 v1.14.1 gago-k8s-node2 192.168.8.227 node节点 v1.14.1 gago-k8s-node3 192.168.8.84 node节点 v1.14.1 二，版本信息1234567# Linux发行版信息[root@gago-k8s-master ~]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) # Linux内核[root@gago-k8s-master ~]# uname -r3.10.0-327.18.2.el7.x86_64 2.1 系统参数设置1234567891011121314151617181920212223242526272829303132333435363738394041# 所有节点设置/etc/hosts主机名，请根据实际情况进行配置[root@gago-k8s-master ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.8.77 gago-k8s-master192.168.8.174 gago-k8s-node1192.168.8.227 gago-k8s-node2192.168.8.84 gago-k8s-node3# 在所有节点上设置SELINUX为disabled模式[root@gago-k8s-master ~]# vi /etc/selinux/configSELINUX=disabled[root@gago-k8s-master ~]# setenforce 0# 在所有节点上设置iptables参数[root@gago-k8s-master ~]# cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF# 让配置生效[root@gago-k8s-master ~]# sysctl --system# 在所有节点上禁用swap[root@gago-k8s-master ~]# swapoff -a# 禁用fstab中的swap项目[root@gago-k8s-master ~]# vi /etc/fstab#/dev/mapper/centos-swap swap swap defaults 0 0或者[root@gago-k8s-master ~]# cat /etc/rc.d/rc.local /sbin/swapoff -a# 确认swap已经被禁用[root@gago-k8s-master ~]# cat /proc/swapsFilename Type Size Used Priority# 重启主机$ reboot 注意，安装kubernetes集群，要求每个节点至少有2个cpu。 三，安装组件3.1 安装docker12345678910111213#安装依赖包[root@gago-k8s-master ~]# yum -y install yum-utils device-mapper-persistent-data lvm2#添加docker的CE版本的yum源配置文件[root@gago-k8s-master ~]# yum-config-manager \\--add-repo \\https://download.docker.com/linux/centos/docker-ce.repo#安装CE版本的docker[root@gago-k8s-master ~]# yum -y install docker-ce[root@gago-k8s-master ~]# systemctl start docker #启动docker[root@gago-k8s-master ~]# systemctl enable docker docker配置(所有节点) 123456789#创建文件/etc/docker/daemon.json, 写入下面的内容。[root@gago-k8s-master ~]# cat /etc/docker/daemon.json &#123; &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;#重启docker[root@gago-k8s-master ~]# systemctl daemon-reload[root@gago-k8s-master ~]# systemctl restart docker 3.2 kubernetes管理软件安装12345678910#设置kubernetes安装yum源[root@gago-k8s-master ~]# cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 安装kubernetes 12[root@gago-k8s-master ~]# yum install -y kubeadm-1.14.1-0.x86_64 kubelet-1.14.1-0.x86_64 kubectl-1.14.1-0.x86_64[root@gago-k8s-master ~]# systemctl enable --now kubelet 说明：若遇到错误：软件包：kubelet-1.14.0-0.x86_64 (kubernetes) 需要：kubernetes-cni &#x3D; 0.7.5现象： 安装kubernetes 1.14.0 版本时提示缺少依赖 解决办法 1234567891011121314151617181920212223241、安装基础软件yum -y install yum-utils device-mapper-persistent-data lvm22、配置YUM仓库yum-config-manager --add-repo http://mirrors.163.com/.help/CentOS7-Base-163.repoyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo3、配置阿里云复制代码cat &lt;&lt; EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0EOF复制代码4、直接单独安装 yum -y install kubernetes-cni = 0.7.5 3.3 拉取镜像（所有机器）12345678910111213141516171819202122232425262728293031323334353637docker pull mirrorgooglecontainers/kube-apiserver:v1.14.1docker pull mirrorgooglecontainers/kube-controller-manager:v1.14.1docker pull mirrorgooglecontainers/kube-scheduler:v1.14.1docker pull mirrorgooglecontainers/kube-proxy:v1.14.1docker pull mirrorgooglecontainers/pause:3.1docker pull mirrorgooglecontainers/etcd:3.3.10docker pull coredns/coredns:1.3.1#修改tagdocker tag mirrorgooglecontainers/kube-apiserver:v1.14.1 k8s.gcr.io/kube-apiserver:v1.14.1docker tag mirrorgooglecontainers/kube-controller-manager:v1.14.1 k8s.gcr.io/kube-controller-manager:v1.14.1docker tag mirrorgooglecontainers/kube-scheduler:v1.14.1 k8s.gcr.io/kube-scheduler:v1.14.1docker tag mirrorgooglecontainers/kube-proxy:v1.14.1 k8s.gcr.io/kube-proxy:v1.14.1docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1docker tag mirrorgooglecontainers/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10docker tag coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1#删除无用镜像docker rmi mirrorgooglecontainers/kube-apiserver:v1.14.1docker rmi mirrorgooglecontainers/kube-controller-manager:v1.14.1docker rmi mirrorgooglecontainers/kube-scheduler:v1.14.1docker rmi mirrorgooglecontainers/kube-proxy:v1.14.1docker rmi mirrorgooglecontainers/pause:3.1docker rmi mirrorgooglecontainers/etcd:3.3.10docker rmi coredns/coredns:1.3.1#查看镜像[root@gago-k8s-master ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEk8s.gcr.io/kube-proxy v1.14.1 20a2d7035165 2 months ago 82.1MBk8s.gcr.io/kube-apiserver v1.14.1 cfaa4ad74c37 2 months ago 210MBk8s.gcr.io/kube-controller-manager v1.14.1 efb3887b411d 2 months ago 158MBk8s.gcr.io/kube-scheduler v1.14.1 8931473d5bdb 2 months ago 81.6MBquay.io/coreos/flannel v0.11.0-amd64 ff281650a721 5 months ago 52.5MBk8s.gcr.io/coredns 1.3.1 eb516548c180 5 months ago 40.3MBk8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MBk8s.gcr.io/pause 3.1 da86e6ba6ca1 18 months ago 742kB 3.4 集群初始化12345678910# 初始化Master（Master需要至少2核）[root@gago-k8s-master ~]# kubeadm init --apiserver-advertise-address 192.168.8.77 --pod-network-cidr 10.244.0.0/16 --kubernetes-version 1.14.1# 输入提示的内容[root@gago-k8s-master ~]# mkdir -p $HOME/.kube[root@gago-k8s-master ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config[root@gago-k8s-master ~]# chown $(id -u):$(id -g) $HOME/.kube/config#初始化有一段join的token，很重要，是添加node节点的命令 3.5 应用flannel网络1[root@gago-k8s-master ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml 3.6 node节点加入集群12345678910111213141516#所有加入集群的node都要执行[root@gago-k8s-node1 ~]# kubeadm join 192.168.8.77:6443 --token dq2d1p.ibp5pavvo9yqorxi --discovery-token-ca-cert-hash sha256:849662fef6a50ef6ea7020138907f83336a559b8a8e79fa748b55e6fa5bbaafe[preflight] Running pre-flight checks[preflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with &#x27;kubectl -n kube-system get cm kubeadm-config -oyaml&#x27;[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.14&quot; ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet-start] Activating the kubelet service[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run &#x27;kubectl get nodes&#x27; on the control-plane to see this node join the cluster. 3.7 查看集群123456789101112131415161718192021222324#master端执行命令[root@gago-k8s-master ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONgago-k8s-master Ready master 5d23h v1.14.1gago-k8s-node1 Ready &lt;none&gt; 5d23h v1.14.1gago-k8s-node2 Ready &lt;none&gt; 5d23h v1.14.1gago-k8s-node3 Ready &lt;none&gt; 5d23h v1.14.1[root@gago-k8s-master ~]# kubectl get pod --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-fb8b8dccf-dn9f2 1/1 Running 0 5d23hkube-system coredns-fb8b8dccf-sfktk 1/1 Running 0 5d23hkube-system etcd-gago-k8s-master 1/1 Running 0 5d23hkube-system kube-apiserver-gago-k8s-master 1/1 Running 0 5d23hkube-system kube-controller-manager-gago-k8s-master 1/1 Running 0 5d23hkube-system kube-flannel-ds-amd64-658fg 1/1 Running 0 5d23hkube-system kube-flannel-ds-amd64-695k2 1/1 Running 0 5d23hkube-system kube-flannel-ds-amd64-hmxp5 1/1 Running 0 5d23hkube-system kube-flannel-ds-amd64-j9f9w 1/1 Running 0 5d23hkube-system kube-proxy-7krvm 1/1 Running 0 5d23hkube-system kube-proxy-95qpq 1/1 Running 0 5d23hkube-system kube-proxy-mfjtg 1/1 Running 0 5d23hkube-system kube-proxy-xgbzv 1/1 Running 0 5d23hkube-system kube-scheduler-gago-k8s-master 1/1 Running 0 5d23h 至此，搭建完毕！！！","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kkabuzs.github.io/tags/Kubernetes/"}]},{"title":"系统初始化之记录登录日志","slug":"system-initialization-jiludenglurizhi","date":"2019-06-24T02:05:14.000Z","updated":"2019-06-24T02:05:14.000Z","comments":true,"path":"articles/2019/06/24/system-initialization-jiludenglurizhi/","permalink":"https://kkabuzs.github.io/articles/2019/06/24/system-initialization-jiludenglurizhi/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 在系统初始化的过程中，需要将登录日志记录到一个隐藏的文件中,下面简单的记录下流程: 123456#配置如下：vim /etc/profileexport HISTTIMEFORMAT=&quot;&#123;\\&quot;TIME\\&quot;:\\&quot;%F %T\\&quot;,\\&quot;HOST\\&quot;:\\&quot;$HOSTNAME\\&quot;,\\&quot;LI\\&quot;:\\&quot;$(who -u am i 2&gt;/dev/null| awk &#x27;&#123;print $NF&#125;&#x27;|sed -e &#x27;s/[()]//g&#x27;)\\&quot;,\\&quot;LU\\&quot;:\\&quot;$(who am i|awk &#x27;&#123;print $1&#125;&#x27;)\\&quot;,\\&quot;NU\\&quot;:\\&quot;$&#123;USER&#125;\\&quot;,\\&quot;CMD\\&quot;:\\&quot;&quot;export PROMPT_COMMAND=&#x27;history 1|tail -1|sed &quot;s/^[ ]\\+[0-9]\\+ //&quot;|sed &quot;s/$/\\&quot;&#125;/&quot;&gt;&gt; /var/log/.command.log&#x27;source /etc/profile 将如下两行追加到文件中，之后source，这时候我们的日志文件(&#x2F;var&#x2F;log&#x2F;.command.log)就有了登录的日志了。此时需要将这个隐藏的目录权限修改为777 1chmod 777 /var/log/.command.log","categories":[{"name":"工作随笔，问题排查","slug":"工作随笔，问题排查","permalink":"https://kkabuzs.github.io/categories/%E5%B7%A5%E4%BD%9C%E9%9A%8F%E7%AC%94%EF%BC%8C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://kkabuzs.github.io/tags/Linux/"}]},{"title":"Kubernetes之Ingress实现","slug":"kuberneteszhiingressshixian","date":"2019-06-24T01:17:18.000Z","updated":"2019-06-24T01:17:18.000Z","comments":true,"path":"articles/2019/06/24/kuberneteszhiingressshixian/","permalink":"https://kkabuzs.github.io/articles/2019/06/24/kuberneteszhiingressshixian/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Kubernetes之Ingress实现一，什么是Ingress？ Ingress将集群外部的HTTP和HTTPS路由暴露给集群中的服务。流量路由由Ingress资源上定义的规则控制。 12345 internet |[ Ingress ]--|-----|--[ Services ] 1.1 理解service和ingress service是后端真实服务的抽象 一个service可以代表多个相同的后端服务 ingress是反向代理规则，用来规定http&#x2F;https请求应该被转发到哪个service上 比如根据请求不同的host和url路径让请求落到不同的service上 ingress controller 就是一个反向代理程序，它负责解析ingress的反向代理规则 如果ingress有增删改的变动 所有的ingress controller都会即使更新自己相应的转发规则 当ingress controller 收到请求后就会根据这些规则将请求到对应的service kubernetes并没有自带ingress controller 他只是一种标准，具体实现方式有多种，需要自己单独安装，常用的是 nginx ingress controller 和 traefik ingress controller 所以 ingress是一种转发规则的抽象 ingress controller 的实现需要根据这些ingress规则来将请求转发到对应的service 从图中可以看出 ingress controller收到请求，匹配ingress转发规则，匹配到了就转发到后端的service 而service可能代表后端的pod有多个。选出一个转发到那个pod 最终由那个pod处理请求。 既然ingress controller 要接受外面的请求 而ingress controller是部署在集群中的 怎么让ingress controller本身能够被外面访问到呢？大概有如下几种方式： ingress controller用deployment方式部署，给他添加一个service 类型为loadBalancer 这样就会自动生成一个ip地址，通过这个ip即可访问 并且一般这个ip是vip是高可用的，但是前提是集群需要支持loadbalancer 通常情况云服务提供商才支持，自建集群一般没有。 使用集群内的某个节点作为边缘节点，给node添加label来标识，ingress controller用daemonset的方式部署，使用nodeselector绑定到边缘节点 保证每个边缘节点启动一个ingress controller实例，用hostport 直接在这些边缘节点宿主机暴露端口，然后我们可以访问边缘节点中ingress controller暴露的端口。这样外部就可以访问到ingress controller了。 ingress controller用deployment的方式部署，给他添加一个service 类型为nodeport 部署完后查看会给出一个端口，通过kubectl get svc 我们可以查看到这个端口。这个端口在集群的每个节点都可以访问，通过访问集群几点的端口就可以访问ingress controller了。但是集群节点这么多 而且端口不仅仅是80或者443，一般我们需要在前方搭建个负载均衡 比如用nginx，将请求转发到集群各个节点的对应端口上，这样我们访问nginx就相当于访问 ingress controller了。 二，安装ingress controller核心附件2.1 创建ingress命名空间123456789101112131415161718vim namespaces.yamlapiVersion: v1kind: Namespacemetadata: name: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx #因为ingress必须创建在单独的名称空间 所以首先创建名称空间[root@k8s-master01 ingress]# kubectl create -f namespaces.yamlnamespace/ingress-nginx created #指定清单文件进行名称空间创建[root@k8s-master01 ingress]# kubectl get nsNAME STATUS AGEdefault Active 66dingress-nginx Active 22skube-public Active 66dkube-system Active 66d 2.2 编写configmap文件12345678910111213141516171819202122232425262728293031323334vim configmap.yamlkind: ConfigMapapiVersion: v1metadata: name: nginx-configuration namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---kind: ConfigMapapiVersion: v1metadata: name: tcp-services namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---kind: ConfigMapapiVersion: v1metadata: name: udp-services namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx #创建configmap[root@k8s-master01 ingress]# kubectl create -f configmap.yamlconfigmap/nginx-configuration createdconfigmap/tcp-services createdconfigmap/udp-services created 2.3 编写rbac的yaml文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150vim rbac.yamlapiVersion: v1kind: ServiceAccountmetadata: name: nginx-ingress-serviceaccount namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: nginx-ingress-clusterrole labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxrules: - apiGroups: - &quot;&quot; resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes verbs: - get - apiGroups: - &quot;&quot; resources: - services verbs: - get - list - watch - apiGroups: - &quot;extensions&quot; resources: - ingresses verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch - apiGroups: - &quot;extensions&quot; resources: - ingresses/status verbs: - update---apiVersion: rbac.authorization.k8s.io/v1beta1kind: Rolemetadata: name: nginx-ingress-role namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxrules: - apiGroups: - &quot;&quot; resources: - configmaps - pods - secrets - namespaces verbs: - get - apiGroups: - &quot;&quot; resources: - configmaps resourceNames: # Defaults to &quot;&lt;election-id&gt;-&lt;ingress-class&gt;&quot; # Here: &quot;&lt;ingress-controller-leader&gt;-&lt;nginx&gt;&quot; # This has to be adapted if you change either parameter # when launching the nginx-ingress-controller. - &quot;ingress-controller-leader-nginx&quot; verbs: - get - update - apiGroups: - &quot;&quot; resources: - configmaps verbs: - create - apiGroups: - &quot;&quot; resources: - endpoints verbs: - get---apiVersion: rbac.authorization.k8s.io/v1beta1kind: RoleBindingmetadata: name: nginx-ingress-role-nisa-binding namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: nginx-ingress-rolesubjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: ingress-nginx---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: nginx-ingress-clusterrole-nisa-binding labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: nginx-ingress-clusterrolesubjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: ingress-nginx#创建rbac.yaml[root@k8s-master01 ingress]# kubectl create -f rbac.yamlserviceaccount/nginx-ingress-serviceaccount createdclusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole createdrole.rbac.authorization.k8s.io/nginx-ingress-role createdrolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding createdclusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created 2.4 编写nginx-ingress-controller.yaml文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384vim nginx-ingress-controller.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: prometheus.io/port: &quot;10254&quot; prometheus.io/scrape: &quot;true&quot; spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true #启动宿主机网络栈（意味着占用宿主机端口来实现外部访问） nodeName: gago-k8s-node1 #指定pod强制分配给此节点 containers: - name: nginx-ingress-controller image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.24.1 args: - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io securityContext: allowPrivilegeEscalation: true capabilities: drop: - ALL add: - NET_BIND_SERVICE # www-data -&gt; 33 runAsUser: 33 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 - name: https containerPort: 443 livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10 #创建nginx-ingress-controller.yaml[root@k8s-master01 ingress]# kubectl create -f nginx-ingress-controller.yamldeployment.apps/nginx-ingress-controller created 2.5 创建ingress controller的service 为了使ingress controller可以接入集群外部流量，现在为ingress controller创建一个service 基于nodePort的service 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455vim ingress-service.yamlapiVersion: v1kind: Servicemetadata: name: ingress-fengkong namespace: ingress-nginxspec: selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx ports: - name: http port: 80 targetPort: 80 - name: https port: 443 targetPort: 443 #创建ingress-service.yamlkubectl create -f ingress-service.yaml#现在其实我们的ingress controller其实就已经是可以从外部访问了 只不过我们暂时还没有配置它的后端服务 所以现在访问可以看出他是404的一个页面 但是nginx已经体现出来了[root@gago-k8s-master ingress]# kubectl get svc --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 21hingress-nginx ingress-service ClusterIP 10.106.87.227 &lt;none&gt; 80/TCP,443/TCP 3skube-system kube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 21h[root@gago-k8s-master ingress]# curl 10.106.87.227 80&lt;html&gt;&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx/1.15.10&lt;/center&gt;&lt;/body&gt;&lt;/html&gt;curl: (7) Failed to connect to 0.0.0.80: Invalid argument[root@gago-k8s-master ingress]# kubectl get nodeNAME STATUS ROLES AGE VERSIONgago-k8s-master Ready master 21h v1.14.1gago-k8s-node1 Ready &lt;none&gt; 21h v1.14.1gago-k8s-node2 Ready &lt;none&gt; 21h v1.14.1gago-k8s-node3 Ready &lt;none&gt; 21h v1.14.1[root@gago-k8s-master ingress]# curl 192.168.8.174 80 #因为前面配置了启动宿主机网络栈&lt;html&gt;&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx/1.15.10&lt;/center&gt;&lt;/body&gt;&lt;/html&gt;curl: (7) Failed to connect to 0.0.0.80: Invalid argument 2.6 配置ingress 创建一个ingress将它关联到相关的运行后端服务的相关pod的service上边 然后使它能够动态的为ingress controller提供后端pod 12345678910111213141516vim ingress-test.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-test namespace: default annotations: kubernetes.io/ingress.class: &quot;nginx&quot;spec: rules: - host: test.gagogroup.com http: paths: - backend: serviceName: test servicePort: 9090 三，配置ingress 配置https访问 首先生成tls文件，因为我这里有购买到的第三方证书，所以可以直接使用，如果没有的话，需要自行生成证书。https://mritd.me/2017/03/04/how-to-use-nginx-ingress/，证书生成查看此篇文章。 123456#现在我的key文件为gago.key，cert文件为gago.crt。下面可以直接生成kubectl create secret tls ingress-secret --key /etc/kubernetes/pki/gago.key --cert /etc/kubernetes/pki/gago.crt --namespace kube-system这样就在kube-system这个空间中生成一个叫做ingress-secret的secret了。如果我们的service分配在多个不同的namespace，需要我们创建几个不同的secret，名字不用改变。只需要更改后面的namespace名字就可以了。 开始配置ingress文件 123456789101112131415161718192021222324252627282930#直接将加上tls字段，然后配置上域名就可以了。之后进行应用。apiVersion: extensions/v1beta1kind: Ingressmetadata: name: testnginx namespace: default annotations: ingress.kubernetes.io/force-ssl-redirect: &quot;false&quot; spec: tls: - hosts: - nginx.gagogroup.cn - jenkins.gagogroup.cn - &quot;*.gagogroup.cn&quot; secretName: ingress-secret rules: - host: nginx.gagogroup.cn http: paths: - path: / backend: serviceName: webapp servicePort: 8800 - host: jenkins.gagogroup.cn http: paths: - path: / backend: serviceName: jenkins servicePort: 8080 访问的话可以看到已经跳转到https了。这个默认的是80会跳转到443上面去。","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kkabuzs.github.io/tags/Kubernetes/"}]},{"title":"解决kubernetes不能从harbor pull image","slug":"jiejue-kubernetes-buneng-harbor-pull-images","date":"2019-06-18T08:52:55.000Z","updated":"2019-06-18T08:52:55.000Z","comments":true,"path":"articles/2019/06/18/jiejue-kubernetes-buneng-harbor-pull-images/","permalink":"https://kkabuzs.github.io/articles/2019/06/18/jiejue-kubernetes-buneng-harbor-pull-images/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 在实现kubernetes ci的时候发现node节点不能从harbor的私有仓库将image直接pull下来。但是直接从node节点直接pull的话又是没有问题的，后来发现如果想要kubernetes直接pull的话需要一个secret。 一，以下面的为例介绍 下面的是创建service以及deployment的例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748apiVersion: v1kind: Namespacemetadata: name: test---apiVersion: v1kind: Servicemetadata: name: webapp namespace: test labels: app: nginxspec: ports: - port: 80 name: webapp selector: app: nginx---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx namespace: test annotations: kubernetes.io/change-cause: &quot;true&quot; labels: app: nginxspec: strategy: rollingUpdate: maxUnavailable: 1 maxSurge: 2 replicas: 1 template: metadata: labels: app: nginx namespace: test spec: containers: - name: nginx image: harbor.gagogroup.cn/api-dev/nginx:1.1.9 imagePullPolicy: IfNotPresent ports: - containerPort: 80 imagePullSecrets: - name: harbor-secret 如果我们想要保证kubernetes能够正常的pull image的话，就需要加上imagePullSecrets这个字段。下面是创建secret的方法。 12345678910kubectl create secret docker-registry SECRET_NAME -–namespace=NAME_SPACE \\-–docker-server=DOCKER_REGISTRY_SERVER -–docker-username=DOCKER_USER \\-–docker-password=DOCKER_PASSWORD -–docker-email=DOCKER_EMAIL#其中：-–namespace 为命名空间--docker-server 为通过docker login登陆时输入的地址--docker-username 为登陆时的账号--docker-password 为登陆时的密码--docker-email 为注册的账号时的邮箱地址 真实命令示例 12345678[root@gago-k8s-master api]# kubectl create secret docker-registry harbor-secret --docker-server=xxx.xxxx.cn --docker-username=yunwei --docker-password=xxxxxx --docker-email=xxx@qq.com -n api secret/harbor-secret created[root@gago-k8s-master api]# kubectl get secret -n api #查看secretNAME TYPE DATA AGEdefault-token-4qgfl kubernetes.io/service-account-token 3 47mharbor-secret kubernetes.io/dockerconfigjson 1 3m52sharbor-yunwei kubernetes.io/dockerconfigjson 1 47m 同时不要忘记的是此用户需要读拉取的image的project有权限。","categories":[{"name":"工作随笔，问题排查","slug":"工作随笔，问题排查","permalink":"https://kkabuzs.github.io/categories/%E5%B7%A5%E4%BD%9C%E9%9A%8F%E7%AC%94%EF%BC%8C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kkabuzs.github.io/tags/Kubernetes/"}]},{"title":"git前端ci文件写法示例","slug":"git-qianduan-ci-example","date":"2019-06-11T03:23:07.000Z","updated":"2019-06-11T03:23:07.000Z","comments":true,"path":"articles/2019/06/11/git-qianduan-ci-example/","permalink":"https://kkabuzs.github.io/articles/2019/06/11/git-qianduan-ci-example/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 12345678910111213#前端ci示例：stages: #有哪些步骤 - picc_ts_testpicc_ts_test: #可以理解为项目名，只是定义一个名字 stage: picc_ts_test #步骤1，跟上面的要对应 tags: # runner名字，决定用哪个runnre来执行 - zhaoshuo_172.16.0.26 only: # 分支名，定义部署分支 - develop script: # 动作 - scp -P 12224 -r dist gago_zhaoshuo@172.17.108.101:/data/home/gago_hetao/web_ci/picc_ts_test/ - ssh -p 12224 gago_zhaoshuo@172.17.108.101 &quot;sudo sh ~/picc_web_ts_ci.sh&quot;","categories":[{"name":"工作随笔，问题排查","slug":"工作随笔，问题排查","permalink":"https://kkabuzs.github.io/categories/%E5%B7%A5%E4%BD%9C%E9%9A%8F%E7%AC%94%EF%BC%8C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"}],"tags":[{"name":"Devops","slug":"Devops","permalink":"https://kkabuzs.github.io/tags/Devops/"}]},{"title":"容器占满了磁盘，但虚机和容器内都没有找到大文件","slug":"rong-qi-zhanman-cipan-fuwuqi-he-rongqi-doumeidawenjian","date":"2019-04-23T07:42:49.000Z","updated":"2019-04-23T07:42:49.000Z","comments":true,"path":"articles/2019/04/23/rong-qi-zhanman-cipan-fuwuqi-he-rongqi-doumeidawenjian/","permalink":"https://kkabuzs.github.io/articles/2019/04/23/rong-qi-zhanman-cipan-fuwuqi-he-rongqi-doumeidawenjian/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 一，原因阐述 领导让我排查一下aws服务器磁盘空间已满的问题，但是看不到大文件的存在，看下图： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#看根下所有目录的空间占用[root@gago-viirs gago_viirs]# cd /[root@gago-viirs /]# du -sh *0 bin49M boot0 data0 dev31M etc52K home0 lib0 lib640 local0 media2.7G mnt104K optdu: 无法访问&quot;proc/12686&quot;: No such file or directorydu: 无法访问&quot;proc/12757/task/12757/fd/3&quot;: No such file or directorydu: 无法访问&quot;proc/12757/task/12757/fdinfo/3&quot;: No such file or directorydu: 无法访问&quot;proc/12757/fd/3&quot;: No such file or directorydu: 无法访问&quot;proc/12757/fdinfo/3&quot;: No such file or directory0 proc12M root664K run0 sbin0 srv0 sys0 tmp1.1G usr9.3G var#看磁盘挂在使用情况[root@gago-viirs /]# df -h文件系统 容量 已用 可用 已用% 挂载点devtmpfs 31G 0 31G 0% /devtmpfs 31G 0 31G 0% /dev/shmtmpfs 31G 664K 31G 1% /runtmpfs 31G 0 31G 0% /sys/fs/cgroup/dev/nvme0n1p1 80G 51G 30G 64% / #已使用51Goverlay 80G 51G 30G 64% /var/lib/docker/overlay2/204265d4f482068252a9dd26de98e19f1d7144a5e90e34f2e8dbec60b5362af7/mergedshm 64M 0 64M 0% /var/lib/docker/containers/a0ef8b7368e7157a1bb92292c43a724ea0894b63f0dc552aef7b7135bf9f1f1c/shmtmpfs 6.2G 0 6.2G 0% /run/user/1001overlay 80G 51G 30G 64% /var/lib/docker/overlay2/52453df85ac0f3d85612c2ca0a5dc1adaaafb420c6e02733d021fd0edb209c7b/mergedshm 64M 0 64M 0% /var/lib/docker/containers/1508613045dbd0338782ee8bac21f830546e5847eb87f24c76172e326d201a6d/shm#查看docker容器磁盘占用情况[root@gago-viirs /]# docker system dfTYPE TOTAL ACTIVE SIZE RECLAIMABLEImages 9 2 5.908 GB 5.575 GB (94%)Containers 2 2 257.6 kB 0 B (0%)Local Volumes 1 1 4.33 MB 0 B (0%)#查看ionde号使用情况[root@gago-viirs /]# df -hi文件系统 Inode 已用(I) 可用(I) 已用(I)% 挂载点devtmpfs 7.7M 381 7.7M 1% /devtmpfs 7.7M 1 7.7M 1% /dev/shmtmpfs 7.7M 517 7.7M 1% /runtmpfs 7.7M 16 7.7M 1% /sys/fs/cgroup/dev/nvme0n1p1 40M 197K 40M 1% /overlay 40M 197K 40M 1% /var/lib/docker/overlay2/204265d4f482068252a9dd26de98e19f1d7144a5e90e34f2e8dbec60b5362af7/mergedshm 7.7M 1 7.7M 1% /var/lib/docker/containers/a0ef8b7368e7157a1bb92292c43a724ea0894b63f0dc552aef7b7135bf9f1f1c/shmtmpfs 7.7M 1 7.7M 1% /run/user/1001overlay 40M 197K 40M 1% /var/lib/docker/overlay2/52453df85ac0f3d85612c2ca0a5dc1adaaafb420c6e02733d021fd0edb209c7b/mergedshm 7.7M 1 7.7M 1% /var/lib/docker/containers/1508613045dbd0338782ee8bac21f830546e5847eb87f24c76172e326d201a6d/shm#查看容器进程[root@gago-viirs /]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1508613045db harbor.gagogroup.cn/mill/gago-viirs:1.0.0 &quot;/bin/sh /opt/proj...&quot; 3 hours ago Up 3 hours gago_viirsa0ef8b7368e7 rabbitmq:3-management &quot;docker-entrypoint...&quot; 5 days ago Up 15 hours 0.0.0.0:4369-&gt;4369/tcp, 0.0.0.0:5671-5672-&gt;5671-5672/tcp, 0.0.0.0:15671-15672-&gt;15671-15672/tcp, 0.0.0.0:25672-&gt;25672/tcp weather_typhoon_rabbitmq#大文件排查[root@gago-viirs /]# du --max-depth=1 -h0 ./dev31M ./etc0 ./localdu: 无法访问&quot;./proc/13486/task/13486/fd/3&quot;: No such file or directorydu: 无法访问&quot;./proc/13486/task/13486/fdinfo/3&quot;: No such file or directorydu: 无法访问&quot;./proc/13486/fd/4&quot;: No such file or directorydu: 无法访问&quot;./proc/13486/fdinfo/4&quot;: No such file or directory0 ./proc676K ./run0 ./sys0 ./tmp9.3G ./var1.1G ./usr49M ./boot52K ./home0 ./media1.9G ./mnt104K ./opt12M ./root0 ./srv0 ./data13G . 随后我又进入到容器里面进行了相应对排查，例如日志，挂载的目录，掩藏文件。皆无收获。但是只要删除gago_viirs容器，空间就得到释放。容器里的程序代码逻辑大致为，下载数据，本地计算，上传aws S3存储桶，本地数据删除。 根据我的深入排查分析，结合容器里跑的程序，排查到了根本原因，如下： 根本原因是：删除的文件仍被某进程打开占用而导致无法真正删除，需要重置相关的进程以释放文件句柄 1234通过lsof |grep -i delete即可查找到相关进程#另外说明：df工具是通过 空间总数-未分配空间=已分配空间 计算出占用空间大小 12345678910111213141516171819202122232425#配上`lsof |grep -i delete`的执行结果[root@gago-viirs /]# lsof |grep -i deletecelery 3068 root 40rR REG 259,3 47775318 88108094 /mnt/resource/77b38840-5b06-49b2-838f-65c534a16fb6/CLDMSK_L2_VIIRS_SNPP.A2018153.0948.001.2019064090928.nc (deleted)celery 3068 root 43rR REG 259,3 56997285 54666830 /mnt/resource/1042adb6-f272-463f-909f-d17f46d4b653/CLDMSK_L2_VIIRS_SNPP.A2018153.0506.001.2019064090800.nc (deleted)celery 3068 root 45rR REG 259,3 53120956 16855884 /mnt/resource/4c7d1b8a-38c8-41d2-83b1-e34a6d741315/CLDMSK_L2_VIIRS_SNPP.A2018153.0324.001.2019064090716.nc (deleted)celery 3068 root 46rR REG 259,3 60397048 54666822 /mnt/resource/19589468-fcc2-4155-8f03-7fbd1d0a411d/CLDMSK_L2_VIIRS_SNPP.A2018153.0000.001.2019064090640.nc (deleted)celery 3068 root 47rR REG 259,3 60403873 83886246 /mnt/resource/87885ecf-6eab-472c-87fa-ecb357ff2193/CLDMSK_L2_VIIRS_SNPP.A2018153.0006.001.2019064090641.nc (deleted)celery 3068 root 48rR REG 259,3 59062322 109052006 /mnt/resource/fbc1f96d-4607-4e49-8676-be7dc98f88db/CLDMSK_L2_VIIRS_SNPP.A2018153.0012.001.2019064090652.nc (deleted)celery 3068 root 49rR REG 259,3 56189540 46149164 /mnt/resource/084e46c0-f3f6-46e2-9dfd-cc7828cc7aa5/CLDMSK_L2_VIIRS_SNPP.A2018153.1318.001.2019064090954.nc (deleted)celery 3068 root 50rR REG 259,3 62916003 37760102 /mnt/resource/404175e8-179e-455d-892d-1af4b05aa3db/CLDMSK_L2_VIIRS_SNPP.A2018153.0130.001.2019064090718.nc (deleted)celery 3068 root 51rR REG 259,3 60900233 142609224 /mnt/resource/d9b870f9-4505-466a-a691-7a986b0bef25/CLDMSK_L2_VIIRS_SNPP.A2018153.0118.001.2019064090714.nc (deleted)celery 3068 root 52rR REG 259,3 55852025 2419657 /mnt/resource/e7eef2e4-52a0-47cb-bf6d-3748d9ab3006/CLDMSK_L2_VIIRS_SNPP.A2018153.0124.001.2019064090715.nc (deleted)celery 3068 root 53rR REG 259,3 58317508 88225496 /mnt/resource/4649b40d-ff75-429b-9483-5d9c9738be6c/CLDMSK_L2_VIIRS_SNPP.A2018153.0136.001.2019064090717.nc (deleted)celery 3068 root 54rR REG 259,3 59929527 130135038 /mnt/resource/6b9f9d10-5eb3-4b29-8df6-e5f7ac466860/CLDMSK_L2_VIIRS_SNPP.A2018153.0142.001.2019064090710.nc (deleted)celery 3068 root 55rR REG 259,3 56194582 159383656 /mnt/resource/34f45694-b2a0-42a4-a720-62a28b8f7fe4/CLDMSK_L2_VIIRS_SNPP.A2018153.0148.001.2019064090708.nc (deleted)celery 3068 root 56rR REG 259,3 60258025 14155641 /mnt/resource/634250af-5cdf-4225-8a8d-797b5cce43cb/CLDMSK_L2_VIIRS_SNPP.A2018153.0154.001.2019064090706.nc (deleted)celery 3068 root 57rR REG 259,3 51452072 67163935 /mnt/resource/624ddd03-c08d-43c9-aa04-dd2b5bb2e080/CLDMSK_L2_VIIRS_SNPP.A2018153.0306.001.2019064090718.nc (deleted)celery 3068 root 58rR REG 259,3 56095178 29497041 /mnt/resource/add8ea9f-c8ff-4dce-9a8f-03f08d2da984/CLDMSK_L2_VIIRS_SNPP.A2018153.0300.001.2019064090726.nc (deleted)celery 3068 root 59rR REG 259,3 57665817 96565225 /mnt/resource/4d34cf0f-bd91-4ac2-8c65-0159914879e1/CLDMSK_L2_VIIRS_SNPP.A2018153.0312.001.2019064090729.nc (deleted)celery 3068 root 60rR REG 259,3 51691131 33589099 /mnt/resource/5fe70fa4-c9e3-44c4-8285-bc1ca44631fe/CLDMSK_L2_VIIRS_SNPP.A2018153.1312.001.2019064090932.nc (deleted)celery 3068 root 61rR REG 259,3 58331092 146805873 /mnt/resource/db617129-b3d7-4bb1-9c9a-31f77e8f3a21/CLDMSK_L2_VIIRS_SNPP.A2018153.0318.001.2019064090720.nc (deleted)celery 3068 root 62rR REG 259,3 53329206 92346925 /mnt/resource/8e66bcb8-bf94-4a7d-9d23-033f6fd677a2/CLDMSK_L2_VIIRS_SNPP.A2018153.0442.001.2019064090715.nc (deleted)celery 3068 root 63rR REG 259,3 56018984 46138411 /mnt/resource/924fe06c-496f-4161-a1e0-308d501eab5b/CLDMSK_L2_VIIRS_SNPP.A2018153.0330.001.2019064090730.nc (deleted)···以下省略若干···","categories":[{"name":"工作随笔，问题排查","slug":"工作随笔，问题排查","permalink":"https://kkabuzs.github.io/categories/%E5%B7%A5%E4%BD%9C%E9%9A%8F%E7%AC%94%EF%BC%8C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://kkabuzs.github.io/tags/Docker/"}]},{"title":"CentOS 7 使用二进制部署 Kubernetes 1.13集群","slug":"centos7erjinzhibushukubernetes1-13jiqun","date":"2019-04-09T02:09:37.000Z","updated":"2019-04-09T02:09:37.000Z","comments":true,"path":"articles/2019/04/09/centos7erjinzhibushukubernetes1-13jiqun/","permalink":"https://kkabuzs.github.io/articles/2019/04/09/centos7erjinzhibushukubernetes1-13jiqun/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 CentOS 7 使用二进制部署 Kubernetes 1.13集群一，概述 Kubernetes是一个开源的，用于管理云平台中多个主机上的容器化的应用，Kubernetes的目标是让部署容器化的应用简单并且高效（powerful）,Kubernetes提供了应用部署，规划，更新，维护的一种机制。 1.1 架构图kubernetes架构图 Flannel网络架构图 数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，这是个P2P的虚拟网卡，flanneld服务监听在网卡的另外一端。 Flannel通过Etcd服务维护了一张节点间的路由表，在稍后的配置部分我们会介绍其中的内容。 源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。 二，二进制部署kubernetes 1.13集群2.1 安装环境准备 主机名 IP地址 CPU 内存 描述 k8s-master01 192.168.101.61 2核 4GB k8s master节点 k8s-node01 192.168.101.62 2核 4GB k8s node节点 k8s-node02 192.168.101.125 2核 4GB k8s node节点 1234567891011121314151617181920212223242526#初始化环境[root@k8s-master01 ~]# cat /etc/redhat-releaseCentOS Linux release 7.5.1804 (Core)[root@k8s-master01 ~]# uname -r3.10.0-862.el7.x86_64[root@k8s-master01 ~]# setenforce 0setenforce: SELinux is disabled[root@k8s-master01 ~]# systemctl stop firewalld[root@k8s-master01 ~]# systemctl disable firewalld#添加hosts映射[root@k8s-master01 ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.101.61 k8s-master01192.168.101.62 k8s-node01192.168.101.125 k8s-node02#关闭swap缓存[root@k8s-master01 ~]# vim /etc/fstab[root@k8s-master01 ~]# cat /etc/fstab | grep swap#UUID=40ab8144-1f1d-48ef-9a00-8ee7e0cfb2db swap swap defaults 0 0[root@k8s-master01 ~]# swapoff -a[root@k8s-master01 ~]# echo &quot;swapoff -a&quot; &gt;&gt; /etc/rc.d/rc.local 2.2 安装docker1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@k8s-master01 ~]# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo已加载插件：fastestmirroradding repo from: https://download.docker.com/linux/centos/docker-ce.repograbbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.reporepo saved to /etc/yum.repos.d/docker-ce.repo[root@k8s-master01 ~]# yum list docker-ce --showduplicates | sort -r已加载插件：fastestmirror已安装的软件包可安装的软件包 * updates: mirrors.huaweicloud.comLoading mirror speeds from cached hostfile * extras: mirror.jdcloud.comdocker-ce.x86_64 3:18.09.4-3.el7 docker-ce-stabledocker-ce.x86_64 3:18.09.3-3.el7 docker-ce-stabledocker-ce.x86_64 3:18.09.2-3.el7 docker-ce-stabledocker-ce.x86_64 3:18.09.1-3.el7 docker-ce-stabledocker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.3.ce-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.2.ce-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stabledocker-ce.x86_64 18.03.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 18.03.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.12.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.12.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.09.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.09.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.06.2.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.06.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.06.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.03.3.ce-1.el7 docker-ce-stabledocker-ce.x86_64 17.03.2.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.03.1.ce-1.el7.centos @docker-ce-stabledocker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable * base: mirrors.huaweicloud.com[root@k8s-master01 ~]# yum -y install docker-ce #安装新版docker[root@k8s-master01 ~]# systemctl start docker[root@k8s-master01 ~]# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.[root@k8s-master01 ~]# docker versionClient: Version: 18.09.4 API version: 1.39 Go version: go1.10.8 Git commit: d14af54266 Built: Wed Mar 27 18:34:51 2019 OS/Arch: linux/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 18.09.4 API version: 1.39 (minimum version 1.12) Go version: go1.10.8 Git commit: d14af54 Built: Wed Mar 27 18:04:46 2019 OS/Arch: linux/amd64 Experimental: false 2.3 创建安装目录12[root@k8s-master01 ~]# mkdir -p /k8s/etcd/&#123;bin,cfg,ssl&#125;[root@k8s-master01 ~]# mkdir -p /k8s/kubernetes/&#123;bin,cfg,ssl&#125; 2.4 安装及配置CFSSL1234567[root@k8s-master01 ~]# wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64[root@k8s-master01 ~]# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64[root@k8s-master01 ~]# wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64[root@k8s-master01 ~]# chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64[root@k8s-master01 ~]# mv cfssl_linux-amd64 /usr/local/bin/cfssl[root@k8s-master01 ~]# mv cfssljson_linux-amd64 /usr/local/bin/cfssljson[root@k8s-master01 ~]# mv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo 2.5 创建认证证书2.5.1 创建 ETCD CA 配置文件123456789101112131415161718192021222324[root@k8s-master01 ~]# cd /k8s/[root@k8s-master01 k8s]# lsetcd kubernetes[root@k8s-master01 k8s]# mkdir etcd_ssl #证书的原json文件目录[root@k8s-master01 k8s]# cd etcd_ssl/[root@k8s-master01 etcd_ssl]# cat ca-config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;www&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125; 2.5.2 创建ETCD CA 证书123456789101112131415[root@k8s-master01 etcd_ssl]# cat ca-csr.json&#123; &quot;CN&quot;: &quot;etcd CA&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot; &#125; ]&#125; 2.5.3 创建ETCD server 证书1234567891011121314151617181920[root@k8s-master01 etcd_ssl]# cat server-csr.json&#123; &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;192.168.101.61&quot;, &quot;192.168.101.62&quot;, &quot;192.168.101.125&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot; &#125; ]&#125; 2.5.4 生成ETCD CA 证书和秘钥1234567891011121314151617[root@k8s-master01 etcd_ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -2019/04/09 16:34:08 [INFO] generating a new CA key and certificate from CSR2019/04/09 16:34:08 [INFO] generate received request2019/04/09 16:34:08 [INFO] received CSR2019/04/09 16:34:08 [INFO] generating key: rsa-20482019/04/09 16:34:08 [INFO] encoded CSR2019/04/09 16:34:08 [INFO] signed certificate with serial number 1858391312899192952195491720058889354936121860[root@k8s-master01 etcd_ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server2019/04/09 16:34:21 [INFO] generate received request2019/04/09 16:34:21 [INFO] received CSR2019/04/09 16:34:21 [INFO] generating key: rsa-20482019/04/09 16:34:21 [INFO] encoded CSR2019/04/09 16:34:21 [INFO] signed certificate with serial number 3432236962510674604672789845172929854363279892842019/04/09 16:34:21 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;). 2.5.5 创建Kubernetes CA 配置文件123456789101112131415161718192021222324252627282930[root@k8s-master01 ~]# cd /k8s/[root@k8s-master01 k8s]# lsetcd etcd_ssl kubernetes[root@k8s-master01 k8s]# mkdir k8s_ssl[root@k8s-master01 k8s]# cd k8s_ssl/[root@k8s-master01 k8s_ssl]# cat ca-config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;字段说明：ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；server auth：表示client可以用该 CA 对server提供的证书进行验证；client auth：表示server可以用该CA对client提供的证书进行验证； 2.5.6 创建Kubernetes CA 证书123456789101112131415161718192021222324252627282930[root@k8s-master01 k8s_ssl]# cat ca-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;字段说明：&quot;CN&quot;：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；&quot;O&quot;：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；#生成kubernetes ca 证书[root@k8s-master01 k8s_ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -2019/04/09 17:49:09 [INFO] generating a new CA key and certificate from CSR2019/04/09 17:49:09 [INFO] generate received request2019/04/09 17:49:09 [INFO] received CSR2019/04/09 17:49:09 [INFO] generating key: rsa-20482019/04/09 17:49:09 [INFO] encoded CSR2019/04/09 17:49:09 [INFO] signed certificate with serial number 555134987760584209046061419310967914967122746147 2.5.7 创建kubernetes（api server）证书123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-master01 k8s_ssl]# cat kubernetes-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;192.168.101.61&quot;, #多master要把所有对master ip都写入 &quot;10.254.0.1&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;说明：如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，由于该证书后续被 etcd 集群和 kubernetes master 集群使用，所以上面分别指定了 etcd 集群、kubernetes master 集群的主机 IP 和 kubernetes 服务的服务 IP（一般是 kube-apiserver 指定的 service-cluster-ip-range 网段的第一个IP，如 10.254.0.1）。#生成api证书[root@k8s-master01 k8s_ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes2019/04/09 18:00:43 [INFO] generate received request2019/04/09 18:00:43 [INFO] received CSR2019/04/09 18:00:43 [INFO] generating key: rsa-20482019/04/09 18:00:43 [INFO] encoded CSR2019/04/09 18:00:43 [INFO] signed certificate with serial number 4570426071439823121880936876565616504510254793212019/04/09 18:00:43 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;). 2.5.8 创建kubernetes proxy 证书123456789101112131415161718192021222324252627282930[root@k8s-master01 k8s_ssl]# cat kube-proxy-csr.json&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;#生成proxy证书：[root@k8s-master01 k8s_ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy2019/04/10 10:01:01 [INFO] generate received request2019/04/10 10:01:01 [INFO] received CSR2019/04/10 10:01:01 [INFO] generating key: rsa-20482019/04/10 10:01:01 [INFO] encoded CSR2019/04/10 10:01:01 [INFO] signed certificate with serial number 519866538184259406425440476980634890931612035322019/04/10 10:01:01 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;). 2.5.9 校验证书 共有两种方法，一种是用openssl命令，另一种是用cfssl-certinfo命令。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125#使用openssl命令校验（以kubernetes证书为例：）[root@k8s-master01 k8s_ssl]# openssl x509 -noout -text -in kubernetes.pemCertificate: Data: Version: 3 (0x2) Serial Number: 50:0e:7f:d2:f5:04:b6:bd:a0:6d:9e:0d:16:61:ac:d1:41:ef:c6:99 Signature Algorithm: sha256WithRSAEncryption Issuer: C=CN, ST=BeiJing, L=BeiJing, O=k8s, OU=System, CN=kubernetes Validity Not Before: Apr 9 09:56:00 2019 GMT Not After : Apr 6 09:56:00 2029 GMT Subject: C=CN, ST=BeiJing, L=BeiJing, O=k8s, OU=System, CN=kubernetes Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:cc:55:52:7d:19:66:58:86:97:6d:3a:0a:b7:7c: b3:a2:16:7c:f5:bf:e1:92:a0:a7:d2:7a:86:e9:6b: 41:8a:22:c0:c7:a7:79:e6:ae:ef:3e:60:c9:64:4d: 8e:10:67:3b:f1:3c:e3:39:8e:f4:5a:65:47:2d:85: fb:31:28:25:1f:bd:f9:5d:9a:81:1b:2b:3a:9b:33: 60:9b:92:07:b7:f3:a7:f9:35:dc:2b:a0:8d:18:29: 6c:52:e2:c6:d7:86:cf:c6:3d:fe:ab:6b:06:37:b3: 37:0e:51:61:1e:16:2d:28:0f:ef:a0:a1:22:4c:b9: 88:ec:c1:94:fd:30:4e:df:4a:3e:f5:b7:bf:c0:72: bc:bf:69:d8:24:db:5e:d2:e4:d0:3d:a4:d3:94:04: 35:ff:f4:8f:95:fd:23:1a:c3:06:0b:5e:73:b4:56: 1b:8e:4c:9b:15:6a:21:ae:65:2d:f7:89:dd:84:8d: 7b:5b:d0:71:ca:b7:cd:c0:cc:05:36:ce:93:3f:84: 5f:f8:a2:1c:86:96:ae:96:0a:46:9c:1f:65:68:e5: 70:cb:de:1c:36:45:70:8b:3c:9e:53:2c:12:c7:fd: e5:99:04:22:8f:16:a1:82:61:8f:89:db:86:3e:a8: 66:b7:54:85:ab:4c:7d:78:c0:e7:78:c4:f5:69:a7: 7c:ad Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Key Identifier: D0:52:1D:C3:61:26:65:43:92:55:9B:77:67:D3:F5:93:E4:BA:AC:0F X509v3 Authority Key Identifier: keyid:21:7A:58:CF:F4:BC:08:88:5D:DF:81:E5:0C:87:42:9C:EE:48:9F:3B X509v3 Subject Alternative Name: DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster, DNS:kubernetes.default.svc.cluster.local, IP Address:127.0.0.1, IP Address:192.168.101.61, IP Address:10.254.0.1 Signature Algorithm: sha256WithRSAEncryption 35:45:b5:af:3e:bd:5a:0d:cf:d2:df:11:72:7a:86:19:b3:55: 30:b1:3f:35:a9:a2:5c:c1:f3:15:ec:b4:5c:27:25:c4:fd:fe: 3f:df:f9:9f:29:df:13:f6:87:07:73:11:73:1b:84:3c:71:69: 37:a0:7c:41:b6:c8:1a:0b:69:8a:da:b1:6e:cb:6c:da:32:49: e6:9a:3b:c7:4b:4c:73:cd:ce:57:20:ca:45:64:9d:46:e9:33: 11:7d:b1:e3:59:49:d8:4a:17:c2:08:76:ed:73:b9:12:b3:d1: 3b:67:31:1a:31:66:a9:3e:aa:8a:0c:fe:b1:01:c1:06:77:25: e2:e0:8c:0f:be:23:9f:fe:70:21:e1:5b:b6:83:75:18:9c:72: f0:f5:e9:fc:20:78:7a:00:7f:ef:9b:8c:5b:85:c9:d6:9f:5f: 15:45:7c:8e:14:81:ba:ee:36:ac:4b:a3:8e:84:5d:6a:31:3b: cb:94:0f:cb:1d:47:e2:69:9d:e4:b3:41:c4:86:31:8b:02:ac: e4:a1:27:af:1b:eb:22:9a:17:c7:4b:96:2f:41:cb:69:87:63: 88:61:84:88:34:bf:1c:f7:65:3a:a6:0f:b4:53:dd:df:c5:f7: 19:c6:ee:81:20:8b:98:6c:4a:94:c0:10:6b:49:16:57:17:f4: 8c:60:d4:93 说明：确认 Issuer 字段的内容和 ca-csr.json 一致；确认 Subject 字段的内容和 kubernetes-csr.json 一致；确认 X509v3 Subject Alternative Name 字段的内容和 kubernetes-csr.json 一致；确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 ca-config.json 中 kubernetes profile 一致；#使用cfssl-certinfo命令校验：[root@k8s-master01 k8s_ssl]# cfssl-certinfo -cert kubernetes.pem&#123; &quot;subject&quot;: &#123; &quot;common_name&quot;: &quot;kubernetes&quot;, &quot;country&quot;: &quot;CN&quot;, &quot;organization&quot;: &quot;k8s&quot;, &quot;organizational_unit&quot;: &quot;System&quot;, &quot;locality&quot;: &quot;BeiJing&quot;, &quot;province&quot;: &quot;BeiJing&quot;, &quot;names&quot;: [ &quot;CN&quot;, &quot;BeiJing&quot;, &quot;BeiJing&quot;, &quot;k8s&quot;, &quot;System&quot;, &quot;kubernetes&quot; ] &#125;, &quot;issuer&quot;: &#123; &quot;common_name&quot;: &quot;kubernetes&quot;, &quot;country&quot;: &quot;CN&quot;, &quot;organization&quot;: &quot;k8s&quot;, &quot;organizational_unit&quot;: &quot;System&quot;, &quot;locality&quot;: &quot;BeiJing&quot;, &quot;province&quot;: &quot;BeiJing&quot;, &quot;names&quot;: [ &quot;CN&quot;, &quot;BeiJing&quot;, &quot;BeiJing&quot;, &quot;k8s&quot;, &quot;System&quot;, &quot;kubernetes&quot; ] &#125;, &quot;serial_number&quot;: &quot;457042607143982312188093687656561650451025479321&quot;, &quot;sans&quot;: [ &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot;, &quot;127.0.0.1&quot;, &quot;192.168.101.61&quot;, &quot;10.254.0.1&quot; ], &quot;not_before&quot;: &quot;2019-04-09T09:56:00Z&quot;, &quot;not_after&quot;: &quot;2029-04-06T09:56:00Z&quot;, &quot;sigalg&quot;: &quot;SHA256WithRSA&quot;, ···省略若干内容··· 2.6 三台机器相互ssh-key免密钥1234567891011121314151617181920212223[root@k8s-master01 ~]# ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:zzyyxu5/AOAJ4b1zQ+kN1lxjfwtLjtspdUG0Cx5eT70 root@k8s-master01The key&#x27;s randomart image is:+---[RSA 2048]----+| .. + .o || .... + o o. o|| .o.o= o =.++|| o=.o * *o*|| o S... * E.|| o =. + o || .. =+ o || oo .o || ++... |+----[SHA256]-----+[root@k8s-master01 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.101.62[root@k8s-master01 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.101.125 三，部署ETCD集群3.1 解压安装文件12345678[root@k8s-master01 ~]# wget https://github.com/etcd-io/etcd/releases/download/v3.1.10/etcd-v3.1.10-linux-amd64.tar.gz #没有安装包需先下载[root@k8s-master01 ~]# tar xf etcd-v3.3.10-linux-amd64.tar.gz -C /usr/src/[root@k8s-master01 ~]# cd /usr/src/etcd-v3.3.10-linux-amd64/[root@k8s-master01 etcd-v3.3.10-linux-amd64]# lsDocumentation etcd etcdctl README-etcdctl.md README.md READMEv2-etcdctl.md[root@k8s-master01 etcd-v3.3.10-linux-amd64]# cp etcd etcdctl /k8s/etcd/bin/[root@k8s-master01 etcd-v3.3.10-linux-amd64]# ls /k8s/etcd/bin/etcd etcdctl 3.2 创建 etcd 的 systemd unit 文件 在&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;目录下创建文件etcd.service，内容如下。注意替换IP地址为你自己的etcd集群的主机IP。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@k8s-master01 ~]# mkdir /var/lib/etcd #创建目录[root@k8s-master01 etcd-v3.3.10-linux-amd64]# cd /usr/lib/systemd/system/[root@k8s-master01 system]# vim etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/var/lib/etcd/ #切记，此目录需要手动创建，三台都创建EnvironmentFile=/k8s/etcd/cfg/etcd.confExecStart=/k8s/etcd/bin/etcd \\ --name $&#123;ETCD_NAME&#125; \\ --cert-file=/k8s/etcd/ssl/server.pem \\ --key-file=/k8s/etcd/ssl/server-key.pem \\ --peer-cert-file=/k8s/etcd/ssl/server.pem \\ --peer-key-file=/k8s/etcd/ssl/server-key.pem \\ --trusted-ca-file=/k8s/etcd/ssl/ca.pem \\ --peer-trusted-ca-file=/k8s/etcd/ssl/ca.pem \\ --initial-advertise-peer-urls $&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \\ --listen-peer-urls $&#123;ETCD_LISTEN_PEER_URLS&#125; \\ --listen-client-urls $&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \\ --advertise-client-urls $&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \\ --initial-cluster-token $&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \\ --initial-cluster etcd01=https://192.168.101.61:2380,etcd02=https://192.168.101.62:2380,etcd03=https://192.168.101.125:2380 \\ --initial-cluster-state new \\ --data-dir=$&#123;ETCD_DATA_DIR&#125;Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target说明：指定 etcd 的工作目录为 /var/lib/etcd，数据目录为 /var/lib/etcd，需在启动服务前创建这个目录，否则启动服务的时候会报错“Failed at step CHDIR spawning /usr/bin/etcd: No such file or directory”；为了保证通信安全，需要指定 etcd 的公私钥(cert-file和key-file)、Peers 通信的公私钥和 CA 证书(peer-cert-file、peer-key-file、peer-trusted-ca-file)、客户端的CA证书（trusted-ca-file）；创建 kubernetes.pem 证书时使用的 kubernetes-csr.json 文件的 hosts 字段包含所有 etcd 节点的IP，否则证书校验会出错；--initial-cluster-state值为new时，--name的参数值必须位于--initial-cluster 列表中；RestartSec=5 服务重启等待五秒 3.3 创建etcd的环境变量配置文件1234567891011121314[root@k8s-master01 system]# cd /k8s/etcd/cfg/# [member]ETCD_NAME=&quot;etcd01&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.101.61:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.101.61:2379&quot;#[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.101.61:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.101.61:2379&quot;#说明：这是192.168.101.61节点的配置，其他两个etcd节点只要将上面的IP地址改成相应节点的IP地址即可。ETCD_NAME换成对应节点的etcd01/02/03。 3.4 拷贝证书文件12345678[root@k8s-master01 ~]# cd /k8s/[root@k8s-master01 k8s]# lsetcd etcd_ssl k8s_ssl kubernetes[root@k8s-master01 k8s]# cd etcd_ssl/[root@k8s-master01 etcd_ssl]# lsca-config.json ca.csr ca-csr.json ca-key.pem ca.pem server.csr server-csr.json server-key.pem server.pem[root@k8s-master01 etcd_ssl]# cp *.pem /k8s/etcd/ssl/ 3.5 将启动文件、配置文件拷贝到节点62、节点1251234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@k8s-node01 ~]# mkdir /k8s #62机器[root@k8s-node02 ~]# mkdir /k8s #125机器[root@k8s-master01 ~]# cd /k8s/[root@k8s-master01 k8s]# scp -r etcd 192.168.101.62:/k8s/etcd 100% 18MB 136.1MB/s 00:00etcdctl 100% 15MB 130.7MB/s 00:00etcd.conf 100% 340 589.1KB/s 00:00ca-key.pem 100% 1675 2.8MB/s 00:00ca.pem 100% 1265 2.8MB/s 00:00server-key.pem 100% 1675 3.3MB/s 00:00server.pem 100% 1338 2.9MB/s 00:00[root@k8s-master01 k8s]# scp -r etcd 192.168.101.125:/k8s/etcd 100% 18MB 140.2MB/s 00:00etcdctl 100% 15MB 138.0MB/s 00:00etcd.conf 100% 340 1.0MB/s 00:00ca-key.pem 100% 1675 5.2MB/s 00:00ca.pem 100% 1265 4.1MB/s 00:00server-key.pem 100% 1675 5.5MB/s 00:00server.pem 100% 1338 4.4MB/s 00:00[root@k8s-master01 k8s]# scp /usr/lib/systemd/system/etcd.service 192.168.101.62:/usr/lib/systemd/system/etcd.serviceetcd.service 100% 1115 2.8MB/s 00:00[root@k8s-master01 k8s]# scp /usr/lib/systemd/system/etcd.service 192.168.101.125:/usr/lib/systemd/system/etcd.serviceetcd.service 100% 1115 1.5MB/s 00:00#62机器操作：[root@k8s-node01 ~]# vim /k8s/etcd/cfg/etcd.conf# [member]ETCD_NAME=&quot;etcd02&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.101.62:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.101.62:2379&quot;#[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.101.62:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.101.62:2379&quot;#125机器操作：[root@k8s-node02 ~]# vim /k8s/etcd/cfg/etcd.conf# [member]ETCD_NAME=&quot;etcd03&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.101.125:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.101.125:2379&quot;#[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.101.125:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.101.125:2379&quot; 3.6 启动ETCD集群123456789101112#三台都启动ETCD服务[root@k8s-master01 etcd_ssl]# systemctl daemon-reload[root@k8s-master01 etcd_ssl]# systemctl enable etcdCreated symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /usr/lib/systemd/system/etcd.service.[root@k8s-master01 etcd_ssl]# systemctl start etcd#验证集群是否正常运行[root@k8s-master01 ~]# /k8s/etcd/bin/etcdctl --ca-file=/k8s/etcd/ssl/ca.pem --cert-file=/k8s/etcd/ssl/server.pem --key-file=/k8s/etcd/ssl/server-key.pem --endpoints=&quot;https://192.168.101.61:2379,https://192.169.101.62:2379,https://192.168.101.125:2379&quot; cluster-healthmember 7773640f74d528b1 is healthy: got healthy result from https://192.168.101.61:2379member 876df443efa52353 is healthy: got healthy result from https://192.168.101.62:2379member f859f1a325e5abf6 is healthy: got healthy result from https://192.168.101.125:2379cluster is healthy 附录：防火墙设置方法 12345678#iptables防火墙：[root@k8s-master01 ~]# iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 2379 -j ACCEPT[root@k8s-master01 ~]# iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 2380 -j ACCEPT#firewalld防火墙：firewall-cmd --zone=public --add-port=2380/tcp --permanentfirewall-cmd --zone=public --add-port=2379/tcp --permanentfirewall-cmd --reload 四，部署 Flannel 网络4.1 向 ETCD 写入集群 Pod 信息12345678910111213141516171819[root@k8s-master01 ~]# cd /k8s/etcd/ssl/[root@k8s-master01 ssl]# /k8s/etcd/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=&quot;https://192.168.101.61:2379,https://192.168.101:2379.62,https://192.168.101.125:2379&quot; mkdir /kube-centos/network[root@k8s-master01 ssl]# /k8s/etcd/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=&quot;https://192.168.101.61:2379,https://192.168.101:2379.62,https://192.168.101.125:2379&quot; mk /kube-centos/network/config &#x27;&#123; &quot;Network&quot;: &quot;172.30.0.0/16&quot;, &quot;Backend&quot;: &#123; &quot;Type&quot;: &quot;vxlan&quot; &#125; &#125;&#x27;&#123; &quot;Network&quot;: &quot;172.30.0.0/16&quot;, &quot;Backend&quot;: &#123; &quot;Type&quot;: &quot;vxlan&quot; &#125; &#125;#说明：flanneld 当前版本 (v0.10.0) 不支持 etcd v3，故使用 etcd v2 API 写入配置 key 和网段数据；写入的 Pod 网段 $&#123;CLUSTER_CIDR&#125; 必须是 /16 段地址，必须与 kube-controller-manager 的 –cluster-cidr 参数值一致；如果你要使用host-gw模式，可以直接将vxlan改成host-gw即可。#查看网络是否写进ETCD集群[root@k8s-master01 ~]# /k8s/etcd/bin/etcdctl --endpoints=$&#123;ETCD_ENDPOINTS&#125; \\ --ca-file=/k8s/etcd/ssl/ca.pem \\ --cert-file=/k8s/etcd/ssl/server.pem \\ --key-file=/k8s/etcd/ssl/server-key.pem \\ get /kube-centos/network/config&#123; &quot;Network&quot;: &quot;172.30.0.0/16&quot;, &quot;Backend&quot;: &#123; &quot;Type&quot;: &quot;vxlan&quot; &#125; &#125; 4.2 解压安装Flannel12345[root@k8s-master01 ~]# wget https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz[root@k8s-master01 ~]# tar xf flannel-v0.10.0-linux-amd64.tar.gz -C /usr/src/[root@k8s-master01 ~]# mv /usr/src/&#123;flanneld,mk-docker-opts.sh&#125; /k8s/kubernetes/bin/[root@k8s-master01 ~]# ls /k8s/kubernetes/bin/flanneld mk-docker-opts.sh 4.3 配置Flannel123456789# etcd url location. Point this to the server where etcd runsFLANNEL_ETCD_ENDPOINTS=&quot;https://192.168.101.61:2379,https://192.168.101.62:2379,https://192.168.101.125:2379&quot;# etcd config key. This is the configuration key that flannel queries# # For address range assignmentFLANNEL_ETCD_PREFIX=&quot;/kube-centos/network&quot;# Any additional options that you want to passFLANNEL_OPTIONS=&quot;-etcd-cafile=/k8s/etcd/ssl/ca.pem -etcd-certfile=/k8s/etcd/ssl/server.pem -etcd-keyfile=/k8s/etcd/ssl/server-key.pem&quot; 4.4 创建Flannel的systemd unit文件12345678910111213141516[root@k8s-master01 ~]# vim /usr/lib/systemd/system/flanneld.service[Unit]Description=Flanneld overlay address etcd agentAfter=network-online.target network.targetBefore=docker.service[Service]Type=notifyEnvironmentFile=-/etc/sysconfig/docker-networkEnvironmentFile=/k8s/kubernetes/cfg/flanneldExecStart=/k8s/kubernetes/bin/flanneld -etcd-endpoints=$&#123;FLANNEL_ETCD_ENDPOINTS&#125; -etcd-prefix=$&#123;FLANNEL_ETCD_PREFIX&#125; $FLANNEL_OPTIONSExecStartPost=/k8s/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/dockerRestart=on-failure[Install]WantedBy=multi-user.target 说明： mk-docker-opts.sh 脚本将分配给 flanneld 的 Pod 子网网段信息写入 &#x2F;run&#x2F;flannel&#x2F;docker 文件，后续 docker 启动时 使用这个文件中的环境变量配置 docker0 网桥； flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 -iface 参数指定通信接口，如上面的 eth0 接口; flanneld 运行时需要 root 权限； 4.5 配置docker启动，自定义子网网段123456[root@k8s-master01 ~]# cat /usr/lib/systemd/system/docker.service | egrep &quot;ExecStart|Env&quot;EnvironmentFile=/run/flannel/docker#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sockExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS注释掉默认行，添加两行新内容 4.6 将flanneld systemd unit 文件到所有节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@k8s-master01 ~]# scp -r /k8s/kubernetes 192.168.101.62:/k8s/flanneld 100% 35MB 121.7MB/s 00:00mk-docker-opts.sh 100% 2139 3.5MB/s 00:00flanneld 100% 356 733.3KB/s 00:00[root@k8s-master01 ~]# scp -r /k8s/kubernetes 192.168.101.125:/k8s/flanneld 100% 35MB 135.2MB/s 00:00mk-docker-opts.sh 100% 2139 4.6MB/s 00:00flanneld 100% 356 817.7KB/s 00:00[root@k8s-master01 ~]# scp /usr/lib/systemd/system/docker.service 192.168.101.62:/usr/lib/systemd/system/docker.servicedocker.service 100% 1736 3.4MB/s 00:00[root@k8s-master01 ~]# scp /usr/lib/systemd/system/docker.service 192.168.101.125:/usr/lib/systemd/system/docker.servicedocker.service 100% 1736 3.2MB/s 00:00[root@k8s-master01 ~]# scp /usr/lib/systemd/system/flanneld.service 192.168.101.62:/usr/lib/systemd/system/flanneld.serviceflanneld.service 100% 458 867.0KB/s 00:00[root@k8s-master01 ~]# scp /usr/lib/systemd/system/flanneld.service 192.168.101.125:/usr/lib/systemd/system/flanneld.serviceflanneld.service 100% 458 1.1MB/s 00:00#启动服务（所有机器）[root@k8s-master01 ~]# systemctl daemon-reload[root@k8s-master01 ~]# systemctl start flanneld[root@k8s-master01 ~]# systemctl enable flanneld[root@k8s-master01 ~]# systemctl restart docker#查看是否生效[root@k8s-master01 ssl]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens192: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:50:56:a6:c6:5d brd ff:ff:ff:ff:ff:ff inet 192.168.101.61/23 brd 192.168.101.255 scope global dynamic ens192 valid_lft 6722sec preferred_lft 6722sec inet6 fe80::250:56ff:fea6:c65d/64 scope link valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:45:fb:1e:bf brd ff:ff:ff:ff:ff:ff inet 172.30.18.1/24 brd 172.30.18.255 scope global docker0 valid_lft forever preferred_lft forever4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default link/ether 6e:40:51:7e:f3:01 brd ff:ff:ff:ff:ff:ff inet 172.30.18.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::6c40:51ff:fe7e:f301/64 scope link valid_lft forever preferred_lft forever 五，部署master节点 kubernetes master 节点运行如下组件： kube-apiserver kube-scheduler kube-controller-managerkube-scheduler 和 kube-controller-manager 可以以集群模式运行，通过 leader 选举产生一个工作进程，其它进程处于阻塞模式。 5.1 将二进制文件解压拷贝到master节点12345678[root@k8s-master01 ~]# wget https://dl.k8s.io/v1.13.0/kubernetes-server-linux-amd64.tar.gz[root@k8s-master01 ~]# tar xf kubernetes-server-linux-amd64.tar.gz -C /usr/src[root@k8s-master01 ~]# cd /usr/src/kubernetes/[root@k8s-master01 kubernetes]# cp server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet&#125; /k8s/kubernetes/bin/#拷贝证书文件[root@k8s-master01 ~]# cd /k8s/k8s_ssl/[root@k8s-master01 k8s_ssl]# cp *pem /k8s/kubernetes/ssl/ 5.2 部署kube-apiserver组件5.2.1 创建TLS Bootstrapping Token1234[root@k8s-master01 ~]# head -c 16 /dev/urandom | od -An -t x | tr -d &#x27; &#x27;ae4d97f8d4a89cae9fedd4e5f8cf45a1[root@k8s-master01 ~]# vim /k8s/kubernetes/cfg/token.csvae4d97f8d4a89cae9fedd4e5f8cf45a1,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot; 5.2.2 创建apiserver配置文件12345678910111213141516171819202122232425262728293031323334353637383940[root@k8s-master01 ~]# vim /k8s/kubernetes/cfg/kube-apiserverKUBE_APISERVER_OPTS=&quot;--logtostderr=true \\--v=4 \\--etcd-servers=https://192.168.101.61:2379,https://192.168.101.62:2379,https://192.168.101.125:2379 \\--bind-address=192.168.101.61 \\--secure-port=6443 \\--advertise-address=192.168.101.61 \\--allow-privileged=true \\--service-cluster-ip-range=10.0.0.0/24 \\--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \\--authorization-mode=RBAC \\--enable-bootstrap-token-auth \\--token-auth-file=/k8s/kubernetes/cfg/token.csv \\--service-node-port-range=30000-50000 \\--tls-cert-file=/k8s/kubernetes/ssl/kubernetes.pem \\--tls-private-key-file=/k8s/kubernetes/ssl/kubernetes-key.pem \\--client-ca-file=/k8s/kubernetes/ssl/ca.pem \\--service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem \\--etcd-cafile=/k8s/etcd/ssl/ca.pem \\--etcd-certfile=/k8s/etcd/ssl/server.pem \\--etcd-keyfile=/k8s/etcd/ssl/server-key.pem&quot;#参数说明：--logtostderr：记录标准错误而不是文件。默认为 ture--v：日志级别详细程度的编号。--etcd-servers：要连接的etcd服务器列表(scheme://ip:port)，逗号分隔。--bind-address：要监听--secure-port端口的IP地址。关联的接口必须可由群集的其余部分以及CLI / Web客户端访问。如果为空，将使用所有接口（所有IPv4接口均为0.0.0.0，所有IPv6接口为::）。--secure-port：通过身份验证和授权为HTTPS提供服务的端口。无法使用0关闭。默认6443--advertise-address：用于群集成员访问apiserver的IP地址。该地址必须可由群集的其余部分访问。如果为空，则使用--bind-address。如果未指定--bind-address，将使用主机的默认接口。--allow-privileged：如果为true，则允许容器获取特权权限。[默认= FALSE]--service-cluster-ip-range：CIDR标记的IP范围，从中分配IP给服务集群。该范围不能与分配给Pod节点的任何IP范围重叠。--enable-admission-plugins：除了默认启用的插件之外还应启用的插件插件。--authorization-mode：在安全端口上执行授权的有序插件列表。以逗号分隔的列表：AlwaysAllow，AlwaysDeny，ABAC，Webhook，RBAC，Node。默认值：AlwaysAllow（授权访问类型。RBAC：基于角色的访问控制）--enable-bootstrap-token-auth：允许在&#x27;kube-system&#x27;命名空间中允许类型为&#x27;bootstrap.kubernetes.io/token&#x27;的机密用于TLS引导认证。--token-auth-file：如果设置，将用于通过令牌身份验证保护API服务器的安全端口的文件。（指定token文件路径）--service-node-port-range：为NodePort可见性的服务保留的端口范围。示例：&#x27;30000-50000&#x27;。包括在范围的两端。--tls-cert-file：包含HTTPS的默认x509证书的文件。（CA证书，如果有的话，在服务器证书之后连接）。如果启用了HTTPS服务，并且未提供--tls-cert-file和--tls-private-key-file，则会为公共地址生成自签名证书和密钥，并将其保存到指定的目录中 --cert-dir。--tls-private-key-file：包含与--tls-cert-file匹配的默认x509私钥的文件。--client-ca-file：指定k8s集群CA证书路径。--service-account-key-file：指定k8s集群CA-key证书路径。 5.2.3 创建kube-apiserver systemd unit文件123456789101112[root@k8s-master01 ~]# vim /usr/lib/systemd/system/kube-apiserver.service[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/k8s/kubernetes/cfg/kube-apiserverExecStart=/k8s/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTSRestart=on-failure[Install]WantedBy=multi-user.target 5.2.4 启动apiserver服务123456789[root@k8s-master01 ~]# systemctl daemon-reload[root@k8s-master01 ~]# systemctl enable kube-apiserverCreated symlink from /etc/systemd/system/multi-user.target.wants/kube-apiserver.service to /usr/lib/systemd/system/kube-apiserver.service.[root@k8s-master01 ~]# systemctl restart kube-apiserver#查看apiserver服务是否运行[root@k8s-master01 cfg]# ps -ef | grep kube-apiserverroot 10051 1 99 14:28 ? 00:00:06 /k8s/kubernetes/bin/kube-apiserver --logtostderr=true --v=4 --etcd-servers=https://192.168.101.61:2379,https://192.168.101.62:2379,https://192.168.101.125:2379 --bind-address=192.168.101.61 --secure-port=6443 --advertise-address=192.168.101.61 --allow-privileged=true --service-cluster-ip-range=10.0.0.0/24 --enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction --authorization-mode=RBAC --enable-bootstrap-token-auth --token-auth-file=/k8s/kubernetes/cfg/token.csv --service-node-port-range=30000-50000 --tls-cert-file=/k8s/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/k8s/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/k8s/kubernetes/ssl/ca.pem --service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem --etcd-cafile=/k8s/etcd/ssl/ca.pem --etcd-certfile=/k8s/etcd/ssl/server.pem --etcd-keyfile=/k8s/etcd/ssl/server-key.pemroot 10064 9023 0 14:28 pts/0 00:00:00 grep --color=auto kube-apiserver 5.3 部署kube-controller-manager5.3.1 创建kube-controller-manager配置文件12345678910111213141516171819[root@k8s-master01 ~]# vim /k8s/kubernetes/cfg/kube-controller-managerKUBE_CONTROLLER_MANAGER_OPTS=&quot;--logtostderr=true \\--v=4 \\--master=127.0.0.1:8080 \\--leader-elect=true \\--address=127.0.0.1 \\--service-cluster-ip-range=10.0.0.0/24 \\--cluster-name=kubernetes \\--cluster-signing-cert-file=/k8s/kubernetes/ssl/ca.pem \\--cluster-signing-key-file=/k8s/kubernetes/ssl/ca-key.pem \\--root-ca-file=/k8s/kubernetes/ssl/ca.pem \\--service-account-private-key-file=/k8s/kubernetes/ssl/ca-key.pem&quot;#说明：--service-cluster-ip-range 参数指定 Cluster 中 Service 的CIDR范围，该网络在各 Node 间必须路由不可达，必须和 kube-apiserver 中的参数一致；--cluster-signing-* 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥；--root-ca-file 用来对 kube-apiserver 证书进行校验，指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件；--address 服务监听地址。值必须为 127.0.0.1，kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器；--leader-elect：在执行主循环之前，启动领导者选举客户并获得领导。在运行复制组件以实现高可用性时启用此功能。默认值：true 5.3.2 创建kube-controller-manager systemd unit文件123456789101112[root@k8s-master01 ~]# vim /usr/lib/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/k8s/kubernetes/cfg/kube-controller-managerExecStart=/k8s/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failure[Install]WantedBy=multi-user.target 5.3.3 启动kube-controller-manager服务12345678910111213141516171819202122232425262728293031323334[root@k8s-master01 ~]# systemctl daemon-reload[root@k8s-master01 ~]# systemctl enable kube-controller-managerCreated symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /usr/lib/systemd/system/kube-controller-manager.service.[root@k8s-master01 ~]# systemctl restart kube-controller-manager#查看服务运行状态：[root@k8s-master01 ~]# /k8s/kubernetes/bin/kubectl get componentstatuses #启动每个组件后可以通过执行命令kubectl get componentstatuses，来查看各个组件的状态;NAME STATUS MESSAGE ERRORscheduler Unhealthy Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refusedcontroller-manager Healthy ok #组件启动成功etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;etcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;[root@k8s-master01 ~]# systemctl status kube-controller-manager● kube-controller-manager.service - Kubernetes Controller Manager Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled) Active: active (running) since 五 2019-04-19 15:01:59 CST; 1min 32s ago Docs: https://github.com/kubernetes/kubernetes Main PID: 10984 (kube-controller) Tasks: 8 Memory: 29.2M CGroup: /system.slice/kube-controller-manager.service └─10984 /k8s/kubernetes/bin/kube-controller-manager --logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect=true --address=127.0.0.1 --service-cluster-ip-range=10.0.0.0/24 --cluster-...4月 19 15:03:20 k8s-master01 kube-controller-manager[10984]: I0419 15:03:20.760802 10984 gc_controller.go:173] GC&#x27;ing unscheduled pods which are terminating.4月 19 15:03:30 k8s-master01 kube-controller-manager[10984]: I0419 15:03:30.693936 10984 cronjob_controller.go:111] Found 0 jobs4月 19 15:03:30 k8s-master01 kube-controller-manager[10984]: I0419 15:03:30.695533 10984 cronjob_controller.go:119] Found 0 cronjobs4月 19 15:03:30 k8s-master01 kube-controller-manager[10984]: I0419 15:03:30.695541 10984 cronjob_controller.go:122] Found 0 groups4月 19 15:03:30 k8s-master01 kube-controller-manager[10984]: I0419 15:03:30.727777 10984 reflector.go:215] k8s.io/client-go/informers/factory.go:132: forcing resync4月 19 15:03:30 k8s-master01 kube-controller-manager[10984]: I0419 15:03:30.728472 10984 reflector.go:215] k8s.io/client-go/informers/factory.go:132: forcing resync4月 19 15:03:30 k8s-master01 kube-controller-manager[10984]: I0419 15:03:30.734602 10984 reflector.go:215] k8s.io/client-go/informers/factory.go:132: forcing resync4月 19 15:03:30 k8s-master01 kube-controller-manager[10984]: I0419 15:03:30.734673 10984 reflector.go:215] k8s.io/client-go/informers/factory.go:132: forcing resync4月 19 15:03:30 k8s-master01 kube-controller-manager[10984]: I0419 15:03:30.793013 10984 pv_controller_base.go:408] resyncing PV controller4月 19 15:03:32 k8s-master01 kube-controller-manager[10984]: I0419 15:03:32.177901 10984 resource_quota_controller.go:422] no resource updates from discovery, skipping resource quota sync 5.4 部署kube-scheduler5.4.1 创建kube-scheduler配置文件12345[root@k8s-master01 ~]# vim /k8s/kubernetes/cfg/kube-schedulerKUBE_SCHEDULER_OPTS=&quot;--logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect&quot;#说明：--master Kubernetes API服务地址，如果kubeconfig中给定，则覆盖掉。 5.4.2 创建kube-scheduler systemd unit 文件123456789101112[root@k8s-master01 ~]# vim /usr/lib/systemd/system/kube-scheduler.service[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/k8s/kubernetes/cfg/kube-schedulerExecStart=/k8s/kubernetes/bin/kube-scheduler $KUBE_SCHEDULER_OPTSRestart=on-failure[Install]WantedBy=multi-user.target 5.4.3 启动kube-scheduler服务12345678910111213[root@k8s-master01 ~]# systemctl daemon-reload[root@k8s-master01 ~]# systemctl enable kube-schedulerCreated symlink from /etc/systemd/system/multi-user.target.wants/kube-scheduler.service to /usr/lib/systemd/system/kube-scheduler.service.[root@k8s-master01 ~]# systemctl restart kube-scheduler#查看启动状态：（两个组件都ok了）[root@k8s-master01 ~]# /k8s/kubernetes/bin/kubectl get componentstatusesNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;etcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; 5.5 给k8s的相关命令做软连接并给master授权123456789101112[root@k8s-master01 ~]# ln -s /k8s/kubernetes/bin/* /usr/local/bin/[root@k8s-master01 ~]# which kubectl/usr/local/bin/kubectl[root@k8s-master01 ~]# kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; 12#进行master授权kubectl create clusterrolebinding cluster-system-anonymous --clusterrole=cluster-admin --user=system:anonymous 六，部署node节点 kubernetes work 节点运行如下组件： docker 前面已经部署 flannel 前面已经部署 kubelet kube-proxy 6.1 创建 kubelet bootstrapping kubeconfig 文件 因为kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权；所以创建kubernetes文件。 以下操作只需要在master节点上执行，生成的*.kubeconfig文件可以直接拷贝到node节点的&#x2F;k8s&#x2F;kubernetes&#x2F;cfg目录下。 123456789101112131415161718192021222324252627[root@k8s-master01 ~]# export BOOTSTRAP_TOKEN=ae4d97f8d4a89cae9fedd4e5f8cf45a1[root@k8s-master01 ~]# export KUBE_APISERVER=&quot;https://192.168.101.61:6443&quot;#设置集群参数[root@k8s-master01 ~]# kubectl config set-cluster kubernetes \\ --certificate-authority=/k8s/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=bootstrap.kubeconfigCluster &quot;kubernetes&quot; set.#设置客户端认证参数[root@k8s-master01 ~]# kubectl config set-credentials kubelet-bootstrap \\ --token=$&#123;BOOTSTRAP_TOKEN&#125; \\ --kubeconfig=bootstrap.kubeconfigUser &quot;kubelet-bootstrap&quot; set.#设置上下文参数[root@k8s-master01 ~]# kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfigContext &quot;default&quot; created.#设置默认上下文[root@k8s-master01 ~]# kubectl config use-context default --kubeconfig=bootstrap.kubeconfigSwitched to context &quot;default&quot;. –embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中；设置客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成； 6.2 创建 kube-proxy kubeconfig 文件1234567891011121314151617181920212223242526# 设置集群参数[root@k8s-master01 ~]# kubectl config set-cluster kubernetes \\ --certificate-authority=/k8s/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=kube-proxy.kubeconfigCluster &quot;kubernetes&quot; set.# 设置客户端认证参数[root@k8s-master01 ~]# kubectl config set-credentials kube-proxy \\ --client-certificate=/k8s/kubernetes/ssl/kube-proxy.pem \\ --client-key=/k8s/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfigUser &quot;kube-proxy&quot; set.# 设置上下文参数[root@k8s-master01 ~]# kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfigContext &quot;default&quot; created.# 设置默认上下文[root@k8s-master01 ~]# kubectl config use-context default --kubeconfig=kube-proxy.kubeconfigSwitched to context &quot;default&quot;. 6.3 分发 kubeconfig 文件 将两个 kubeconfig 文件分发到所有 Node 机器的 &#x2F;k8s&#x2F;kubernetes&#x2F;cfg&#x2F; 目录 12345678[root@k8s-master01 ~]# mv bootstrap.kubeconfig /k8s/kubernetes/cfg/[root@k8s-master01 ~]# mv kube-proxy.kubeconfig /k8s/kubernetes/cfg/[root@k8s-master01 ~]# scp /k8s/kubernetes/cfg/&#123;bootstrap.kubeconfig,kube-proxy.kubeconfig&#125; 192.168.101.62:/k8s/kubernetes/cfg/bootstrap.kubeconfig 100% 2168 4.9MB/s 00:00kube-proxy.kubeconfig 100% 6274 13.6MB/s 00:00[root@k8s-master01 ~]# scp /k8s/kubernetes/cfg/&#123;bootstrap.kubeconfig,kube-proxy.kubeconfig&#125; 192.168.101.125:/k8s/kubernetes/cfg/bootstrap.kubeconfig 100% 2168 4.4MB/s 00:00kube-proxy.kubeconfig 100% 6274 11.8MB/s 00:00 6.4 分发kubelet和kube-proxy命令123456[root@k8s-master01 ~]# scp /k8s/kubernetes/bin/&#123;kubelet,kube-proxy&#125; 192.168.101.62:/k8s/kubernetes/bin/kubelet 100% 108MB 149.3MB/s 00:00kube-proxy 100% 33MB 149.6MB/s 00:00[root@k8s-master01 ~]# scp /k8s/kubernetes/bin/&#123;kubelet,kube-proxy&#125; 192.168.101.125:/k8s/kubernetes/bin/kubelet 100% 108MB 128.2MB/s 00:00kube-proxy 100% 33MB 145.2MB/s 00:00 6.5 创建 kubelet 参数配置模板文件 两台node机器同样操作 12345678910111213[root@k8s-node01 ~]# vim /k8s/kubernetes/cfg/kubelet.configkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 192.168.101.61port: 10250readOnlyPort: 10255cgroupDriver: cgroupfsclusterDNS: [&quot;10.0.0.2&quot;]clusterDomain: cluster.local.failSwapOn: falseauthentication: anonymous: enabled: true 创建kubelet配置文件 123456789[root@k8s-node01 ~]# vim /k8s/kubernetes/cfg/kubeletKUBELET_OPTS=&quot;--logtostderr=true \\--v=4 \\--hostname-override=192.168.101.61 \\--kubeconfig=/k8s/kubernetes/cfg/kubelet.kubeconfig \\--bootstrap-kubeconfig=/k8s/kubernetes/cfg/bootstrap.kubeconfig \\--config=/k8s/kubernetes/cfg/kubelet.config \\--cert-dir=/k8s/kubernetes/ssl \\--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0&quot; 创建kubelet systemd unit 文件 1234567891011121314[root@k8s-node01 ~]# vim /usr/lib/systemd/system/kubelet.service[Unit]Description=Kubernetes KubeletAfter=docker.serviceRequires=docker.service[Service]EnvironmentFile=/k8s/kubernetes/cfg/kubeletExecStart=/k8s/kubernetes/bin/kubelet $KUBELET_OPTSRestart=on-failureKillMode=process[Install]WantedBy=multi-user.target 将kubelet-bootstrap用户绑定到系统集群角色（master机器） 12345678# master 机器操作(创建集群角色)：[root@k8s-master01 ~]# kubectl create clusterrolebinding kubelet-bootstrap \\ --clusterrole=system:node-bootstrapper \\ --group=system:nodesclusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created#删除集群角色方法：[root@k8s-master01 ~]# kubectl delete clusterrolebinding/kubelet-bootstrap 启动服务 12345[root@k8s-node01 ~]# systemctl daemon-reload[root@k8s-node01 ~]# systemctl enable kubeletCreated symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.[root@k8s-node01 ~]# systemctl restart kubelet master端授权node节点 123456789101112131415161718192021222324252627# master端查看csr[root@k8s-master01 ~]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-Bw88ux21GTp7OhJoJpyUUgdwnh6GlClymloIqGEra3c 7m7s kubelet-bootstrap Pendingnode-csr-ku4FxcEcbrcdCdKvUapH3MS21VRypJw8jrFZcS5LctE 6m57s kubelet-bootstrap Pending# 授权approve kubelet CSR 请求[root@k8s-master01 ~]# kubectl certificate approve node-csr-Bw88ux21GTp7OhJoJpyUUgdwnh6GlClymloIqGEra3ccertificatesigningrequest.certificates.k8s.io/node-csr-Bw88ux21GTp7OhJoJpyUUgdwnh6GlClymloIqGEra3c approved[root@k8s-master01 ~]# kubectl certificate approve node-csr-ku4FxcEcbrcdCdKvUapH3MS21VRypJw8jrFZcS5LctEcertificatesigningrequest.certificates.k8s.io/node-csr-ku4FxcEcbrcdCdKvUapH3MS21VRypJw8jrFZcS5LctE approved# 查看csr[root@k8s-master01 ~]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-Bw88ux21GTp7OhJoJpyUUgdwnh6GlClymloIqGEra3c 8m55s kubelet-bootstrap Approved,Issuednode-csr-ku4FxcEcbrcdCdKvUapH3MS21VRypJw8jrFZcS5LctE 8m45s kubelet-bootstrap Approved,Issued#删除多余csr方法[root@k8s-master01 ~]# kubectl delete csr [csr名称]# 查看集群状态[root@k8s-master01 ~]# kubectl get nodeNAME STATUS ROLES AGE VERSION192.168.101.125 Ready &lt;none&gt; 16h v1.13.0192.168.101.62 Ready &lt;none&gt; 16h v1.13.0 6.6 部署 kube-proxy 组件 kube-proxy 运行在所有 node节点上，它监听 apiserver 中 service 和 Endpoint 的变化情况，创建路由规则来进行服务负载均衡。 6.6.1 创建 kube-proxy 配置文件1234567891011[root@k8s-node01 ~]# vim /k8s/kubernetes/cfg/kube-proxyKUBE_PROXY_OPTS=&quot;--logtostderr=true \\--v=4 \\--hostname-override=192.168.101.62 \\--cluster-cidr=10.0.0.0/24 \\--kubeconfig=/k8s/kubernetes/cfg/kube-proxy.kubeconfig&quot;#说明：--hostname-override：本机节点ip--kubeconfig：包含授权信息的 kubeconfig 文件的路径 6.6.2 创建kube-proxy systemd unit文件123456789101112[root@k8s-node01 ~]# vim /usr/lib/systemd/system/kube-proxy.service[Unit]Description=Kubernetes ProxyAfter=network.target[Service]EnvironmentFile=-/k8s/kubernetes/cfg/kube-proxyExecStart=/k8s/kubernetes/bin/kube-proxy $KUBE_PROXY_OPTSRestart=on-failure[Install]WantedBy=multi-user.target 6.6.3 启动kube-proxy服务12345678910111213141516171819202122232425systemctl daemon-reloadsystemctl enable kube-proxysystemctl restart kube-proxy#查看服务状态[root@k8s-node01 ~]# systemctl status kube-proxy● kube-proxy.service - Kubernetes Proxy Loaded: loaded (/usr/lib/systemd/system/kube-proxy.service; enabled; vendor preset: disabled) Active: active (running) since 二 2019-04-23 10:15:09 CST; 25s ago Main PID: 880 (kube-proxy) Tasks: 0 Memory: 11.0M CGroup: /system.slice/kube-proxy.service ‣ 880 /k8s/kubernetes/bin/kube-proxy --logtostderr=true --v=4 --hostname-override=192.168.101.62 --cluster-cidr=10.0.0.0/24 --kubeconfig=/k8s/kubernetes/cfg/kube-proxy.kubeconfig4月 23 10:15:26 k8s-node01 kube-proxy[880]: I0423 10:15:26.484711 880 config.go:141] Calling handler.OnEndpointsUpdate4月 23 10:15:26 k8s-node01 kube-proxy[880]: I0423 10:15:26.486098 880 config.go:141] Calling handler.OnEndpointsUpdate4月 23 10:15:28 k8s-node01 kube-proxy[880]: I0423 10:15:28.492244 880 config.go:141] Calling handler.OnEndpointsUpdate4月 23 10:15:28 k8s-node01 kube-proxy[880]: I0423 10:15:28.492271 880 config.go:141] Calling handler.OnEndpointsUpdate4月 23 10:15:30 k8s-node01 kube-proxy[880]: I0423 10:15:30.500034 880 config.go:141] Calling handler.OnEndpointsUpdate4月 23 10:15:30 k8s-node01 kube-proxy[880]: I0423 10:15:30.501511 880 config.go:141] Calling handler.OnEndpointsUpdate4月 23 10:15:32 k8s-node01 kube-proxy[880]: I0423 10:15:32.508419 880 config.go:141] Calling handler.OnEndpointsUpdate4月 23 10:15:32 k8s-node01 kube-proxy[880]: I0423 10:15:32.508444 880 config.go:141] Calling handler.OnEndpointsUpdate4月 23 10:15:34 k8s-node01 kube-proxy[880]: I0423 10:15:34.513354 880 config.go:141] Calling handler.OnEndpointsUpdate4月 23 10:15:34 k8s-node01 kube-proxy[880]: I0423 10:15:34.514406 880 config.go:141] Calling handler.OnEndpointsUpdate 6.7 集群状态 打开node节点集群标签 1234567891011121314151617181920212223[root@k8s-master01 ~]# kubectl label node 192.168.101.62 node-role.kubernetes.io/node=&#x27;node&#x27;node/192.168.101.62 labeled[root@k8s-master01 ~]# kubectl label node 192.168.101.125 node-role.kubernetes.io/node=&#x27;node&#x27;node/192.168.101.125 labeled[root@k8s-master01 ~]# kubectl get nodeNAME STATUS ROLES AGE VERSION192.168.101.125 Ready node 17h v1.13.0192.168.101.62 Ready node 17h v1.13.0[root@k8s-master01 ~]# kubectl get node,csNAME STATUS ROLES AGE VERSIONnode/192.168.101.125 Ready node 17h v1.13.0node/192.168.101.62 Ready node 17h v1.13.0NAME STATUS MESSAGE ERRORcomponentstatus/scheduler Healthy okcomponentstatus/controller-manager Healthy okcomponentstatus/etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;componentstatus/etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;componentstatus/etcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;#附录：master集群打开标签方法：举例：[root@k8s-master01 ~]# kubectl label node 192.168.101.62 node-role.kubernetes.io/master=&#x27;master&#x27; 至此，k8s集群搭建完毕。","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kkabuzs.github.io/tags/Kubernetes/"}]},{"title":"Linux 下非root用户使用docker的问题","slug":"Linux-fei-root-yonghushiyong-docker-dewenti","date":"2019-04-02T05:18:24.000Z","updated":"2019-04-02T05:18:24.000Z","comments":true,"path":"articles/2019/04/02/Linux-fei-root-yonghushiyong-docker-dewenti/","permalink":"https://kkabuzs.github.io/articles/2019/04/02/Linux-fei-root-yonghushiyong-docker-dewenti/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 一，解决非root用户使用docker Linux 下非root用户使用docker，如果不需要root权限的话，需要将用户加入到docker组。 12#报错：ERROR: Couldn&#x27;t connect to Docker daemon at http+docker://localunixsocket - is it running? 通常我们使用linux系统的时候，最好是不要直接使用root账号，但是使用Docker的时候，默认又是不能使用非root用户的，关于原因，官方说法如下： The docker daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user root and other users can access it with sudo. For this reason, docker daemon always runs as the root user.To avoid having to use sudo when you use the docker command, create a Unix group called docker and add users to it. When the docker daemon starts, it makes the ownership of the Unix socket read&#x2F;writable by the docker group. 二，下面是让非root用户可用root的步骤：2.1创建docker组1sudo groupadd docker 2.2 将当前用户加入docker组1sudo gpasswd -a $&#123;USER&#125; docker 2.3 重新启动docker服务（下面是CentOS7的命令）1sudo systemctl restart docker 当前用户退出系统重新登陆 运行docker命令，测试","categories":[{"name":"工作随笔，问题排查","slug":"工作随笔，问题排查","permalink":"https://kkabuzs.github.io/categories/%E5%B7%A5%E4%BD%9C%E9%9A%8F%E7%AC%94%EF%BC%8C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://kkabuzs.github.io/tags/Docker/"}]},{"title":"kubernetes yaml部署及各种yaml写法和含义","slug":"kubernetesyamlbushujixiefadingyi","date":"2018-12-28T04:06:01.000Z","updated":"2018-12-28T04:06:01.000Z","comments":true,"path":"articles/2018/12/28/kubernetesyamlbushujixiefadingyi/","permalink":"https://kkabuzs.github.io/articles/2018/12/28/kubernetesyamlbushujixiefadingyi/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 kubernetes yaml部署及各种yaml写法和含义一，环境介绍 主机名 IP地址 CPU 内存 描述 k8s-master01 192.168.101.61 2核 4GB k8s master k8s-node01 192.168.101.62 2核 4GB k8s node节点 k8s-node02 192.168.101.125 2核 4GB k8s node节点 二，RC的写法及SVC(service)的写法及用法2.1 创建一个mysql的RC123456789101112131415161718192021222324252627[root@k8s-master01 kubernetes-yaml]# pwd/root/kubernetes-yaml[root@k8s-master01 kubernetes-yaml]# vim msyql-rc.yamlapiVersion: v1kind: ReplicationController #副本控制器RCmetadata: name: mysql #RC的名称，全局唯一。spec: replicas: 1 #Pod副本期待数量 selector: app: mysql #符合目标的Pod拥有此标签 template: #根据此模版创建Pod的副本（实例） metadata: labels: app: mysql #Pod副本拥有的标签。对应RC的selector spec: containers: #Pod内容器的定义部分。 - name: mysql #容器的名称。 image: docker.io/mysql:5.6 #容器对应的docker image ports: - containerPort: 3306 #容器暴露的端口号。 env: #注入到容器内的环境变量 - name: MYSQL_ROOT_PASSWORD value: &quot;123123&quot;#说明：yaml文件中定义的kind属性，用来表明组员对象的类型。“ReplicationController”，表示这是一个RC；spec一节中是RC的相关属性的定义，比如spec.selector是RC的Pod标签(Label)选择器，即监控和管理拥有这些标签的Pod实例，确保当前集群上始终且仅有replicas个Pod实例在运行，设置replicas=1表示只能运行一个mysql pod实例。当前集群中运行的Pod数量小于replicas时，RC会根据spec.template一节中定义的Pod模版来生成一个新的Pod实例，spec.template.metadata.labels指定了该Pod的标签，需要特别注意的是：这里的labels必须匹配之前的spec.selector，否则此RC每次创建了一个无法匹配label的Pod，就会不停的尝试创建新的Pod。 将mysql RC发布到kubernetes集群中 12345678910111213141516171819202122232425262728293031323334#创建RC[root@k8s-master01 kubernetes-yaml]# kubectl create -f msyql-rc.yamlreplicationcontroller/mysql created#删除RC，SVC方法：kubectl delete -f [文件名]#查看刚刚创建的RC[root@k8s-master01 kubernetes-yaml]# kubectl get rcNAME DESIRED CURRENT READY AGEmysql 1 1 0 11s#查看RC创建的Pod[root@k8s-master01 kubernetes-yaml]# kubectl get podsNAME READY STATUS RESTARTS AGEmysql-f7f6g 1/1 Running 0 39s#说明：我们看到有一个mysql-XXXX的pod实例，这时kubernetes根据mysql这个RC自动创建的Pod。[root@k8s-master01 kubernetes-yaml]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmysql-f7f6g 1/1 Running 0 14m 172.30.35.2 192.168.101.62 &lt;none&gt; &lt;none&gt;#我们看到pod运行在62这台node节点上。#192.168.101.62机器操作：[root@k8s-node01 ~]# docker ps #可以看到运行的mysql容器。CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf1a91e4e32b4 mysql &quot;docker-entrypoint.s…&quot; 2 minutes ago Up 2 minutes k8s_mysql_mysql-f7f6g_default_9423943c-6589-11e9-b265-005056a6c65d_028f9a125e44c registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0 &quot;/pause&quot; 2 minutes ago Up 2 minutes k8s_POD_mysql-f7f6g_default_9423943c-6589-11e9-b265-005056a6c65d_0#Pause是谷歌的容器，也可以理解为Pod的根容器。#优点：引入与业务无关并且不易死亡的Pause容器作为Pod的根容器，以它的状态代表整个容器组的状态，解决了两个问题。第一：一个容器死亡，此时算是整体死亡吗？第二：Pod里的多个业务容器共享Pause容器的ip，共享Pause容器挂接的Volume，既简化了密切关联的业务容器之间的通信问题，有很好的解决了它们之间文件共享的问题。 再创建一个与之关联的kubernetes service——MYSQL的定义文件 1234567891011121314151617181920212223242526[root@k8s-master01 kubernetes-yaml]# vim mysql-svc.yamlapiVersion: v1kind: Service #表明是kubernetes servicemetadata: name: mysql #service的全局唯一名称spec: ports: - port: 3306 #service提供服务的端口号 selector: #service对应的Pod拥有这里定义的标签 app: mysql #说明：其中，metadata.name是Service的服务名(ServiceName)；port属性定义了Service的虚端口；spec.selector确定了哪些Pod副本(实例)对应到本服务。类似的，我们可以通过`kubectl create`命令创建Service对象。#创建service[root@k8s-master01 kubernetes-yaml]# kubectl create -f mysql-svc.yamlservice/mysql created#查看刚刚创建的service[root@k8s-master01 kubernetes-yaml]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 4d2hmysql ClusterIP 10.0.0.13 &lt;none&gt; 3306/TCP 15s#kubernetes有一个发现机制，利用linux的环境变量，根据service的唯一名字，容器可以从环境变量中获取到service对应的CLUSTER-IP和端口，从而发起TCP/IP请求。 测试访问 12345678910111213141516171819202122232425262728293031323334353637#192.168.101.62机器测试访问：[root@k8s-node01 ~]# mysql -h 10.0.0.13 -P 3306 -pEnter password: #123123Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 8Server version: 8.0.15 MySQL Community Server - GPLCopyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt; exitBye#192.168.101.125机器测试访问：[root@k8s-node02 ~]# mysql -h 10.0.0.13 -P 3306 -pEnter password: #123123Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 9Server version: 8.0.15 MySQL Community Server - GPLCopyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt; exitBye[root@k8s-node02 ~]# 2.2 创建一个tomcat的RC123456789101112131415161718192021222324252627[root@k8s-master01 kubernetes-yaml]# vim tomcat-rc.yamlapiVersion: v1kind: ReplicationControllermetadata: name: tomcatspec: replicas: 2 selector: app: tomcat template: metadata: labels: app: tomcat spec: containers: - name: tomcat image: kubeguide/tomcat-app:v1 ports: - containerPort: 8080 env: - name: MYSQL_SERVICE_HOST value: &#x27;10.0.0.13&#x27; - name: MYSQL_SERVICE_PORT value: &#x27;3306&#x27; #说明：这个RC的tomcat容器里引用了MYSQL_SERVICE_HOST变量，而&#x27;10.0.0.13&#x27;刚好是我们之前定义的mysql服务的cluster ip。 将tomcat RC发布到kubernetes集群中 12345678910111213141516#创建RC[root@k8s-master01 kubernetes-yaml]# kubectl create -f tomcat-rc.yamlreplicationcontroller/tomcat created#查看RC[root@k8s-master01 kubernetes-yaml]# kubectl get rcNAME DESIRED CURRENT READY AGEmysql 1 1 1 20htomcat 2 2 2 48s #创建成功#由于我们RC里定义的tomcat容器replicas=2，所以会有两个tomcat容器。[root@k8s-master01 kubernetes-yaml]# kubectl get podsNAME READY STATUS RESTARTS AGEmysql-f7f6g 1/1 Running 0 20htomcat-dbwjv 1/1 Running 0 98stomcat-mvjjf 1/1 Running 0 98s 创建对应的Service，暴露tomcat服务 1234567891011121314151617181920212223242526[root@k8s-master01 kubernetes-yaml]# vim tomcat-svc.yamlapiVersion: v1kind: Servicemetadata: name: tomcatspec: type: NodePort ports: - port: 8080 nodePort: 30001 selector: app: tomcat #注意！！注意type: NodePort和nodePort: 30001的两个属性，表明此service开启了NodePort方式的外网访问模式，在kubernetes集群之外，例如在本机的浏览器里，可以通过30001端口访问tomcat(对应到8080虚端口上)#创建service[root@k8s-master01 kubernetes-yaml]# kubectl create -f tomcat-svc.yamlservice/tomcat created#查看创建的service[root@k8s-master01 kubernetes-yaml]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 4d19hmysql ClusterIP 10.0.0.13 &lt;none&gt; 3306/TCP 16htomcat NodePort 10.0.0.58 &lt;none&gt; 8080:30001/TCP 63s #外部30001对应8080虚端口 浏览器输入IP+端口测试访问 123456#我起了两个tomcat pod，被平均调度在了两台node节点上，所以任意一台都可以访问。[root@k8s-master01 kubernetes-yaml]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmysql-f7f6g 1/1 Running 0 20h 172.30.35.2 192.168.101.62 &lt;none&gt; &lt;none&gt;tomcat-dbwjv 1/1 Running 0 18m 172.30.35.3 192.168.101.62 &lt;none&gt; &lt;none&gt;tomcat-mvjjf 1/1 Running 0 18m 172.30.20.2 192.168.101.125 &lt;none&gt; &lt;none&gt; 选择add，添加一条数据 进入数据库，查看是否有刚刚添加的数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@k8s-node02 ~]# mysql -h 10.0.0.13 -P 3306 -p123123mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 9Server version: 5.6.43 MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || HPE_APP || mysql || performance_schema |+--------------------+4 rows in set (0.00 sec)mysql&gt; use HPE_APP;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; select * from T_USERS;+----+-----------+-------+| ID | USER_NAME | LEVEL |+----+-----------+-------+| 1 | me | 100 || 2 | our team | 100 || 3 | HPE | 100 || 4 | teacher | 100 || 5 | docker | 100 || 6 | google | 100 || 7 | zhaoshuo | 100 | #数据添加成功！+----+-----------+-------+7 rows in set (0.00 sec) 三，Deployment概述 Depolyment是Kubernetes 1.2 引入到新概念，目的是更好的解决Pod的编排问题。为此，Deployment内部使用了Replica set来实现目的。无论从Deployment的作用与目的、他的yaml定义，还是从它的具体命令行操作来看，我们都可以把他看作是RC的一次升级，两者相似度超过90%。Deployment相对于RC的最大一次升级是我们可以随时知道当前Pod“部署”的进度。实际上由于一个Pod的创建，调度绑定节点及在目标Node上启动对应的容器这一完整过程需要一定的时间，所以我们期待系统启动N个Pod副本的目标状态，实际上是一个连续变化的“部署过程”导致的最终状态。 3.1 Deployment的典型使用场景 创建一个Deployment对象来生成对应的Replica set并完成Pod副本的创建过程。 检查Deployment的状态来看部署动作是否完成。（Pod副本的数量是否达到预期的值） 更新Deployment以创建新的Pod。（例如镜像升级） 如果当前Deployment不稳定，则回滚到早先的Deployment版本。 挂起或恢复一个Deployment。 Deployment的定义与Replica set的定义很类似，除了API声明和kind类型有所区别 1234apiVersion: extensions/v1beta1 apiVersion: v1kind: Deployment kind: ReplicaSetmetadata: metadata: name: nginx-Deployment name: nginx-repset 3.2 创建一个Nginx的Deployment123456789101112131415161718192021222324252627[root@k8s-master01 kubernetes-yaml]# vim nginx-deployment.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: frontendspec: replicas: 1 selector: matchLabels: tier: frontend template: metadata: labels: app: app-demo tier: frontend spec: nodeName: gago-k8s-node1 #绑定节点 containers: - name: nginx-demo image: nginx imagePullPolicy: IfNotPresent ports: - containerPort: 8080#说明：imagePullPolicy：填写IfNotPresent就代表本地有镜像，就不再去网上拉去镜像。matchLabels：是一个&#123;key,value&#125;的映射。一个单独的 &#123;key,value&#125; 相当于 matchExpressions 的一个元素，它的key字段是”key”,操作符是 In ，并且value数组value包含”value”。 将nginx Deployment发布到kubernetes集群中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#创建Deployment[root@k8s-master01 kubernetes-yaml]# kubectl create -f nginx-deployment.yamldeployment.extensions/frontend created#查看deployment信息[root@k8s-master01 kubernetes-yaml]# kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEfrontend 1/1 1 1 27m输出内容说明：DESIRED：Pod副本数量的期望值。即Deployment里定义的Replica。CURRENT：当前Replica的值，实际上是Deployment所创建的Replica Set里的Replica值，这个值不断增加，直到达到DESIRED为止。P-TO-DATE：最新版本的Pod数量，用于指示在滚动升级的过程中，有多少个Pod副本以成功升级。AVAILABLE：当前集群中可用的Pod副本数量。即集群中当前存活的Pod数量。#查看Replica Set[root@k8s-master01 ~]# kubectl get rsNAME DESIRED CURRENT READY AGEfrontend-8d9b575b9 1 1 1 22h#查看pod[root@k8s-master01 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEfrontend-8d9b575b9-8mv4t 1/1 Running 0 23hmysql-n9b57 1/1 Running 0 16dtomcat-4vzs4 1/1 Running 0 16dtomcat-gns9v 1/1 Running 0 16d#查看deployments的详细信息[root@k8s-master01 ~]# kubectl describe deploymentsName: frontendNamespace: defaultCreationTimestamp: Thu, 09 May 2019 15:30:40 +0800Labels: app=app-demo tier=frontendAnnotations: deployment.kubernetes.io/revision: 1Selector: tier=frontend,tier in (frontend)Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailableStrategyType: RollingUpdateMinReadySeconds: 0RollingUpdateStrategy: 1 max unavailable, 1 max surgePod Template: Labels: app=app-demo tier=frontend Containers: nginx-demo: Image: nginx Port: 8080/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailableOldReplicaSets: &lt;none&gt;NewReplicaSet: frontend-8d9b575b9 (1/1 replicas created)Events: &lt;none&gt; 四，Service(SVC)概述4.1 概述 kubernetes Pod是有生命周期的，它们可以被创建，也可以被销毁，然而一旦被销毁生命就永远结束。通过 ReplicaSets 能够动态地创建和销毁 Pod（例如，需要进行扩缩容，或者执行 滚动升级）。 每个 Pod 都会获取它自己的 IP 地址，即使这些 IP 地址不总是稳定可依赖的。 这会导致一个问题：在 Kubernetes 集群中，如果一组 Pod（称为 backend）为其它 Pod （称为 frontend）提供服务，那么那些 frontend 该如何发现，并连接到这组 Pod 中的哪些 backend 呢？ 关于 Service Service也是kubernetes里的最核心的资源对象之一。Kubernetes Service 定义了这样一种抽象：逻辑上的一组 Pod，一种可以访问它们的策略 —— 通常称为微服务。 这一组 Pod 能够被 Service 访问到，通常是通过 Label Selector实现的。 4.2 为上文的nginx deployment创建一个service123456789101112131415161718192021222324252627282930313233[root@k8s-master01 kubernetes-yaml]# vim nginx-deployment-service.yamlapiVersion: v1kind: Servicemetadata: name: nginx-deployment-servicespec: ports: - port : 8080 protocol: TCP selector: tier: frontend 我们创建了一个nginx的service，它的服务端口为8080。拥有“tier: frontend”这个Label的所有Pod实例都属于他。# 启动nginx-deployment-service[root@k8s-master01 kubernetes-yaml]# kubectl create -f nginx-deployment-service.yamlservice/nginx-deployment-service created# 查看endpoints[root@k8s-master01 kubernetes-yaml]# kubectl get endpointsNAME ENDPOINTS AGEkubernetes 192.168.101.61:6443 33dmysql 172.30.24.3:3306 28dnginx-deployment-service 172.30.34.3:8080 57stomcat 172.30.24.2:8080,172.30.34.2:8080 28d# 查看SVC[root@k8s-master01 kubernetes-yaml]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 33d &lt;none&gt;mysql ClusterIP 10.0.0.32 &lt;none&gt; 3306/TCP 28d app=mysqlnginx-deployment-service ClusterIP 10.0.0.140 &lt;none&gt; 8080/TCP 95s tier=frontendtomcat NodePort 10.0.0.12 &lt;none&gt; 8080:30001/TCP 28d app=tomcat 4.3 附录：多端口Service12345678910111213141516kind: ServiceapiVersion: v1metadata: name: my-servicespec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 - name: https protocol: TCP port: 443 targetPort: 9377","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kkabuzs.github.io/tags/Kubernetes/"}]},{"title":"FTP服务器的搭建","slug":"ftpfuwuqidajian","date":"2018-12-22T06:15:52.000Z","updated":"2018-12-22T06:15:52.000Z","comments":true,"path":"articles/2018/12/22/ftpfuwuqidajian/","permalink":"https://kkabuzs.github.io/articles/2018/12/22/ftpfuwuqidajian/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 FTP服务器的搭建一，FTP的基本概念 FTP （File Transfer Protocol，文件传输协议）简称为“文传协议”。用于Internet上的控制文件的双向传输。同时，它也是一个应用程序（Application）。基于不同的操作系统有不同的FTP应用程序，而所有这些应用程序都遵守同一种协议以传输文件。在FTP的使用当中，用户经常遇到两个概念：”下载”（Download）和”上传”（Upload）。”下载”文件就是从远程主机拷贝文件至自己的计算机上；”上传”文件就是将文件从自己的计算机中拷贝至远程主机上。用Internet语言来说，用户可通过客户机程序向（从）远程主机上传（下载）文件。 1.1 FTP数据连接的建立类型 主动模式：服务器主动发起数据连接 被动模式：服务器被动等待数据连接 1.2 传输模式 文本模式 二进制模式 1.3 FTP用户类型 匿名用户 本地用户 虚拟用户 1.4 命令参数 FTP服务器的登陆 匿名用户登录：ftp 密码：空 显示文件信息：dir&#x2F;ls 下载文件：get 文件名（下载到当前目录） 上传文件：put 文件名 多文件下载：mget 多文件上传：mput 退出：bye 帮助：help 二，FTP服务的搭建2.1 搭建匿名访问的FTP服务1234567891011121314151617181920212223242526272829303132333435363738394041[root@loaclhost ~]# rpm -qa | grep vsftpdvsftpd-2.2.2-11.el6_4.1.x86_64 #如果没有yum安装一下[root@localhost ~]# yum -y install vsftpd#准备匿名FTP访问的目录[root@localhost ~]# cd /var/ftp/[root@loaclhost ftp]# lltotal 4drwxr-xr-x 2 root root 4096 Mar 1 2013 pub[root@localhost ftp]# chown ftp pub/ #修改共享目录属主为ftp[root@localhost ftp]# lltotal 4drwxr-xr-x 2 ftp root 4096 Mar 1 2013 pub[root@localhost ftp]# cd pub/ [root@localhost pub]# touch test[root@localhost pub]# lstest #创建一个测试文件#开放匿名用户并启动vsftpd服务[root@localhost ftp]# cd /etc/vsftpd/[root@localhost vsftpd]# lsftpusers user_list vsftpd.conf vsftpd_conf_migrate.sh [root@localhost vsftpd]# cp vsftpd.conf&#123;,.bak&#125;[root@localhost vsftpd]# lsftpusers user_list vsftpd.conf vsftpd.conf.bak vsftpd_conf_migrate .sh[root@localhost vsftpd]# vim vsftpd.confanonymous_enable=YES #匿名用户是否开启local_enable=YES #本地用户是否开启write_enable=YES #是否开启写权限local_umask=022 #用户创建目录权限掩码anon_upload_enable=YES #去注释 开启匿名用户上传权限anon_umask=022 #此条手动添加匿名用户的权限掩码anon_mkdir_write_enable=YES #去注释 开启匿名用户新建目录权限anon_other_write_enable=YES #此条手动添加匿名用户的其他写入权限(删除 移动等)[root@localhost vsftpd]# /etc/init.d/vsftpd start #启动服务Starting vsftpd for vsftpd: [ OK ][root@localhost vsftpd]# netstat -antup |grep vsftpd #查看服务端口 默认21端口tcp 0 0 0.0.0.0:21 0.0.0.0:* LISTEN 1177/vsftpd 测试匿名FTP服务器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#在客户端安装FTP客户端[root@localhost ~]# yum -y install ftp#访问FTP服务器[root@localhost ~]# ftp 192.168.200.54Connected to 192.168.200.54 (192.168.200.54).220 (vsFTPd 2.2.2)Name (192.168.200.54:root): ftp #匿名用户统一叫ftp331 Please specify the password.Password: #密码为空 直接登录230 Login successful.Remote system type is UNIX.Using binary mode to transfer files.ftp&gt; #登录成功ftp&gt; ls227 Entering Passive Mode (192,168,200,54,69,45).150 Here comes the directory listing.drwxr-xr-x 2 14 0 4096 Mar 01 2013 pub226 Directory send OK.ftp&gt; helpCommands may be abbreviated. Commands are:! debug mdir sendport site$ dir mget put sizeaccount disconnect mkdir pwd statusappend exit mls quit structascii form mode quote systembell get modtime recv suniquebinary glob mput reget tenexbye hash newer rstatus tickcase help nmap rhelp tracecd idle nlist rename typecdup image ntrans reset userchmod lcd open restart umaskclose ls prompt rmdir verbosecr macdef passive runique ?delete mdelete proxy sendftp&gt; cd pub250 Directory successfully changed.ftp&gt; ls227 Entering Passive Mode (192,168,200,54,98,48).150 Here comes the directory listing.-rw-r--r-- 1 0 0 0 Sep 25 13:53 test226 Directory send OK.ftp&gt; get test #下载测试local: test remote: test227 Entering Passive Mode (192,168,200,54,152,59).150 Opening BINARY mode data connection for test (0 bytes).226 Transfer complete.[root@localhost ~]# lsanaconda-ks.cfg httpd-2.2.9.tar.gz install.log install.log.syslog test #下载成功ftp&gt; put anaconda-ks.cfg #上传测试local: anaconda-ks.cfg remote: anaconda-ks.cfg227 Entering Passive Mode (192,168,200,54,69,33).150 Ok to send data.226 Transfer complete.1127 bytes sent in 0.0133 secs (84.63 Kbytes/sec)[root@localhost pub]# lsanaconda-ks.cfg test #上传成功 2.2 搭建本地用户验证的FTP服务1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@localhost pub]# cd /etc/vsftpd/[root@localhost vsftpd]# lsftpusers user_list vsftpd.conf vsftpd.conf.bak vsftpd_conf_migrate.sh[root@localhost vsftpd]# mv vsftpd.conf vsftpd.conf.anon #将匿名用户配 置文件备份[root@localhost vsftpd]# cp vsftpd.conf.bak vsftpd.conf #还原配置文件[root@localhost vsftpd]# vim vsftpd.confchroot_local_user=YES #96行 开启本地用的家目录锁定 用户上传下载就在普通用户的 家目录 而不是在共享目录#重启服务[root@localhost vsftpd]# /etc/init.d/vsftpd reloadShutting down vsftpd: [ OK ]Starting vsftpd for vsftpd: [ OK ][root@localhost ~]# id yunjisuanid: yunjisuan: No such user[root@localhost ~]# useradd yunjisuan #创建一个普通用户[root@localhost ~]# passwd yunjisuanChanging password for user yunjisuan.New password:BAD PASSWORD: it does not contain enough DIFFERENT charactersBAD PASSWORD: is too simple 24. Retype new password:passwd: all authentication tokens updated successfully.[root@localhost ~]# ls /home/yunjisuan[root@localhost vsftpd]# cd /home/yunjisuan/[root@localhost yunjisuan]# touch 111111[root@localhost yunjisuan]# ls111111 #在普通用户家目录创建一个测试文件#客户端测试[root@localhost ~]# ftp 192.168.200.54Connected to 192.168.200.54 (192.168.200.54).220 (vsFTPd 2.2.2)Name (192.168.200.54:root): yunjisuan #登陆用户选择yunjisuan331 Please specify the password.Password:230 Login successful.Remote system type is UNIX.Using binary mode to transfer files.ftp&gt; #登陆成功ftp&gt; ls227 Entering Passive Mode (192,168,200,54,177,182).150 Here comes the directory listing.-rw-r--r-- 1 0 0 0 Sep 25 14:31 111111226 Directory send OK.#上传 下载同匿名用户一样 2.3 搭建虚拟用户FTP服务12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485[root@localhost vsftpd]# mv vsftpd.conf vsftpd.conf.local[root@localhost vsftpd]# cp vsftpd.conf.bak vsftpd.conf #还原配置文件[root@localhost vsftpd]# vim ./vusers.list 新建一个虚拟用户账号密码列表文件zhangsan #虚拟用户123lisi123wangwu456[root@localhost vsftpd]# db_load -T -t hash -f vusers.list vusers.db # 文件加密[root@localhost vsftpd]# lsftpusers vsftpd.conf vsftpd.conf.bak vsftpd_conf_migrate.sh vusers.listuser_list vsftpd.conf.anon vsftpd.conf.local vusers.db[root@localhost vsftpd]# chmod 600 vusers.db #加600权限[root@localhost vsftpd]# ll vusers.db-rw------- 1 root root 12288 Sep 25 22:47 vusers.db#添加虚拟映射账号 为ftp根目录修改权限[root@localhost vsftpd]# useradd -d /var/ftproot/ -s /sbin/nologin virtual #-d 指定家目录[root@localhost vsftpd]# ll /var/ftproot/ #此目录为虚拟用户上传下载目录total 0[root@localhost vsftpd]# chmod 755 /var/ftproot/[root@localhost vsftpd]# cd /var/ftproot/[root@localhost ftproot]# touch 22222 [root@localhost ftproot]# ls22222 #创建测试文件#创建pam认证文件[root@localhost vsftpd]# vim /etc/pam.d/vsftpd.vuauth required pam_userdb.so db=/etc/vsftpd/vusersaccount required pam_userdb.so db=/etc/vsftpd/vusers#修改配置文件[root@localhost vsftpd]# vim vsftpd.confpam_service_name=vsftpd.vu #118行 pam认证文件guest_enable=YES #是否开启本地映射账号guest_username=virtual #虚拟映射账号#重启服务[root@localhost vsftpd]# /etc/init.d/vsftpd reloadShutting down vsftpd: [ OK ]Starting vsftpd for vsftpd: [ OK ][root@localhost vsftpd]# cat vusers.listzhangsan123lisi123wangwu456#客户端测试[root@localhost ~]# ftp 192.168.200.54Connected to 192.168.200.54 (192.168.200.54).220 (vsFTPd 2.2.2)Name (192.168.200.54:root): zhangsan331 Please specify the password.Password:230 Login successful.Remote system type is UNIX.Using binary mode to transfer files.ftp&gt; ls227 Entering Passive Mode (192,168,200,54,132,132).150 Here comes the directory listing.-rw-r--r-- 1 0 0 0 Sep 25 15:04 22222226 Directory send OK.ftp&gt; bye221 Goodbye.[root@localhost ~]# ftp 192.168.200.54Connected to 192.168.200.54 (192.168.200.54).220 (vsFTPd 2.2.2)Name (192.168.200.54:root): lisi331 Please specify the password.Password:230 Login successful.Remote system type is UNIX.Using binary mode to transfer files.ftp&gt; ls227 Entering Passive Mode (192,168,200,54,180,98).150 Here comes the directory listing.-rw-r--r-- 1 0 0 0 Sep 25 15:04 22222226 Directory send OK.ftp&gt;","categories":[{"name":"服务基础","slug":"服务基础","permalink":"https://kkabuzs.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Ftp","slug":"Ftp","permalink":"https://kkabuzs.github.io/tags/Ftp/"}]},{"title":"Kubernetes之Volume挂载","slug":"kuberneteszhivolumeguazai","date":"2018-11-21T14:06:10.000Z","updated":"2018-11-21T14:06:10.000Z","comments":true,"path":"articles/2018/11/21/kuberneteszhivolumeguazai/","permalink":"https://kkabuzs.github.io/articles/2018/11/21/kuberneteszhivolumeguazai/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Kubernetes之Volume挂载一，Volume Container中的磁盘文件是短暂的，这在容器中运行时会给非平凡的应用程序带来一些问题。首先，当容器崩溃时，kubelet将重新启动它，但文件将丢失，Container以干净状态启动。其次，当在一起运行Container时，Pod通常需要在这些容器之间共享文件。Kubernetes Volume抽象解决了这两个问题。 1.1 背景 Docker也有卷的概念，虽然它有点宽松和管理较少。在Docker中，卷只是磁盘上或另一个Container中的目录。生命周期不受管理，直到最近才有本地磁盘支持的卷。Docker现在提供了卷驱动程序，但是现在功能非常有限。 另一方面，Kubernetes卷具有明确的生命周期，与包围它的Pod相同。因此，卷可以比Pod中运行的任何Container更长，并且可以在Container重新启动之间保留数据。当然，当Pod不再存在时，音量也将不复存在。也许更重要的是，Kubernetes支持多种类型的卷，Pod可以同时使用任意数量的卷。 从本质上讲，卷只是一个目录，可能包含一些数据，Pod中的容器可以访问它。该目录是如何形成的，支持它的介质以及它的内容由所使用的特定卷类型决定。 要使用卷，Pod指定要为Pod提供的卷（.spec.volumes字段）以及将这些卷安装到容器（ .spec.containers.volumeMounts字段）的位置。 1.2 Volume类型 emptyDir hostPath gcePersistentDisk awsElasticBlockStore nfs iscsi glusterfs rbd gitRepo secret persistentVolumeClaim 二，类型介绍与创建2.1 emptyDir 一个emptyDir 第一次创建是在一个pod被指定到具体node的时候，并且会一直存在在pod的生命周期当中，正如它的名字一样，它初始化是一个空的目录，pod中的容器都可以读写这个目录，这个目录可以被挂在到各个容器相同或者不相同的的路径下。当一个pod因为任何原因被移除的时候，这些数据会被永久删除。注意：一个容器崩溃了不会导致数据的丢失，因为容器的崩溃并不移除pod. emptyDir 磁盘的作用： 1. 普通空间，基于磁盘的数据存储 2. 作为从崩溃中恢复的备份点 3. 存储那些那些需要长久保存的数据，例web服务中的数据 默认的，emptyDir磁盘会存储在主机所使用的媒介上，可能是SSD，或者网络硬盘，这主要取决于你的环境。当然，我们也可以将emptyDir.medium的值设置为Memory来告诉Kubernetes来挂在一个基于内存的目录tmpfs，因为tmpfs速度会比硬盘块多了，但是，当主机重启的时候所有的数据都会丢失 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@k8s-master01 kubernetes-yaml]# vim volume-test-pod.yamlapiVersion: v1kind: Podmetadata: name: volume-test-podspec: containers: - image: nginx name: test-nginx-volume volumeMounts: - mountPath: /NginxTestVolume name: test-valume volumes: - name: test-valume emptyDir: &#123;&#125; #创建pod[root@k8s-master01 kubernetes-yaml]# kubectl create -f volume-test-pod.yamlpod/volume-test-pod created#查看刚刚创建的pod[root@k8s-master01 kubernetes-yaml]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESfrontend-8d9b575b9-8mv4t 1/1 Running 1 24d 172.30.34.3 192.168.101.125 &lt;none&gt; &lt;none&gt;mysql-n9b57 1/1 Running 2 39d 172.30.24.3 192.168.101.62 &lt;none&gt; &lt;none&gt;tomcat-4vzs4 1/1 Running 2 39d 172.30.24.2 192.168.101.62 &lt;none&gt; &lt;none&gt;tomcat-gns9v 1/1 Running 1 39d 172.30.34.2 192.168.101.125 &lt;none&gt; &lt;none&gt;volume-test-pod 1/1 Running 0 56s 172.30.24.4 192.168.101.62 &lt;none&gt; &lt;none&gt; #在62机器#在62机器上看磁盘挂载[root@k8s-node01 ~]# df -h文件系统 容量 已用 可用 已用% 挂载点/dev/sda5 16G 8.4G 7.7G 53% /devtmpfs 1.9G 0 1.9G 0% /devtmpfs 1.9G 4.0K 1.9G 1% /dev/shmtmpfs 1.9G 175M 1.7G 10% /runtmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup/dev/sda1 20G 45M 19G 1% /data/dev/sda2 9.8G 37M 9.2G 1% /hometmpfs 380M 0 380M 0% /run/user/1000tmpfs 1.9G 12K 1.9G 1% /var/lib/kubelet/pods/7d1708ff-6658-11e9-b265-005056a6c65d/volumes/kubernetes.io~secret/default-token-4v8dp #已经有了，是tmpfs文件格式tmpfs 1.9G 12K 1.9G 1% /var/lib/kubelet/pods/448848ff-6645-11e9-b265-005056a6c65d/volumes/kubernetes.io~secret/default-token-4v8dpoverlay 16G 8.4G 7.7G 53% /var/lib/docker/overlay2/850432dab03c894b99b795a4037c22940ff4410dd95955f954bc7e9d31437ff7/mergedshm 64M 0 64M 0% /var/lib/docker/containers/86ef62bd4238371e8809abb8b4ebfd57ff253770f181bcc3e6fba6de35683bab/mounts/shmoverlay 16G 8.4G 7.7G 53% /var/lib/docker/overlay2/f44572ef3d5b611e4c737f4463242fa06bb1faae88f317a555f04201e3d9ce4c/mergedoverlay 16G 8.4G 7.7G 53% /var/lib/docker/overlay2/c05fb67f67f21e01bd81fcc18b7d0c62d06a5ee29257cc081350d9b9c6aff311/mergedshm 64M 0 64M 0% /var/lib/docker/containers/12792092fe67cb131a0ae613ebf6739275ef0cbc1176ee84303848f68968132e/mounts/shmoverlay 16G 8.4G 7.7G 53% /var/lib/docker/overlay2/674d9417bc6793b81e9b889acd892323a4c5f12fd5ee18f6b2baca4bdbf9d7a3/mergedtmpfs 380M 0 380M 0% /run/user/0tmpfs 1.9G 12K 1.9G 1% /var/lib/kubelet/pods/43093572-85ad-11e9-8f67-005056a6c65d/volumes/kubernetes.io~secret/default-token-4v8dpoverlay 16G 8.4G 7.7G 53% /var/lib/docker/overlay2/8a631aaea296f94a285720fb81428406c33f16ab714e2c5e15315b4ce77dd3a7/mergedshm 64M 0 64M 0% /var/lib/docker/containers/573857b1debec09fbc1edbaaa6d75adba9afb8e275c68d2558c5bbea893fa49a/mounts/shmoverlay 16G 8.4G 7.7G 53% /var/lib/docker/overlay2/cb7fcf86e9fdceed098ae788dc73c47703289664b73d4187247e8d355b87ccc7/merged 三，PV PVC实现 管理存储是管理计算的一个明显问题。该PersistentVolume子系统为用户和管理员提供了一个API，用于抽象如何根据消费方式提供存储的详细信息。为此，我们引入了两个新的API资源：PersistentVolume和PersistentVolumeClaim。 PersistentVolume（PV）是群集中由管理员配置的一块存储。它是集群中的资源，就像节点是集群资源一样。PV是容量插件，如Volumes，但其生命周期独立于使用PV的任何单个pod。此API对象捕获存储实现的详细信息，包括NFS，iSCSI或特定于云提供程序的存储系统。 PersistentVolumeClaim（PVC）是由用户进行存储的请求。它类似于一个吊舱。Pod消耗节点资源，PVC消耗PV资源。Pod可以请求特定级别的资源（CPU和内存）。声明可以请求特定的大小和访问模式（例如，可以一次读&#x2F;写或多次只读）。 虽然PersistentVolumeClaims允许用户使用抽象存储资源，但是PersistentVolumes对于不同的问题，用户通常需要具有不同属性（例如性能）。群集管理员需要能够提供各种PersistentVolumes不同的方式，而不仅仅是大小和访问模式，而不会让用户了解这些卷的实现方式。对于这些需求，有StorageClass 资源。 3.1 持久化存储卷和声明的生命周期 在Kubernetes集群中，PV 作为存储资源存在。PVC 是对PV资源的请求和使用，也是对PV存储资源的”提取证”，而Pod通过PVC来使用PV。PV 和 PVC 之间的交互过程有着自己的生命周期，这个生命周期分为5个阶段： 供应(Provisioning)：即PV的创建，可以直接创建PV（静态方式），也可以使用StorageClass动态创建 绑定（Binding）：将PV分配给PVC 使用（Using）：Pod通过PVC使用该Volume 释放（Releasing）：Pod释放Volume并删除PVC 回收（Reclaiming）：回收PV，可以保留PV以便下次使用，也可以直接从云存储中删除 根据上述的5个阶段，存储卷的存在下面的4种状态： Available：可用状态，处于此状态表明PV以及准备就绪了，可以被PVC使用了。 Bound：绑定状态，表明PV已被分配给了PVC。 Released：释放状态，表明PVC解绑PV，但还未执行回收策略。 Failed：错误状态，表明PV发生错误。 3.2 访问模式（Access Modes） 只要资源提供者支持，持久卷能够通过任何方式加载到主机上。每种存储都会有不同的能力，每个PV的访问模式也会被设置成为该卷所支持的特定模式。例如NFS能够支持多个读写客户端，但某个NFS PV可能会在服务器上以只读方式使用。每个PV都有自己的一系列的访问模式，这些访问模式取决于PV的能力。 访问模式的可选范围如下： ReadWriteOnce：该卷能够以读写模式被加载到一个节点上。 ReadOnlyMany：该卷能够以只读模式加载到多个节点上。 ReadWriteMany：该卷能够以读写模式被多个节点同时加载。 在 CLI 下，访问模式缩写为： RWO：ReadWriteOnce ROX：ReadOnlyMany RWX：ReadWriteMany 一个卷不论支持多少种访问模式，同时只能以一种访问模式加载。例如一个 GCEPersistentDisk既能支持ReadWriteOnce，也能支持ReadOnlyMany。 3.3 NFS实现Volume挂载环境介绍 主机名 IP 介绍 k8s-master01 192.168.101.61 k8s master节点 k8s-node01 192.168.101.62 k8s node节点 k8s-node02 192.168.101.125 k8s node节点、nfs服务端 3.3.1 NFS服务端配置123456789101112131415161718192021222324252627282930#服务端安装nfs（centos 7 nfs和rpcbind集成在一起了）[root@k8s-node02 ~]# yum -y install nfs-utils #客户端也需安装#编辑配置文件[root@k8s-node02 ~]# cat /etc/exports/data 192.168.101.0/24(rw,sync,insecure,no_subtree_check,no_root_squash)#更改属主属组[root@k8s-node02 ~]# chown nfsnobody.nfsnobody /data#启动服务[root@k8s-node02 ~]# systemctl start rpcbind[root@k8s-node02 ~]# systemctl enable rpcbind[root@k8s-node02 ~]# systemctl start nfs[root@k8s-node02 ~]# systemctl enable nfs#本地挂载测试[root@k8s-node02 ~]# mount 192.168.101.125:/data /mnt[root@k8s-node02 ~]# df -h文件系统 容量 已用 可用 已用% 挂载点/dev/sda5 16G 8.1G 8.0G 51% /devtmpfs 1.9G 0 1.9G 0% /devtmpfs 1.9G 0 1.9G 0% /dev/shmtmpfs 1.9G 191M 1.7G 11% /runtmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup/dev/sda1 20G 45M 19G 1% /data/dev/sda2 9.8G 54M 9.2G 1% /home...省略若干...tmpfs 380M 0 380M 0% /run/user/0192.168.101.125:/data 20G 44M 19G 1% /mnt #成功。 3.3.2 NFS作为Volume nfs可以直接作为一个存储卷使用，下面是一个reids部署的yaml配置文件。再此示例中，redis在容器中的持久化数据保存在&#x2F;data目录下；存储卷使用nfs，nfs的服务地址为：192.168.101.125，存储路径为：&#x2F;data。容器通过volumeMounts.name的值确定所使用的存储卷。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[root@k8s-master01 kubernetes-yaml]# vim redis.yamlapiVersion: apps/v1beta1kind: Deploymentmetadata: name: redisspec: selector: matchLabels: app: redis revisionHistoryLimit: 2 template: metadata: labels: app: redis spec: containers:#应用的镜像 - image: redis name: redis imagePullPolicy: IfNotPresent#应用的内部端口 ports: - containerPort: 6379 name: redis6379 env: - name: ALLOW_EMPTY_PASSWORD value: &quot;yes&quot; - name: REDIS_PASSWORD value: &quot;redis&quot;#持久化挂接位置，在docker中 volumeMounts: - name: redis-persistent-storage mountPath: /data#宿主机上的目录 volumes: - name: redis-persistent-storage nfs: path: /data server: 192.168.101.125#创建Deployment[root@k8s-master01 kubernetes-yaml]# kubectl create -f redis.yamldeployment.apps/redis created#查看挂载结果[root@k8s-master01 kubernetes-yaml]# kubectl get podsNAME READY STATUS RESTARTS AGEfrontend-8d9b575b9-8mv4t 1/1 Running 1 31dmysql-zq255 1/1 Running 1 3d17hredis-5f66b6446b-g4q7c 1/1 Running 0 4m5stomcat-bk4fd 1/1 Running 0 3d17htomcat-gns9v 1/1 Running 1 46d[root@k8s-master01 kubernetes-yaml]# kubectl exec -it redis-5f66b6446b-g4q7c bash #进入podroot@redis-5f66b6446b-g4q7c:/data# lslost+found redisroot@redis-5f66b6446b-g4q7c:/data# cd redis/root@redis-5f66b6446b-g4q7c:/data/redis# lsdump.rdb redis.log#查看nfs服务器的宿主机目录[root@k8s-node02 ~]# ls /data/lost+found redis[root@k8s-node02 ~]# ls /data/redis/dump.rdb redis.log #挂载成功 3.4 NFS实现PV PVC3.4.1 NFS实现PersistentVolum(PV) 在Kubernetes当前版本的中，可以创建类型为nfs的持久化存储卷，用于为PersistentVolumClaim提供存储卷。在下面的PersistenVolume YAML配置文件中，定义了一个名为nfs-pv的持久化存储卷，此存储卷提供了5G的存储空间，只能由一个PersistentVolumClaim进行可读可写操作。此持久化存储卷使用的nfs服务器地址为192.168.101.125，存储的路径为&#x2F;data。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118#创建多个目录，指定多个PV[root@k8s-node02 ~]# mkdir -p /data/&#123;1,2,3,4,5&#125;[root@k8s-node02 ~]# ls /data/1 2 3 4 5[root@k8s-node02 ~]# vim /etc/exports #修改配置文件/data/1 192.168.101.0/24(rw,no_root_squash)/data/2 192.168.101.0/24(rw,no_root_squash)/data/3 192.168.101.0/24(rw,no_root_squash)/data/4 192.168.101.0/24(rw,no_root_squash)/data/5 192.168.101.0/24(rw,no_root_squash)[root@k8s-node02 ~]# systemctl restart nfs #重启服务[root@k8s-node02 ~]# showmount -eExport list for k8s-node02:/data/5 192.168.101.0/24/data/4 192.168.101.0/24/data/3 192.168.101.0/24/data/2 192.168.101.0/24/data/1 192.168.101.0/24#将这五个存储空间定义成pv[root@k8s-master01 kubernetes-yaml]# vim nfs-pv.yamlapiVersion: v1kind: PersistentVolumemetadata: name: nfs-pv01 labels: zhaoshuo: pv01spec: capacity: storage: 1Gi accessModes: - ReadWriteMany # 此持久化存储卷使用nfs插件 nfs: # nfs共享目录为/data path: /data/1 # nfs服务器的地址 server: 192.168.101.125---apiVersion: v1kind: PersistentVolumemetadata: name: nfs-pv02 labels: zhaoshuo: pv02spec: capacity: storage: 2Gi accessModes: - ReadWriteMany nfs: path: /data/2 server: 192.168.101.125---apiVersion: v1kind: PersistentVolumemetadata: name: nfs-pv03 labels: zhaoshuo: pv03spec: capacity: storage: 3Gi accessModes: - ReadWriteOnce nfs: path: /data/3 server: 192.168.101.125---apiVersion: v1kind: PersistentVolumemetadata: name: nfs-pv04 labels: zhaoshuo: pv04spec: capacity: storage: 4Gi accessModes: - ReadWriteMany nfs: path: /data/4 server: 192.168.101.125---apiVersion: v1kind: PersistentVolumemetadata: name: nfs-pv05 labels: zhaoshuo: pv05spec: capacity: storage: 5Gi accessModes: - ReadWriteMany nfs: path: /data/5 server: 192.168.101.125#创建PV[root@k8s-master01 kubernetes-yaml]# kubectl create -f nfs-pv.yamlpersistentvolume/nfs-pv01 createdpersistentvolume/nfs-pv02 createdpersistentvolume/nfs-pv03 createdpersistentvolume/nfs-pv04 createdpersistentvolume/nfs-pv05 created[root@k8s-master01 kubernetes-yaml]# kubectl get pv #pv已经运行起来了NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEnfs-pv01 1Gi RWX Retain Available 27snfs-pv02 2Gi RWX Retain Available 27snfs-pv03 3Gi RWO Retain Available 27snfs-pv04 4Gi RWX Retain Available 27snfs-pv05 5Gi RWX Retain Available 27s 存储卷创建成功后将处于可用状态，等待PersistentVolumClaim使用。PersistentVolumClaim会通过访问模式和存储空间自动选择合适存储卷，并与其进行绑定。 3.4.2 创建PersistenetVolumeClaim(PVC) PersistenetVolumeClaim是对PersistenetVolume的声明，即PersistenetVolume为存储的提供者，而PersistenetVolumeClaim为存储的消费者。 12345678910111213141516171819202122232425262728[root@k8s-master01 kubernetes-yaml]# vim nfs-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfs-pvc namespace: defaultspec: accessModes: - ReadWriteMany #此访问方式必须与pv的方式相同 resources: requests: storage: 1Gi #需求1G的存储空间 (如果不指定名字，会根据以上两条规则匹配) #创建pvc[root@k8s-master01 kubernetes-yaml]# kubectl create -f nfs-pvc.yamlpersistentvolumeclaim/nfs-pvc created[root@k8s-master01 kubernetes-yaml]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEnfs-pvc Bound nfs-pv01 1Gi RWX 3s#再次查看pv状态[root@k8s-master01 kubernetes-yaml]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEnfs-pv01 1Gi RWX Retain Bound default/nfs-pvc 11m #已经被绑定nfs-pv02 2Gi RWX Retain Available 11mnfs-pv03 3Gi RWO Retain Available 11mnfs-pv04 4Gi RWX Retain Available 11mnfs-pv05 5Gi RWX Retain Available 11m 3.4.3 创建一个pod，使用pvc12345678910111213141516171819202122232425262728293031323334353637383940414243[root@k8s-master01 kubernetes-yaml]# vim mypod.yamlapiVersion: v1kind: Podmetadata: name: mypodspec: containers: - name: mypod image: nginx volumeMounts: - mountPath: &quot;/usr/share/nginx/html&quot; name: mypod volumes: - name: mypod persistentVolumeClaim: claimName: nfs-pvc [root@k8s-master01 kubernetes-yaml]# kubectl create -f mypod.yamlpod/mypod created[root@k8s-master01 kubernetes-yaml]# kubectl get podNAME READY STATUS RESTARTS AGEfrontend-8d9b575b9-8mv4t 1/1 Running 3 40dmypod 1/1 Running 0 40s #已经创建mysql-zq255 1/1 Running 1 12dtomcat-bk4fd 1/1 Running 1 12dtomcat-gns9v 1/1 Running 3 55d[root@k8s-master01 kubernetes-yaml]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESfrontend-8d9b575b9-8mv4t 1/1 Running 3 40d 172.30.34.3 192.168.101.125 &lt;none&gt; &lt;none&gt;mypod 1/1 Running 0 2m53s 172.30.49.2 192.168.101.62 &lt;none&gt; &lt;none&gt;mysql-zq255 1/1 Running 1 12d 172.30.34.5 192.168.101.125 &lt;none&gt; &lt;none&gt;tomcat-bk4fd 1/1 Running 1 12d 172.30.34.4 192.168.101.125 &lt;none&gt; &lt;none&gt;tomcat-gns9v 1/1 Running 3 55d 172.30.34.2 192.168.101.125 &lt;none&gt; &lt;none&gt;#去这个pv对应的存储空间目录创建index文件测试挂载[root@k8s-node02 ~]# cd /data/1/[root@k8s-node02 1]# ls[root@k8s-node02 1]# vim index.html[root@k8s-node02 1]# curl 172.30.49.2dsadasdwq#完成静态存储 https://www.kubernetes.org.cn/4069.html","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kkabuzs.github.io/tags/Kubernetes/"}]},{"title":"Harbor私有镜像仓库","slug":"harborsiyoujingxiangcangku","date":"2018-11-15T08:26:48.000Z","updated":"2018-11-15T08:26:48.000Z","comments":true,"path":"articles/2018/11/15/harborsiyoujingxiangcangku/","permalink":"https://kkabuzs.github.io/articles/2018/11/15/harborsiyoujingxiangcangku/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Harbor私有镜像仓库一，Harbor简介 Harbor可帮助用户迅速搭建企业级注册服务。它提供了管理图形界面，基于角色的访问控制（Role Based Access Control），镜像远程复制（同步），AD&#x2F;LDAP集成，以及审计日志等企业用户需求的功能，同时还原生支持中文。 该项目自推出以来，在Github获得了超过3300多个star和900多个forks。 1.1 基于角色的访问控制 用户与Docker镜像仓库通过“项目”进行组织管理，一个用户可以对多个镜像仓库在同一命名空间（project）里有不同的权限。 1.2 图形化用户界面 用户可以通过浏览器来浏览，检索当前Docker镜像仓库，管理项目和命名空间 1.3 审计管理 所有针对镜像仓库的操作都可以被记录追溯，用于审计管理。 1.4 国际化 基于英文与中文语言进行了本地化。可以增加更多的语言支持。 1.5 RESTful API 提供给管理员对于Harbor更多的操控，使得与其他管理软件集成变得更容易。 1.6 LDAP认证1.7 镜像复制 基于策略的Docker镜像复制功能，可在不同的数据中心，不同的运行环境之间同步镜像，并提供有好的管理界面，大大简化了实际运维中的镜像管理工作。 1.8 与Clair集成 与Clair集成，添加漏洞扫描功能。Clair是coreos开源的容器漏洞扫描工具，在容器逐渐普及的今天，容器镜像安全问题日益严重。Clair是目前少数的开源安全扫描工具。 1.9 Notary签名工具 Notary是Docker镜像的签名工具，用来保证镜像在pull，push和传输工程中的一致性和完整性，避免中间人攻击，避免非法的镜像更新和运行。 二，为Harbor签发域名证书 openssl是目前最流行的SSL密码库工具，提供了一个通用，功能完备的工具套件，用以支持SSL&#x2F;TLS协议的实现。官网：https://www.openssl.org/source/ 环境准备 主机名 ip 用途 最小资源配比 最佳资源配比 harbor-master 192.168.200.103 harbor私有镜像仓库 2CPU 4CPU 4G mem 8G mem 1234567[root@harbor-master ~]# hostname -I192.168.200.103 [root@harbor-master ~]# uname -r3.10.0-862.el7.x86_64[root@harbor-master ~]# cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core) 官方文档https://github.com/goharbor/harbor/blob/master/docs/configure_https.md 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#创建自己的CA证书[root@harbor-master ~]# mkdir -p /data/ssl[root@harbor-master ~]# cd /data/ssl/[root@harbor-master ssl]# which openssl/usr/bin/openssl[root@harbor-master ssl]# openssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 365 -out ca.crtGenerating a 4096 bit RSA private key..................................................................++.................................................++writing new private key to &#x27;ca.key&#x27;-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter &#x27;.&#x27;, the field will be left blank.-----Country Name (2 letter code) [XX]:CN #国家的名字State or Province Name (full name) []:Beijing #城市名字Locality Name (eg, city) [Default City]:BeijingOrganization Name (eg, company) [Default Company Ltd]:yunjisuan #公司名字Organizational Unit Name (eg, section) []:yunjisuanCommon Name (eg, your name or your server&#x27;s hostname) []:www.yunjisuan.com #域名Email Address []:#生成签名证书请求[root@harbor-master ssl]# openssl req -newkey rsa:4096 -nodes -sha256 -keyout www.yunjisuan.com.key -out www.yunjisuan.com.csrGenerating a 4096 bit RSA private key...................................................++....++writing new private key to &#x27;www.yunjisuan.com.key&#x27;-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter &#x27;.&#x27;, the field will be left blank.-----Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:BeijingLocality Name (eg, city) [Default City]:BeijingOrganization Name (eg, company) [Default Company Ltd]:yunjisuanOrganizational Unit Name (eg, section) []:yunjisuanCommon Name (eg, your name or your server&#x27;s hostname) []:www.yunjisuan.comEmail Address []:Please enter the following &#x27;extra&#x27; attributesto be sent with your certificate requestA challenge password []:An optional company name []:#生成注册表主机的证书[root@harbor-master ssl]# openssl x509 -req -days 365 -in www.yunjisuan.com.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out www.yunjisuan.com.crtSignature oksubject=/C=CN/ST=Beijing/L=Beijing/O=yunjisuan/OU=yunjisuan/CN=www.yunjisuan.comGetting CA Private Key[root@harbor-master ssl]# lltotal 24-rw-r--r--. 1 root root 2049 Jan 14 23:45 ca.crt-rw-r--r--. 1 root root 3272 Jan 14 23:45 ca.key-rw-r--r--. 1 root root 17 Jan 15 00:00 ca.srl-rw-r--r--. 1 root root 1931 Jan 15 00:00 www.yunjisuan.com.crt-rw-r--r--. 1 root root 1716 Jan 14 23:56 www.yunjisuan.com.csr-rw-r--r--. 1 root root 3272 Jan 14 23:56 www.yunjisuan.com.key 三，信任自签发的域名证书 由于CA证书是我们自己签发的，linux操作系统是不信任的 ，因此，需要把证书加到系统的信任证书里。 123456789#将自签ca证书添加到系统信任[root@harbor-master ssl]# pwd/data/ssl[root@harbor-master ssl]# cp www.yunjisuan.com.crt /etc/pki/ca-trust/source/anchors/#让ca认证立刻生效[root@harbor-master ssl]# update-ca-trust enable[root@harbor-master ssl]# update-ca-trust extract 让证书立刻生效之后，要重启docker服务，否则无效。 四，Harbor 1.4 版本配置与安装4.1 安装docker-ce社区版123456789101112131415161718192021222324252627[root@harbor-master ~]# sestatus SELinux status: disabled#安装依赖包[root@harbor-master ~]# yum -y install yum-utils device-mapper-persistent-data lvm2#添加docker的CE版本的yum源配置文件[root@harbor-master ~]# curl https://download.docker.com/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 2424 100 2424 0 0 2544 0 --:--:-- --:--:-- --:--:-- 2543[root@harbor-master ~]# ll /etc/yum.repos.d/docker-ce.repo-rw-r--r-- 1 root root 2424 10月 13 21:12 /etc/yum.repos.d/docker-ce.repo#安装CE版本的docker[root@harbor-master ~]# yum -y install docker-ce[root@harbor-master ~]# systemctl start docker #启动docker[root@harbor-master ~]# systemctl enable docker #添加开机启动Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.[root@harbor-master ~]# docker version #查看docker版本Client: Version: 18.06.1-ce API version: 1.38 Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17:23:03 2018 OS/Arch: linux/amd64 Experimental: false 4.2 下载并安装Harbor私有仓库123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687#创建harbor的证书目录，并复制[root@harbor-master ~]# mkdir -p /etc/ssl/harbor[root@harbor-master ~]# cp /data/ssl/www.yunjisuan.com.key /etc/ssl/harbor/[root@harbor-master ~]# cp /data/ssl/www.yunjisuan.com.crt /etc/ssl/harbor/[root@harbor-master ~]# ll /etc/ssl/harbor/total 8-rw-r--r-- 1 root root 1931 Jan 15 00:16 www.yunjisuan.com.crt-rw-r--r-- 1 root root 3272 Jan 15 00:16 www.yunjisuan.com.key#创建harbor下载目录，并下载harbor-offline-installer-v1.5.0.tgz[root@harbor-master ~]# mkdir -p /data/install[root@harbor-master ~]# cd /data/install/[root@harbor-master install]# pwd/data/install[root@harbor-master install]# wget http://harbor.orientsoft.cn/harbor-v1.5.0/harbor-offline-installer-v1.5.0.tgz[root@harbor-master install]# ls harbor-offline-installer-v1.5.0.tgz[root@harbor-master install]# tar xf harbor-offline-installer-v1.5.0.tgz [root@harbor-master install]# lsharbor harbor-offline-installer-v1.5.0.tgz[root@harbor-master install]# cd harbor[root@harbor-master harbor]# lltotal 854960drwxr-xr-x 3 root root 23 Jan 15 00:58 common #模板目录-rw-r--r-- 1 root root 1185 May 2 2018 docker-compose.clair.yml-rw-r--r-- 1 root root 1725 May 2 2018 docker-compose.notary.yml-rw-r--r-- 1 root root 3596 May 2 2018 docker-compose.ymldrwxr-xr-x 3 root root 156 May 2 2018 ha #高可用配置-rw-r--r-- 1 root root 6687 May 2 2018 harbor.cfg #harbor配置文件-rw-r--r-- 1 root root 875401338 May 2 2018 harbor.v1.5.0.tar.gz-rwxr-xr-x 1 root root 5773 May 2 2018 install.sh-rw-r--r-- 1 root root 10771 May 2 2018 LICENSE-rw-r--r-- 1 root root 482 May 2 2018 NOTICE-rwxr-xr-x 1 root root 27379 May 2 2018 prepare[root@harbor-master harbor]# cp harbor.cfg&#123;,.bak&#125;#修改habor.cfg配置文件[root@harbor-master harbor]# cat -n harbor.cfg.bak | sed -n &#x27;7p;11p;23p;24p;68p&#x27; 7 hostname = reg.mydomain.com #修改成证书的域名 11 ui_url_protocol = http #启用加密传输https 23 ssl_cert = /data/cert/server.crt #证书位置 24 ssl_cert_key = /data/cert/server.key #证书密钥位置 68 harbor_admin_password = Harbor12345 #默认管理员及密码[root@harbor-master harbor]# cat -n harbor.cfg | sed -n &#x27;7p;11p;23p;24p;68p&#x27; 7 hostname = www.yunjisuan.com 11 ui_url_protocol = https 23 ssl_cert = /etc/ssl/harbor/www.yunjisuan.com.crt 24 ssl_cert_key = /etc/ssl/harbor/www.yunjisuan.com.key 68 harbor_admin_password = Harbor12345#安装命令docker-compose（需要1.21版本）[root@harbor-master ~]# curl -L https://github.com/docker/compose/releases/download/1.21.2/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 617 0 617 0 0 367 0 --:--:-- 0:00:01 --:--:-- 367100 10.3M 100 10.3M 0 0 289k 0 0:00:36 0:00:36 --:--:-- 573k[root@harbor-master ~]# ll /usr/local/bin/docker-compose -rw-r--r-- 1 root root 10858808 Jan 15 01:44 /usr/local/bin/docker-compose[root@harbor-master ~]# chmod +x /usr/local/bin/docker-compose [root@harbor-master ~]# which docker-compose/usr/local/bin/docker-compose[root@harbor-master ~]# docker-compose --versiondocker-compose version 1.21.2, build a133471#安装harbor私有镜像仓库[root@harbor-master ~]# cd /data/install/harbor[root@harbor-master harbor]# ./install.sh --with-notary --with-clair #--with-notary:镜像签名功能 #--with-clair:漏洞扫描功能#查看harbor启动的镜像[root@harbor-master harbor]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES90c62ec359ff vmware/nginx-photon:v1.5.0 &quot;nginx -g &#x27;daemon of…&quot; 23 seconds ago Up 20 seconds (health: starting) 0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp nginxfeaa06731077 vmware/harbor-jobservice:v1.5.0 &quot;/harbor/start.sh&quot; 23 seconds ago Up 8 seconds harbor-jobservicece7a51285631 vmware/notary-server-photon:v0.5.1-v1.5.0 &quot;/bin/server-start.sh&quot; 24 seconds ago Up 22 seconds notary-server2f59412d3e6e vmware/harbor-ui:v1.5.0 &quot;/harbor/start.sh&quot; 25 seconds ago Up 23 seconds (health: starting) harbor-uif58ad9931c17 vmware/clair-photon:v2.0.1-v1.5.0 &quot;/docker-entrypoint.…&quot; 25 seconds ago Up 1 second (health: starting) 6060-6061/tcp claira8c46d927e1b vmware/notary-signer-photon:v0.5.1-v1.5.0 &quot;/bin/signer-start.sh&quot; 26 seconds ago Up 24 seconds notary-signerfb6de4a74c1c vmware/postgresql-photon:v1.5.0 &quot;/entrypoint.sh post…&quot; 27 seconds ago Up 24 seconds (health: starting) 5432/tcp clair-dba233f7affef7 vmware/harbor-adminserver:v1.5.0 &quot;/harbor/start.sh&quot; 27 seconds ago Up 25 seconds (health: starting) harbor-adminserverea71c3eb0e26 vmware/harbor-db:v1.5.0 &quot;/usr/local/bin/dock…&quot; 27 seconds ago Up 25 seconds (health: starting) 3306/tcp harbor-db86ea4c257b89 vmware/registry-photon:v2.6.2-v1.5.0 &quot;/entrypoint.sh serv…&quot; 27 seconds ago Up 24 seconds (health: starting) 5000/tcp registry16b6e4bdc01f vmware/mariadb-photon:v1.5.0 &quot;/usr/local/bin/dock…&quot; 27 seconds ago Up 26 seconds 3306/tcp notary-dbe7e5425be2b3 vmware/redis-photon:v1.5.0 &quot;docker-entrypoint.s…&quot; 27 seconds ago Up 26 seconds 6379/tcp redisc022f1ce72dc vmware/harbor-log:v1.5.0 &quot;/bin/sh -c /usr/loc…&quot; 29 seconds ago Up 27 seconds (health: starting) 127.0.0.1:1514-&gt;10514/tcp harbor-log 通过浏览器进行访问测试 https://192.168.200.103/ 需要修改一下安全模式 项目创建：仅管理员不允许自动注册 五，镜像管理与安全：漏洞扫描和镜像签名5.1 添加docker国内公有镜像源1234567[root@harbor-master harbor]# cat /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;:[ &quot;https://registry.docker-cn.com&quot; ]&#125;[root@harbor-master harbor]# systemctl daemon-reload[root@harbor-master harbor]# systemctl restart docker 5.2 重新启动harbor私有镜像仓库1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#让harbor修改过的配置立刻生效[root@harbor-master harbor]# ./prepare Clearing the configuration file: ./common/config/adminserver/envClearing the configuration file: ./common/config/ui/envClearing the configuration file: ./common/config/ui/app.confClearing the configuration file: ./common/config/ui/private_key.pemClearing the configuration file: ./common/config/db/envClearing the configuration file: ./common/config/jobservice/envClearing the configuration file: ./common/config/jobservice/config.ymlClearing the configuration file: ./common/config/registry/config.ymlClearing the configuration file: ./common/config/registry/root.crtClearing the configuration file: ./common/config/nginx/conf.d/notary.upstream.confClearing the configuration file: ./common/config/nginx/conf.d/notary.server.confClearing the configuration file: ./common/config/nginx/cert/www.yunjisuan.com.crtClearing the configuration file: ./common/config/nginx/cert/www.yunjisuan.com.keyClearing the configuration file: ./common/config/nginx/nginx.confClearing the configuration file: ./common/config/log/logrotate.confClearing the configuration file: ./common/config/notary/mysql-initdb.d/initial-notarysigner.sqlClearing the configuration file: ./common/config/notary/mysql-initdb.d/initial-notaryserver.sqlClearing the configuration file: ./common/config/notary/notary-signer.crtClearing the configuration file: ./common/config/notary/notary-signer.keyClearing the configuration file: ./common/config/notary/notary-signer-ca.crtClearing the configuration file: ./common/config/notary/root.crtClearing the configuration file: ./common/config/notary/signer-config.jsonClearing the configuration file: ./common/config/notary/server-config.jsonClearing the configuration file: ./common/config/notary/signer_envClearing the configuration file: ./common/config/clair/postgresql-init.d/README.mdClearing the configuration file: ./common/config/clair/postgres_envClearing the configuration file: ./common/config/clair/config.yamlClearing the configuration file: ./common/config/clair/clair_envloaded secret from file: /data/secretkeyGenerated configuration file: ./common/config/nginx/nginx.confGenerated configuration file: ./common/config/adminserver/envGenerated configuration file: ./common/config/ui/envGenerated configuration file: ./common/config/registry/config.ymlGenerated configuration file: ./common/config/db/envGenerated configuration file: ./common/config/jobservice/envGenerated configuration file: ./common/config/jobservice/config.ymlGenerated configuration file: ./common/config/log/logrotate.confGenerated configuration file: ./common/config/jobservice/config.ymlGenerated configuration file: ./common/config/ui/app.confGenerated certificate, key file: ./common/config/ui/private_key.pem, cert file: ./common/config/registry/root.crtThe configuration files are ready, please use docker-compose to start the service.#清理所有harbor容器进程[root@harbor-master harbor]# docker-compose downStopping nginx ... doneStopping harbor-jobservice ... doneStopping harbor-ui ... doneStopping redis ... doneStopping harbor-log ... doneRemoving nginx ... doneRemoving harbor-jobservice ... doneRemoving harbor-ui ... doneRemoving harbor-adminserver ... doneRemoving harbor-db ... doneRemoving registry ... doneRemoving redis ... doneRemoving harbor-log ... doneRemoving network harbor_harbor#后台启动所有harbor容器进程[root@harbor-master harbor]# docker-compose up -dCreating network &quot;harbor_harbor&quot; with the default driverCreating harbor-log ... doneCreating harbor-adminserver ... doneCreating redis ... doneCreating harbor-db ... doneCreating registry ... doneCreating harbor-ui ... doneCreating harbor-jobservice ... doneCreating nginx ... done 5.3 下载一个公有镜像上传到harbor123456789101112131415161718192021222324252627282930313233343536#harbor本地下载一个公有镜像centos:7[root@harbor-master harbor]# docker pull centosUsing default tag: latestlatest: Pulling from library/centosa02a4930cb5d: Pull complete Digest: sha256:184e5f35598e333bfa7de10d8fb1cebb5ee4df5bc0f970bf2b1e7c7345136426Status: Downloaded newer image for centos:latest#本地映射私有仓库域名[root@harbor-master harbor]# tail -1 /etc/hosts192.168.200.103 www.yunjisuan.com#将centos镜像改名并上传到私有镜像仓库[root@harbor-master harbor]# docker tag centos:latest www.yunjisuan.com/library/centos:7[root@harbor-master harbor]# docker images | grep centoscentos latest 1e1148e4cc2c 5 weeks ago 202MBwww.yunjisuan.com/library/centos 7 1e1148e4cc2c 5 weeks ago 202MB[root@harbor-master harbor]# docker push www.yunjisuan.com/library/centos:7The push refers to repository [www.yunjisuan.com/library/centos]071d8bd76517: Preparing denied: requested access to the resource is denied #连接拒绝，因为还没有进行认证#登陆验证harbor私有镜像仓库，并上传镜像[root@harbor-master harbor]# docker login www.yunjisuan.comUsername: adminPassword: #密码Harbor12345WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded[root@harbor-master harbor]# docker push www.yunjisuan.com/library/centos:7The push refers to repository [www.yunjisuan.com/library/centos]071d8bd76517: Pushed 7: digest: sha256:365fc7f33107869dfcf2b3ba220ce0aa42e16d3f8e8b3c21d72af1ee622f0cf0 size: 529 5.4 登陆浏览器查看镜像上传结果，并扫描漏洞 5.5 设置镜像安全等级 5.6 为docker客户端下发域名证书 主机名 ip 用途 最小资源配比 最佳资源配比 docker-client 192.168.200.105 docker客户端 harbor-master 192.168.200.103 harbor私有镜像仓库 2CPU 4CPU 4G mem 8G mem 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#映射harbor私有仓库域名[root@docker-client ~]# cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core) [root@docker-client ~]# uname -r3.10.0-862.el7.x86_64[root@docker-client ~]# hostname -I192.168.200.105 [root@docker-client ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.103 www.yunjisuan.com#安装docker-ce社区版[root@docker-client ~]# sestatusSELinux status: disabled[root@docker-client ~]# systemctl stop firewalld[root@docker-client ~]# systemctl disable firewalld[root@docker-client ~]# yum -y install yum-utils device-mapper-persistent-data lvm2[root@docker-client ~]# curl https://download.docker.com/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 2424 100 2424 0 0 486 0 0:00:04 0:00:04 --:--:-- 698[root@docker-client ~]# yum -y install docker-ce[root@docker-client ~]# systemctl start docker[root@docker-client ~]# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.[root@docker-client ~]# docker versionClient: Version: 18.09.1 API version: 1.39 Go version: go1.10.6 Git commit: 4c52b90 Built: Wed Jan 9 19:35:01 2019 OS/Arch: linux/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 18.09.1 API version: 1.39 (minimum version 1.12) Go version: go1.10.6 Git commit: 4c52b90 Built: Wed Jan 9 19:06:30 2019 OS/Arch: linux/amd64 Experimental: false#配置国内镜像源[root@docker-client ~]# cat /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;:[ &quot;https://registry.docker-cn.com&quot; ]&#125;[root@docker-client ~]# systemctl daemon-reload[root@docker-client ~]# systemctl restart docker#下载mongo公有镜像[root@docker-client ~]# docker pull mongoUsing default tag: latestlatest: Pulling from library/mongob849b56b69e7: Pull complete 42986ef25bcd: Pull complete d927c1b717ec: Pull complete 15b86ea20233: Pull complete 95dc958d65c6: Pull complete aec60d69dd50: Pull complete bf92a6681913: Pull complete 8911fe7d4b35: Pull complete 8ce44114b060: Pull complete 1a944a194d13: Pull complete 5519cf0ef45d: Pull complete 6c688677ac8e: Pull complete 2f147ac236bb: Pull complete Digest: sha256:d69e9e4d983c080cdbbafc64cbbd26373e0e417cb9cd18189eeb554c534fc26eStatus: Downloaded newer image for mongo:latest[root@docker-client ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEmongo latest 7177e01e8c01 2 weeks ago 393MB#为docker客户端下发域名（在harbor本地执行）#将harbor上的自签发域名证书www.yunjisuan.com.crt复制到docker客户端的对应目录下[root@harbor-master harbor]# cd /data/ssl/[root@harbor-master ssl]# scp www.yunjisuan.com.crt 192.168.200.105:/etc/pki//ca-trust/source/anchors/The authenticity of host &#x27;192.168.200.105 (192.168.200.105)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:Tb+xKsJLCQlT4u/RS9qENMAdjnoXMLlWbowNVGcfdsg.ECDSA key fingerprint is MD5:68:b5:d8:28:9f:50:6d:97:1d:26:1c:ea:7e:12:3a:04.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &#x27;192.168.200.105&#x27; (ECDSA) to the list of known hosts.root@192.168.200.105&#x27;s password: www.yunjisuan.com.crt 100% 1931 102.2KB/s 00:00 #在docker客户端执行，让证书立刻生效[root@docker-client ~]# update-ca-trust enable[root@docker-client ~]# update-ca-trust extract#下发证书重新启动docker-client的docker服务[root@docker-client ~]# systemctl restart docker#docker-client登陆docker仓库进行验证[root@docker-client ~]# docker login www.yunjisuan.comUsername: adminPassword: #密码：Harbor12345WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded#修改镜像名字并上传到私有仓库[root@docker-client ~]# docker tag mongo:latest www.yunjisuan.com/library/mongo[root@docker-client ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEmongo latest 7177e01e8c01 2 weeks ago 393MBwww.yunjisuan.com/library/mongo latest 7177e01e8c01 2 weeks ago 393MB[root@docker-client ~]# docker push www.yunjisuan.com/library/mongo #上传镜像The push refers to repository [www.yunjisuan.com/library/mongo]8daf1a7ee9a2: Pushed c718fe180d79: Pushed 56572579bdba: Pushed df60ba57445a: Pushed 032fcf7da9f7: Pushed fb77fd3e12e9: Pushed 8fd54ee41170: Pushed 9bbcbe8a46a5: Pushed b77c8e5e2140: Pushed 428c1ba11354: Pushed b097f5edab7b: Pushed 27712caf4371: Pushed 8241afc74c6f: Pushed latest: digest: sha256:e9e97b5cce9bf6d95c79bf3cb23d90e4d79aa8b469854f263fcb18f69af6195a size: 3028 浏览器登陆harbor进行查看 5.7 FAQ：问题解答5.7.1 win10版本默认拒绝非认证的域名证书 如果启动harbor采用的是https加密证书的方式，最新版win10浏览器访问，会直接提示“站点不安全，拒绝连接”。那么可以采用非https的方式启动harbor 123[root@harbor-master ssl]# sed -n &#x27;11p&#x27; /data/install/harbor/harbor.cfgui_url_protocol = http#修改之后要重启harbor。 如果采用非https加密方式启动harhor，最新版的docker是登陆不了的，因为新版docker默认以https的方式登陆harbor。 1234[root@harbor-master harbor]# docker login -uadmin -pHarbor123456 www.yunjisuan.comWARNING! Using --password via the CLI is insecure. Use --password-stdin.Error response from daemon: Get https://www.yunjisuan.com/v2/: unauthorized: authentication required 为了解决登陆问题，需要在&#x2F;etc&#x2F;docker下创建一个daemon.josn的名字文件，加入http方式登陆的harbor域名 12345678910111213141516[root@harbor-master harbor]# cat /etc/docker/daemon.json &#123; &quot;registry-mirrors&quot;:[ &quot;https://registry.docker-cn.com&quot; ], &quot;insecure-registries&quot;:[ &quot;www.yunjisuan.com&quot; ] #添加此行&#125;[root@harbor-master harbor]# systemctl restart docker #进行重启 #再次登陆harbor[root@harbor-master harbor]# docker login -uadmin -pHarbor12345 www.yunjisuan.comWARNING! Using --password via the CLI is insecure. Use --password-stdin.WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded #登陆成功 ###附录：harbor更改镜像数据目录方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172cd /&#123;myharbordir&#125;/common/templates/registryvim config.ymlversion: 0.1log: level: debug fields: service: registrystorage: #此字段为数据存放位置字段，下面我把数据存储在S3 bucket cache: layerinfo: inmemory filesystem: rootdirectory: /storage maintenance: uploadpurging: enabled: false delete: enabled: truehttp: addr: :5000 secret: placeholder debug: addr: localhost:5001auth: token: issuer: harbor-token-issuer realm: $ui_url/service/token rootcertbundle: /etc/registry/root.crt service: harbor-registrynotifications: endpoints: - name: harbor disabled: false url: http://ui/service/notifications timeout: 3000ms threshold: 5 backoff: 1s #修改后文件内容：[root@gago-harbor-airflow registry]# cat config.yml version: 0.1log: level: debug fields: service: registrystorage: s3: accesskey: xxxxxx secretkey: xxxxxx region: cn-north-1 bucket: xxxxxxhttp: addr: :5000 secret: placeholder debug: addr: localhost:5001auth: token: issuer: harbor-token-issuer realm: $ui_url/service/token rootcertbundle: /etc/registry/root.crt service: harbor-registrynotifications: endpoints: - name: harbor disabled: false url: http://ui/service/notifications timeout: 3000ms threshold: 5 backoff: 1s","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://kkabuzs.github.io/tags/Docker/"}]},{"title":"Docker基础（下）","slug":"dockerjichu-xia","date":"2018-10-14T09:05:54.000Z","updated":"2018-10-14T09:05:54.000Z","comments":true,"path":"articles/2018/10/14/dockerjichu-xia/","permalink":"https://kkabuzs.github.io/articles/2018/10/14/dockerjichu-xia/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Docker基础（下）五，网络管理5.1 容器网络模式 Docker支持5种网络模式 bridge 默认网络，Docker启动后默认创建一个docker0网桥，默认创建的容器也是添加到这个网桥中 host 容器不会获得一个独立的network namespace，而是与宿主机共用一个 none 获取独立的network namespace，但不为容器进行任何网络配置 container 与指定的容器使用同一个network namespace，网卡配置也都是相同的 自定义 自定义网桥，默认与bridge网络一样 5.1.1 bridge网络类型123456789101112131415161718192021222324252627282930313233343536373839404142#安装bridge管理工具[root@localhost ~]# yum -y install bridge-utils#查看网桥状态[root@localhost ~]# brctl showbridge name bridge id STP enabled interfacesbr-e9c6f3aab48b 8000.0242a80f103f no #网桥br-br0没绑定虚拟网卡docker0 8000.024239f5c901 no veth7e686d5 #网桥docker0绑定了一个虚拟网卡 vetha8708ae [root@localhost ~]# docker network lsNETWORK ID NAME DRIVER SCOPEcf9f63728159 bridge bridge local #两个网桥类型的网络0f75cc017204 host host locale9c6f3aab48b lnmp bridge local #两个网桥类型的网络c71020a2992f none null local#查看容器进程[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESfebd7da82596 mysql:5.6 &quot;docker-entrypoint.s…&quot; 3 seconds ago Up 2 seconds 0.0.0.0:3306-&gt;3306/tcp lnmp_mysql1d71324b351f be1 &quot;nginx -g &#x27;daemon of…&quot; 4 minutes ago Up 3 minutes 80/tcp xenodochial_engelbart#查看容器lnmp_mysql的网络信息[root@localhost ~]# docker inspect lnmp_mysql | grep -A 15 &quot;Networks&quot; &quot;Networks&quot;: &#123; &quot;lnmp&quot;: &#123; #网络类型lnmp &quot;IPAMConfig&quot;: null, &quot;Links&quot;: null, &quot;Aliases&quot;: [ &quot;febd7da82596&quot; ], &quot;NetworkID&quot;: &quot;e9c6f3aab48b1048af23ad5347408ef0b460c3dbb1917f04565dcd7d4f8498b7&quot;, &quot;EndpointID&quot;: &quot;e1428ec05df1ffd116b1db24d9c0ac401e200392d09937c8cc0a3822768d4ebe&quot;, &quot;Gateway&quot;: &quot;172.18.0.1&quot;, #网关172.18.0.1，这就是网桥br-be9c6f3aab48b &quot;IPAddress&quot;: &quot;172.18.0.2&quot;, #容器IP172.18.0.2 &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;, 5.1.2 host网络类型1234567891011121314151617#启动一个网络类型为host的容器[root@localhost ~]# docker run -dit --name test2 --network host centos:latest /bin/bash6ce541e470c3e846f1fc3a176d1fcc4fcfcfaafc8162142e11ea1a036f28d337[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES6ce541e470c3 centos:latest &quot;/bin/bash&quot; 4 seconds ago Up 2 seconds test2febd7da82596 mysql:5.6 &quot;docker-entrypoint.s…&quot; 2 minutes ago Up 2 minutes 0.0.0.0:3306-&gt;3306/tcp lnmp_mysql1d71324b351f be1 &quot;nginx -g &#x27;daemon of…&quot; 7 minutes ago Up 6 minutes 80/tcp xenodochial_engelbart#查看容器的ip[root@localhost ~]# docker exec test2 hostname -I #这就是网络类型为host的容器，ip地址和docker宿主机完全一样192.168.131.162 172.17.0.1 172.18.0.1 [root@localhost ~]# docker exec lnmp_mysql hostname -i #网桥类型容器172.18.0.2[root@localhost ~]# docker exec xenodochial_engelbart hostname -i #网桥类型容器172.17.0.2 5.1.3 none网络类型（用于建立与宿主机的桥接模式）1234567#启动一个网络类型为none的容器[root@localhost ~]# docker run -dit --name test3 --net none centos:latest5137e04e622c30e8f46bb2e492afcec2b8367eb84a86d257c0aefb8035b3580a#查看容器IP地址[root@localhost ~]# docker exec test3 hostname -I none类型就是暂时不给容器指定网卡。 5.1.4 container网络类型 指定新容器使用指定容器的网卡 12345678910111213141516171819202122232425262728#启动一个容器，网络类型container，使用lnmp_mysql容器的网卡[root@localhost ~]# docker run -dit --name test4 --net container:lnmp_mysql centos:latest /bin/bash046a3d3cac1889e34a6a2fca0e92c8a8fd0cca14a4b235d902f2ea5b88b07a19[root@localhost ~]# docker inspect lnmp_mysql | grep -A 15 &quot;Networks&quot; &quot;Networks&quot;: &#123; &quot;lnmp&quot;: &#123; &quot;IPAMConfig&quot;: null, &quot;Links&quot;: null, &quot;Aliases&quot;: [ &quot;febd7da82596&quot; ], &quot;NetworkID&quot;: &quot;e9c6f3aab48b1048af23ad5347408ef0b460c3dbb1917f04565dcd7d4f8498b7&quot;, &quot;EndpointID&quot;: &quot;e1428ec05df1ffd116b1db24d9c0ac401e200392d09937c8cc0a3822768d4ebe&quot;, &quot;Gateway&quot;: &quot;172.18.0.1&quot;, #lnmp_mysql容器的网关 &quot;IPAddress&quot;: &quot;172.18.0.2&quot;, #lnmp_mysql容器模式网桥 &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;,[root@localhost ~]# docker inspect test4 | grep -A 15 &quot;Networks&quot; &quot;Networks&quot;: &#123;&#125; #test4容器并没有自己的网络设置 &#125; &#125;][root@localhost ~]# docker exec test4 hostname -I #test4没有网络设置却有IP地址和test容器完全一样172.18.0.2 ###5.2 桥接宿主机网络与配置固定IP地址 5.2.1 建立网桥桥接到宿主机网络 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#构建一个永久生效的网桥br0[root@localhost ~]# cd /etc/sysconfig/network-scripts/[root@localhost network-scripts]# vim ifcfg-ens32 [root@localhost network-scripts]# cat ifcfg-ens32TYPE=EthernetBOOTPROTO=dhcpNAME=ens32DEVICE=ens32ONBOOT=yesBRIDGE=br0[root@localhost network-scripts]# vim ifcfg-br0[root@localhost network-scripts]# cat ifcfg-br0 TYPE=BridgeBOOTPROTO=staticDEVICE=br0ONBOOT=yesIPADDR=192.168.131.162NETMASK=255.255.255.0GATEWAY=192.168.131.2DNS1=192.168.131.2[root@localhost network-scripts]# service network restartRestarting network (via systemctl): [ OK ]#查看网卡IP[root@localhost network-scripts]# ifconfig ens32ens32: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet6 fe80::20c:29ff:fe2c:46cf prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:2c:46:cf txqueuelen 1000 (Ethernet) #ens32网卡已经没有IP地址了 RX packets 291550 bytes 386603821 (368.6 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 96068 bytes 7558633 (7.2 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0[root@localhost network-scripts]# ifconfig br0br0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.131.162 netmask 255.255.255.0 broadcast 192.168.131.255 #网桥br0代替了ens32 inet6 fe80::20c:29ff:fe2c:46cf prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:2c:46:cf txqueuelen 1000 (Ethernet) RX packets 26 bytes 1633 (1.5 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 28 bytes 2886 (2.8 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0[root@localhost network-scripts]# brctl showbridge name bridge id STP enabled interfacesbr-e9c6f3aab48b 8000.0242a80f103f no veth2700d35br0 8000.000c292c46cf no ens32 #网桥br0，桥接在了真实的物理网卡ens32上docker0 8000.024239f5c901 no vetha8708ae 5.2.2 通过pipework工具配置容器固定IP pipework工具下载地址：https://github.com/jpetazzo/pipework. 1234567891011121314151617181920212223242526272829303132333435363738394041424344#解压安装pipework工具[root@localhost ~]# yum -y install unzip[root@localhost ~]# unzip pipework-master.zipArchive: pipework-master.zipae42f1b5fef82b3bc23fe93c95c345e7af65fef3 creating: pipework-master/ extracting: pipework-master/.gitignore inflating: pipework-master/LICENSE inflating: pipework-master/README.md inflating: pipework-master/docker-compose.yml creating: pipework-master/doctoc/ inflating: pipework-master/doctoc/Dockerfile inflating: pipework-master/pipework inflating: pipework-master/pipework.spec [root@localhost ~]# mv pipework-master /usr/local/[root@localhost ~]# ln -s /usr/local/pipework-master/pipework /usr/local/bin/[root@localhost ~]# which pipework/usr/local/bin/pipework#建立网络类型为none的容器，并通过pipework配置固定ip地址[root@localhost ~]# docker run -dit --name test5 --net none centos:latest /bin/bash2cb23bc9c459d628d833ad72681f8fe5b49da9f41cb5ebf160c6b330dad6d877[root@localhost ~]# pipework br0 test5 192.168.131.199/24@192.168.131.2 #设置容器固定IP为192.168.131.199网关192.168.131.2[root@localhost ~]# docker exec test5 hostname -I #有IP了192.168.131.199 [root@localhost ~]# ping 192.168.131.199 #宿主机ping能通PING 192.168.131.199 (192.168.131.199) 56(84) bytes of data.64 bytes from 192.168.131.199: icmp_seq=1 ttl=64 time=0.210 ms64 bytes from 192.168.131.199: icmp_seq=2 ttl=64 time=0.037 ms^C--- 192.168.131.199 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1000msrtt min/avg/max/mdev = 0.037/0.123/0.210/0.087 ms[root@localhost ~]# docker exec -it test5 /bin/bash #进入容器[root@2cb23bc9c459 /]# ping www.baidu.com #能连接外网PING www.a.shifen.com (119.75.213.61) 56(84) bytes of data.64 bytes from 127.0.0.1 (119.75.213.61): icmp_seq=1 ttl=128 time=12.6 ms64 bytes from 127.0.0.1 (119.75.213.61): icmp_seq=2 ttl=128 time=28.2 ms64 bytes from 127.0.0.1 (119.75.213.61): icmp_seq=3 ttl=128 time=17.5 ms^C--- www.a.shifen.com ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2003msrtt min/avg/max/mdev = 12.696/19.496/28.216/6.480 ms 通过windows宿主机ping虚拟机中的容器进程IP进行测试 综上，外部机器访问虚拟机中的容器进程也可以联通了 六，Docker的镜像制作6.1 Dockerfile常用指令介绍|指令| 描述||-||FROM| 构建的新镜像是基于哪个镜像。例如：FROM centos:6|MAINTAINER| 镜像维护者姓名或邮箱地址。例如：MAINTAINER zhaaoshuo|RUN| 构建镜像时运行的Shell命令。例如：RUN [“yum”,”install”,”httpd”]||或者RUN yum install httpd|CMD| 运行容器时执行的Shell命令（可以被运行时传递的参数覆盖）。例如：CMD [“-c”,”&#x2F;start.sh”]||或者CMD [“&#x2F;usr&#x2F;sbin&#x2F;sshd”,”-D”]或者CMD &#x2F;usr&#x2F;sbin&#x2F;sshd -D|EXPOSE| 声明容器运行的服务端口。例如：EXPOSE 80 443|ENV| 设置容器内环境变量。例如：ENV MYSQL_ROOT_PASSWORD 123456|ADD| 拷贝文件或目录到镜像（可以自动解压缩或者下载）||例如：ADD [“src”,”dest”]或者ADD https://xxx.com/html.tar.gz &#x2F;var&#x2F;www&#x2F;html||或者：ADD html.tar.gz &#x2F;var&#x2F;www&#x2F;html|COPY| 拷贝文件或目录到镜像（不能自动解压缩）。例如：COPY .&#x2F;start.sh &#x2F;start.sh|ENTRYPOINT| 运行容器时执行的Shell命令（不能被运行时传递的参数覆盖)。例如：ENTRYPOINT [“&#x2F;bin&#x2F;bash”,”-c”,”&#x2F;start.sh”]||或者ENTRYPOINT &#x2F;bin&#x2F;bash -c “&#x2F;start.sh”|VOLUME| 指定容器挂载点到宿主机自动生成的目录或其他容器||例如：VOLUME [“&#x2F;var&#x2F;lib&#x2F;mysql”]|USER| 为RUN，CMD和ENTRYPOINT执行命令指定运行用户||例如：USER zhaoshuo|WORKDIR| 为RUN，CMD，ENTRYPOINT，COPY和ADD设置工作目录（指定进入容器中默认被切换的目录）。||例如：WORKDIR &#x2F;data|HEALTHCHECK| 健康检查。例如：HEALTHCHECK –interval&#x3D;5m –timeout&#x3D;3s –retries&#x3D;3||CMD curl -f http://localhost/ || exit 1|ARG| 在构建镜像时指定一些参数。例如：ARG user 6.2 利用Dockerfile编写nginxWeb镜像123456789101112131415161718192021222324252627282930313233343536#首先开启ipv4转发[root@localhost ~]# echo &quot;net.ipv4.ip_forward=1&quot; &gt;&gt; /etc/sysctl.conf [root@localhost ~]# sysctl -pnet.ipv4.ip_forward = 1#创建nginx的镜像目录[root@localhost ~]# mkdir -p dockerfile/lib/centos/nginx[root@localhost ~]# cd dockerfile/lib/centos/nginx#准备Dockerfile文件[root@localhost nginx]# cat Dockerfile [root@localhost nginx]# cat Dockerfile FROM centos:7MAINTAINER www.aliangedu.comRUN yum install -y gcc gcc-c++ make openssl-devel pcre-develADD nginx-1.12.1.tar.gz /tmpRUN cd /tmp/nginx-1.12.1 &amp;&amp; \\ ./configure --prefix=/usr/local/nginx &amp;&amp; \\ make -j 2 &amp;&amp; \\ make installRUN rm -rf /tmp/nginx-1.12.1* &amp;&amp; yum clean allCOPY nginx.conf /usr/local/nginx/confWORKDIR /usr/local/nginxEXPOSE 80CMD [&quot;./sbin/nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]#将事先准备好的nginx-1.12.1.tar.gz和nginx.conf配置文件拷贝到当前目录下[root@localhost nginx]# lsDockerfile nginx-1.12.1.tar.gz nginx.conf#build基于nginx的docker镜像[root@docker nginx]# docker build -t nginx:1 . 6.3 构建PHP网站平台镜像 创建一个PHP的docker镜像 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#创建php的镜像目录[root@localhost ~]# mkdir -p /root/dockerfile/lib/centos/php[root@localhost ~]# cd /root/dockerfile/lib/centos/php#准备Dockerfile文件[root@localhost php]# cat Dockerfile FROM centos:7MAINTAINER www.aliangedu.comRUN yum install -y gcc gcc-c++ make gd-devel libxml2-devel libcurl-devel libjpeg-devel libpng-devel openssl-develADD php-5.6.31.tar.gz /tmp/RUN cd /tmp/php-5.6.31 &amp;&amp; \\ ./configure --prefix=/usr/local/php \\ --with-config-file-path=/usr/local/php/etc \\ --with-mysql --with-mysqli \\ --with-openssl --with-zlib --with-curl --with-gd \\ --with-jpeg-dir --with-png-dir --with-iconv \\ --enable-fpm --enable-zip --enable-mbstring &amp;&amp; \\ make -j 4 &amp;&amp; \\ make install &amp;&amp; \\ cp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.conf &amp;&amp; \\ sed -i &quot;s/127.0.0.1/0.0.0.0/&quot; /usr/local/php/etc/php-fpm.conf &amp;&amp; \\ sed -i &quot;21a \\daemonize = no&quot; /usr/local/php/etc/php-fpm.confCOPY php.ini /usr/local/php/etcRUN rm -rf /tmp/php-5.6.31* &amp;&amp; yum clean allWORKDIR /usr/local/phpEXPOSE 9000CMD [&quot;./sbin/php-fpm&quot;, &quot;-c&quot;, &quot;/usr/local/php/etc/php-fpm.conf&quot;]#将事先准备好的php.ini文件及php-5.6.31.tar.gz拷贝到当前目录下[root@localhost php]# lsDockerfile php-5.6.31.tar.gz php.ini#build基于php的docker镜像[root@localhost php]# docker build -t php:1 .[root@localhost php]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEphp 1 9bb406993d6b 2 minutes ago 996MBnginx 1 27d8d2e6a038 18 minutes ago 417MBcentos 7 75835a67d134 4 days ago 200MBcentos latest 75835a67d134 4 days ago 200MBnginx latest be1f31be9a87 11 days ago 109MBricharvey/nginx-php-fpm latest de1554d0b081 2 weeks ago 302MBmysql 5.6 1f47fade220d 5 weeks ago 256MB 6.4 创建nginx-php网络环境，并启动容器1234567891011121314151617181920212223242526272829303132333435[root@localhost ~]# docker network create lnmp[root@localhost ~]# docker network lsNETWORK ID NAME DRIVER SCOPEcf9f63728159 bridge bridge local0f75cc017204 host host locale9c6f3aab48b lnmp bridge localc71020a2992f none null local#创建nginx-php网页挂载目录[root@localhost ~]# mkdir -p /app/wwwroot#启动php容器[root@localhost ~]# docker run -dit --name lnmp_php --network lnmp --mount type=bind,src=/app/wwwroot/,dst=/usr/local/nginx/html php:1eaf4d08600a45287800dbc3b451c423acc3a15ba6104dfacbc10fd745f4e407b[root@localhost ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESeaf4d08600a4 php:1 &quot;./sbin/php-fpm -c /…&quot; 8 seconds ago Up 6 seconds 9000/tcp lnmp_php[root@localhost ~]# docker exec lnmp_php hostname -I172.18.0.2 #启动nginx容器[root@localhost ~]# docker run -dit --name lnmp_nginx -p 888:80 --network lnmp --mount type=bind,src=/app/wwwroot,dst=/usr/local/nginx/html nginx:1252721e419c61fc41704a46364d650f471dd9e7ea94b7a1636f0db5aeec4edbc[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES252721e419c6 nginx:1 &quot;./sbin/nginx -g &#x27;da…&quot; 5 seconds ago Up 4 seconds 0.0.0.0:888-&gt;80/tcp lnmp_nginxeaf4d08600a4 php:1 &quot;./sbin/php-fpm -c /…&quot; About a minute ago Up About a minute 9000/tcp lnmp_php[root@localhost ~]# docker exec lnmp_nginx hostname -I172.18.0.3 #创建测试页面php.info[root@localhost wwwroot]# pwd/app/wwwroot[root@localhost wwwroot]# echo &quot;&lt;?php phpinfo();?&gt;&quot; &gt; /app/wwwroot/index.php 最后通过浏览器进行访问测试 6.5 docke基础阶段应用 ：在nginx-php镜像的基础上构建mysql:1的镜像，并启动lnmp_mysql容器进程 ：实现wordpress博客的部署 6.6 构建JAVA网站环境镜像12345678910111213141516171819202122232425262728293031323334353637383940414243444546#创建java镜像构建存储目录[root@localhost ~]# mkdir -p dockerfile/lib/centos/tomcat[root@localhost ~]# cd dockerfile/lib/centos/tomcat#将所需软件包拷贝到当前目录下[root@localhost tomcat]# lsapache-tomcat-8.0.46.tar.gz Dockerfile jdk-8u45-linux-x64.tar.gz server.xml#筹备Dockerfile[root@localhost tomcat]# cat Dockerfile FROM centos:7MAINTAINER www.aliangedu.com ADD jdk-8u45-linux-x64.tar.gz /usr/localENV JAVA_HOME /usr/local/jdk1.8.0_45ADD apache-tomcat-8.0.46.tar.gz /usr/localCOPY server.xml /usr/local/apache-tomcat-8.0.46/confRUN rm -f /usr/local/*.tar.gzWORKDIR /usr/local/apache-tomcat-8.0.46EXPOSE 8080ENTRYPOINT [&quot;./bin/catalina.sh&quot;, &quot;run&quot;]#build基于tomcat的镜像[root@localhost tomcat]# docker build -t tomcat:1 .#创建网页挂载目录[root@localhost tomcat]# mkdir -p /app/webapps#启动tomcat的容器进程[root@localhost tomcat]# docker run -dit --name=tomcat -p 8080:8080 --mount type=bind,src=/app/webapps/,dst=/usr/local/apache-tomcat-8.0.46/webapps tomcat:17b3978a08052ca580201f6d2e3a362da2f2f302f4e6455ae9dc0effdf02a0293#查看容器进程[root@localhost tomcat]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES7b3978a08052 tomcat:1 &quot;./bin/catalina.sh r…&quot; 21 seconds ago Up 20 seconds 0.0.0.0:8080-&gt;8080/tcp tomcat252721e419c6 nginx:1 &quot;./sbin/nginx -g &#x27;da…&quot; 12 minutes ago Up 12 minutes 0.0.0.0:888-&gt;80/tcp lnmp_nginxeaf4d08600a4 php:1 &quot;./sbin/php-fpm -c /…&quot; 13 minutes ago Up 13 minutes 9000/tcp lnmp_php#创建网站测试页[root@localhost tomcat]# mkdir -p /app/webapps/ROOT[root@localhost tomcat]# echo &quot;welcome to yunjisuan&quot; &gt;&gt; /app/webapps/ROOT/index.html 然后用浏览器访问8080端口 特别提示： tomcat有三种启动方式: 直接启动.&#x2F;startup.sh 作为服务启动 nohup .&#x2F;startup.sh &amp; 控制台动态输出方式启动.&#x2F;catalina.sh run动态地显示tomcat后台的控制台输出信息，Ctrl+C后退出并关闭服务 构建镜像的Dockerfile里的最后一步启动的进程不能是后台模式，否则容器直接退出。因此，nginxWeb镜像构建时，用CMD [“.&#x2F;sbin&#x2F;nginx”, “-g”, “daemon off;”]方式进行启动 七，Docker-compose7.1 Docker Compose用法7.1.1 介绍 Compose是一个定义和管理多容器的工具，使用Python语言编写。使用Compose配置文件描述多个容器应用的架构，比如使用什么镜像，数据卷，网络，映射端口等；然后一条命令管理所有服务，比如启动，停止，重启等。 7.1.2 安装 下载docker-compose-linux-x86_64.zip 123456789101112131415161718[root@localhost ~]# ll docker-compose-linux-x86_64.zip -rw-r--r-- 1 root root 8733389 10月 14 19:32 docker-compose-linux-x86_64.zip[root@localhost ~]# which unzip/usr/bin/unzip[root@localhost ~]# unzip docker-compose-linux-x86_64.zipArchive: docker-compose-linux-x86_64.zip inflating: docker-compose [root@localhost ~]# ll docker-compose-rw-r--r-- 1 root root 8858496 8月 31 2017 docker-compose[root@localhost ~]# chmod +x docker-compose[root@localhost ~]# mv docker-compose /usr/bin/[root@localhost ~]# which docker-compose/usr/bin/docker-compose#第二种安装方法：[root@localhost ~]# curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose[root@localhost ~]# chmod +x /usr/local/bin/docker-compose 7.1.3 YAML文件格式及编写注意事项 YAML是一种标记语言很直观的数据序列化格式，可读性高。类似于XML数据描述语言，语法比XML简单的很多。YAML数据结构通过缩进来表示，连续的项目通过减号来表示，键值对用冒号分隔，数组用中括号括起来，hash用花括号括起来。 YAML文件格式注意事项： 不支持制表符tab键缩进，需要使用空格缩进 通常开头缩进2个空格 字符后缩进1个空格，如冒号，逗号，横杆 用井号注释 如果包含特殊字符用单引号引起来 布尔值（true，false，yes，no，on，off）必须用引号括起来，这样分析器会将他们解释为字符串。 配置文件常用字段 |字段| 描述||-||build| 下级字段dockerfile：指定Dockerfile文件名||下级字段context；构建镜像上下文路径|image| 指定镜像|command| 执行命令，覆盖默认命令|container_name| 指定容器名称|deploy| 指定部署和运行服务相关配置，只能在Swarm模式使用|environment| 添加环境变量|networks| 加入网络，引用顶级networks下条目|ports| 暴露端口，与-p相同，但端口不能低于60|volumes| 挂载宿主机路径或命名卷。如果是命名卷在顶级volumes定义卷名称|restart| 重启策略，默认no，always | on-failure | unless-stopped|hostname| 容器主机名 常用命令 |字段| 描述||-||build| 重新构建服务||ps| 列出容器|up| 创建和启动容器|exec| 在容器里执行命令|scale| 指定一个服务容器启动数量|top| 显示容器进程|logs| 查看容器输出|down| 删除容器，网络，数据卷和镜像|stop&#x2F;start&#x2F;restart| 停止&#x2F;启动&#x2F;重启服务 例如： 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@localhost ~]# cat compose_lnmp/docker-compose.yml version: &#x27;3&#x27;services: nginx: hostname: nginx build: context: ./nginx dockerfile: Dockerfile ports: - 81:80 networks: - lnmp volumes: - ./wwwroot:/usr/local/nginx/html php: hostname: php build: context: ./php dockerfile: Dockerfile networks: - lnmp volumes: - ./wwwroot:/usr/local/nginx/html mysql: hostname: mysql image: mysql:5.6 ports: - 3306:3306 networks: - lnmp volumes: - ./mysql/conf:/etc/mysql/conf.d - ./mysql/data:/var/lib/mysql command: --character-set-server=utf8 environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_DATABASE: wordpress MYSQL_USER: user MYSQL_PASSWORD: user123networks: lnmp: 7.2 docker-compose 一键部署LNMP123456789101112131415161718192021222324#查看compose_lnmp包[root@localhost ~]# tree compose_lnmp/compose_lnmp/├── docker-compose.yml #docker-compose启动接口文件├── mysql│ ├── conf│ │ └── my.cnf #mysql配置文件│ └── data #待挂载mysql数据目录├── nginx│ ├── Dockerfile #自定义nginx的Docker镜像配置文件│ ├── nginx-1.12.1.tar.gz #源码包│ └── nginx.conf #nginx配置文件├── php│ ├── Dockerfile #自定义php的Docker镜像配置文件│ ├── php-5.6.31.tar.gz #源码包│ └── php.ini #php解析器配置文件 └── wwwroot #nginx容器和php容器待挂载的网页目录 └── index.php #网页测试文件 6 directories, 9 files#一键部署LNMP[root@localhost compose_lnmp]# docker-compose -f docker-compose.yml up 浏览器访问： 1234567891011121314151617181920212223242526272829303132#一键查看所有部署的容器进程[root@localhost compose_lnmp]# docker-compose -f docker-compose.yml ps Name Command State Ports -------------------------------------------------------------------------------------composelnmp_mysql_1 docker-entrypoint.sh --cha ... Up 0.0.0.0:3306-&gt;3306/tcp composelnmp_nginx_1 ./sbin/nginx -g daemon off; Up 0.0.0.0:81-&gt;80/tcp composelnmp_php_1 ./sbin/php-fpm -c /usr/loc ... Up 9000/tcp #一键终止所有部署的容器进程[root@localhost compose_lnmp]# docker-compose -f docker-compose.yml stopStopping composelnmp_mysql_1 ... doneStopping composelnmp_php_1 ... doneStopping composelnmp_nginx_1 ... done#一键查看所有部署的容器进程[root@localhost compose_lnmp]# docker-compose -f docker-compose.yml ps Name Command State Ports ---------------------------------------------------------------------composelnmp_mysql_1 docker-entrypoint.sh --cha ... Exit 0 composelnmp_nginx_1 ./sbin/nginx -g daemon off; Exit 0 composelnmp_php_1 ./sbin/php-fpm -c /usr/loc ... Exit 0 #一键清理所有部署的容器进程[root@localhost compose_lnmp]# docker-compose -f docker-compose.yml downRemoving composelnmp_mysql_1 ... doneRemoving composelnmp_php_1 ... doneRemoving composelnmp_nginx_1 ... doneRemoving network composelnmp_lnmp[root@localhost compose_lnmp]# docker-compose -f docker-compose.yml psName Command State Ports ------------------------------ 7.3 一键部署Nginx反向代理Tomcat集群1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#查看compose_nginx_tomcat包[root@localhost ~]# tree compose_nginx_tomcat/compose_nginx_tomcat/├── docker-compose.yml├── mysql│ ├── conf│ │ └── my.cnf #mysql的配置文件│ └── data #待挂载mysql数据目录├── nginx│ ├── Dockerfile #自定义镜像配置文件│ ├── nginx-1.12.1.tar.gz #源码包│ └── nginx.conf #nginx配置文件├── tomcat│ ├── apache-tomcat-8.0.46.tar.gz #源码包│ ├── Dockerfile #自定义镜像配置文件│ └── server.xml #tomcat配置文件└── webapps └── ROOT └── index.html #网页测试文件 7 directories, 9 files#查看docker-compose的启动接口文件[root@localhost ~]# cat compose_nginx_tomcat/docker-compose.ymlversion: &#x27;3&#x27;services: nginx: hostname: nginx build: context: ./nginx dockerfile: Dockerfile ports: - 82:80 networks: - lnmt volumes: - ./webapps:/opt/webapps tomcat01: hostname: tomcat01 build: ./tomcat networks: - lnmt volumes: - /usr/local/jdk1.8.0_45:/usr/local/jdk1.8.0_45 - ./webapps:/usr/local/apache-tomcat-8.0.46/webapps tomcat02: hostname: tomcat02 build: ./tomcat networks: - lnmt volumes: - /usr/local/jdk1.8.0_45:/usr/local/jdk1.8.0_45 #docker宿主机的java环境被挂载了！ - ./webapps:/usr/local/apache-tomcat-8.0.46/webapps mysql: hostname: mysql image: mysql:5.6 ports: - 3307:3306 networks: - lnmt volumes: - ./mysql/conf:/etc/mysql/conf.d - ./mysql/data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_DATABASE: db MYSQL_USER: user MYSQL_PASSWORD: user123networks: lnmt:#由于tomcat容器进程需要挂载docker宿主机本地的java环境#所以在docker宿主机本地安装jdk-8u45-linux-x64.tar.gz[root@localhost ~]# ll jdk-8u45-linux-x64.tar.gz -rw-r--r-- 1 root root 173271626 Oct 14 19:41 jdk-8u45-linux-x64.tar.gz[root@localhost ~]# tar xf jdk-8u45-linux-x64.tar.gz -C /usr/local[root@localhost ~]# ll -d /usr/local/jdk1.8.0_45drwxr-xr-x 8 10 143 255 Apr 11 2015 /usr/local/jdk1.8.0_45#一键部署ngxin+tomcat反向代理集群[root@localhost ~]# cd compose_nginx_tomcat/[root@localhost compose_nginx_tomcat]# pwd/root/compose_nginx_tomcat[root@localhost compose_nginx_tomcat]# docker-compose -f docker-compose.yml up 浏览器访问： 1234567891011121314151617181920212223242526272829[root@localhost ~]# docker-compose -f compose_nginx_tomcat/docker-compose.yml ps Name Command State Ports --------------------------------------------------------------------------------------------composenginxtomcat_mysql_1 docker-entrypoint.sh mysqld Up 0.0.0.0:3307-&gt;3306/tcp composenginxtomcat_nginx_1 ./sbin/nginx -g daemon off; Up 0.0.0.0:82-&gt;80/tcp composenginxtomcat_tomcat01_1 ./bin/catalina.sh run Up 8080/tcp composenginxtomcat_tomcat02_1 ./bin/catalina.sh run Up 8080/tcp [root@localhost ~]# docker-compose -f compose_nginx_tomcat/docker-compose.yml stopStopping composenginxtomcat_mysql_1 ... doneStopping composenginxtomcat_tomcat02_1 ... doneStopping composenginxtomcat_tomcat01_1 ... doneStopping composenginxtomcat_nginx_1 ... done[root@localhost ~]# docker-compose -f compose_nginx_tomcat/docker-compose.yml ps Name Command State Ports ------------------------------------------------------------------------------composenginxtomcat_mysql_1 docker-entrypoint.sh mysqld Exit 0 composenginxtomcat_nginx_1 ./sbin/nginx -g daemon off; Exit 0 composenginxtomcat_tomcat01_1 ./bin/catalina.sh run Exit 143 composenginxtomcat_tomcat02_1 ./bin/catalina.sh run Exit 143 [root@localhost ~]# docker-compose -f compose_nginx_tomcat/docker-compose.yml downRemoving composenginxtomcat_mysql_1 ... doneRemoving composenginxtomcat_tomcat02_1 ... doneRemoving composenginxtomcat_tomcat01_1 ... doneRemoving composenginxtomcat_nginx_1 ... doneRemoving network composenginxtomcat_lnmt[root@localhost ~]# docker-compose -f compose_nginx_tomcat/docker-compose.yml psName Command State Ports ------------------------------","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://kkabuzs.github.io/tags/Docker/"}]},{"title":"Docker基础（上）","slug":"dockerjichu-shang","date":"2018-10-02T13:18:35.000Z","updated":"2018-10-02T13:18:35.000Z","comments":true,"path":"articles/2018/10/02/dockerjichu-shang/","permalink":"https://kkabuzs.github.io/articles/2018/10/02/dockerjichu-shang/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Docker基础（上）一，Docker简介，功能特性与应用场景1.1 Docker简介 Docker是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的Linux机器上，也可以实现虚拟化，容器是完全使用沙箱机制，相互之间不会有任何接口。 一个完整的Docker有以下几个部分组成： Docker Client 客户端 Docker Daemon 守护进程 Docker Image 镜像 Docker Container 容器 1.2 Docker功能特性 隔离环境（系统，网络，文件系统）与应用 解决依赖与版本问题 易于分发，开箱即用 节点与容器快速扩容 镜像制作简单便捷，管理方便 （1）隔离 通过cgroup（隔离和跟踪资源的使用）&amp; namespace（组与组之间隔离）来实现轻量级的进程隔离 对于容器中运行的进程来说，自己独占了一个系统 容器间网络，文件及其他资源都互相隔离 （2）版本与依赖 传统模式下，多个不同环境或版本的项目需要部署在不同机器上，部署与后期维护管理复杂繁琐。 使用Docker，通过多个不同版本或者环境的镜像，可以同时运行在一台机器上互不干扰，部署与后期维护简单方便。 （3）分发与使用 镜像可以通过导入，导出，上传到镜像仓库等多种方式进行分发 在启动了Docker的系统上直接使用docker run即可启动镜像，无需特别配置。 （4）扩容 容器扩容简单方便 扩容节点只需安装并启动Docker即可 （5）镜像制作 镜像的灵魂Dockerfile 使用Dockerfile进行指令控制 基于Linux命令，易于理解，快速上手 易于定制与修改 1.3 Docker应用场景 Docker通常应用于如下场景： web应用的自动化打包和发布； 自动化测试和持续集成，发布； 应用服务，如MySQL，Redis等，通过Docker实现快速部署； k8s私有云 1.3.1 场景1：多版本多种类系统与软件 1.3.2 场景2： 环境 1.3.3 场景3： 分发 二，Docker的安装2.1 安装环境 最小化安装Centos7.5 12345[root@localhost ~]# cat /etc/redhat-releaseCentOS Linux release 7.5.1804 (Core) [root@localhost ~]# uname -r3.10.0-862.el7.x86_64 关闭防火墙和selinux 1234[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# systemctl disable firewalld[root@localhost ~]# setenforce 0 2.2 版本选择 Docker 17.03之后版本变为Docker CE Docker CE 社区版，Community Edition Docker EE 企业版，Enterprise Edition 收费版本，强调安全性，提供一些高级特性及商业支持 2.3 Docker安装：标准版本 特别提示：centos7.5在搭建本地yum仓库的时候只需要修改CentOS-Media.repo 文件即可（不需要将其他文件仍子目录里）确保虚拟机能正常上网 1234567891011121314151617181920212223242526[root@localhost ~]# yum -y install docker[root@localhost ~]# systemctl start docker #启动docker进程[root@localhost ~]# systemctl enable docker #加入docker开机启动Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.[root@localhost ~]# docker ps #查看封装在docker镜像中的正在运行的镜像进程CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES[root@localhost ~]# docker versionClient: Version: 1.13.1 API version: 1.26 Package version: docker-1.13.1-75.git8633870.el7.centos.x86_64 Go version: go1.9.4 Git commit: 8633870/1.13.1 Built: Fri Sep 28 19:45:08 2018 OS/Arch: linux/amd64Server: Version: 1.13.1 API version: 1.26 (minimum version 1.12) Package version: docker-1.13.1-75.git8633870.el7.centos.x86_64 Go version: go1.9.4 Git commit: 8633870/1.13.1 Built: Fri Sep 28 19:45:08 2018 OS/Arch: linux/amd64 Experimental: false 2.4 Docker安装：CE社区版 首先清理掉yum安装的docker标准版这里有两种方式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#第一种清理docker方法[root@localhost ~]# yum -y remove docker#第二种清理docker方法[root@localhost ~]# yum history list #查看yum安装的历史列表Loaded plugins: fastestmirrorID | Login user | Date and time | Action(s) | Altered------------------------------------------------------------------------------- 5 | root &lt;root&gt; | 2018-10-03 05:30 | I, U | 29 #这次就是安装的docker 4 | root &lt;root&gt; | 2018-09-10 21:53 | Install | 1 3 | root &lt;root&gt; | 2018-09-10 21:52 | Install | 51 2 | root &lt;root&gt; | 2018-09-10 21:49 | I, U | 42 1 | System &lt;unset&gt; | 2018-09-10 21:14 | Install | 311 history list[root@localhost ~]# yum history info 5 #查看yum历史安装ID为5的安装信息Loaded plugins: fastestmirrorTransaction ID : 5Begin time : Wed Oct 3 05:30:44 2018Begin rpmdb : 402:a21e595e89a560c63b9b56a960a15973500ac077End time : 05:31:31 2018 (47 seconds)End rpmdb : 429:96445b97491375cff0acc3f0dfcbc2155ee5f7e7User : root &lt;root&gt;Return-Code : SuccessCommand Line : -y install dockerTransaction performed with: Installed rpm-4.11.3-32.el7.x86_64 @anaconda Installed yum-3.4.3-158.el7.centos.noarch @anaconda Installed yum-plugin-fastestmirror-1.1.31-45.el7.noarch @anacondaPackages Altered: Dep-Install PyYAML-3.10-11.el7.x86_64 @base Dep-Install atomic-registries-1:1.22.1-25.git5a342e3.el7.centos.x86_64 @extras Updated audit-2.8.1-3.el7.x86_64 @anaconda Update 2.8.1-3.el7_5.1.x86_64 @updates Updated audit-libs-2.8.1-3.el7.x86_64 @anaconda Update 2.8.1-3.el7_5.1.x86_64 @updates Dep-Install audit-libs-python-2.8.1-3.el7_5.1.x86_64 @updates Dep-Install checkpolicy-2.5-6.el7.x86_64 @base Dep-Install container-selinux-2:2.68-1.el7.noarch @extras Dep-Install container-storage-setup-0.11.0-2.git5eaf76c.el7.noarch @extras Install docker-2:1.13.1-75.git8633870.el7.centos.x86_64 @extras Dep-Install docker-client-2:1.13.1-75.git8633870.el7.centos.x86_64 @extras Dep-Install docker-common-2:1.13.1-75.git8633870.el7.centos.x86_64 @extras Dep-Install libcgroup-0.41-15.el7.x86_64 @base Dep-Install libseccomp-2.3.1-3.el7.x86_64 @base Dep-Install libsemanage-python-2.5-11.el7.x86_64 @base Dep-Install libyaml-0.1.4-11.el7_0.x86_64 @base Dep-Install oci-register-machine-1:0-6.git2b44233.el7.x86_64 @extras Dep-Install oci-systemd-hook-1:0.1.17-2.git83283a0.el7.x86_64 @extras Dep-Install oci-umount-2:2.3.3-3.gite3c9055.el7.x86_64 @extras Dep-Install policycoreutils-python-2.5-22.el7.x86_64 @base Dep-Install python-IPy-0.75-6.el7.noarch @base Dep-Install python-backports-1.0-8.el7.x86_64 @base Dep-Install python-backports-ssl_match_hostname-3.5.0.1-1.el7.noarch @base Dep-Install python-ipaddress-1.0.16-2.el7.noarch @base Dep-Install python-pytoml-0.1.14-1.git7dea353.el7.noarch @extras Dep-Install python-setuptools-0.9.8-7.el7.noarch @base Dep-Install setools-libs-3.3.8-2.el7.x86_64 @base Dep-Install skopeo-containers-1:0.1.31-1.dev.gitae64ff7.el7.centos.x86_64 @extras Dep-Install subscription-manager-rhsm-certificates-1.20.11-1.el7.centos.x86_64 @base Dep-Install yajl-2.0.4-4.el7.x86_64 @basehistory info[root@localhost ~]# yum -y history undo 5 #进行yum安装操作回退 yum的回退安装在工作中很有用接下来安装Docker的CE社区版 12345678910111213141516171819202122232425262728293031323334#安装依赖包[root@localhost ~]# yum -y install yum-utils device-mapper-persistent-data lvm2#添加docker的CE版本的yum源配置文件[root@Docker ~]# yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo[root@localhost ~]# yum makecache fast#安装CE版本的docker[root@localhost ~]# yum -y install docker-ce[root@localhost ~]# systemctl start docker #启动docker[root@localhost ~]# systemctl enable docker #添加开机启动Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.[root@localhost ~]# docker version #查看docker版本Client: Version: 18.06.1-ce API version: 1.38 Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17:23:03 2018 OS/Arch: linux/amd64 Experimental: falseServer: Engine: Version: 18.06.1-ce API version: 1.38 (minimum version 1.12) Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17:25:29 2018 OS/Arch: linux/amd64 Experimental: false 三，Docker的操作命令3.0 添加docker国内镜像源 要添加国内的源地址 12345678[root@localhost ~]# vim /etc/docker/daemon.json[root@localhost ~]# cat /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;:[ &quot;https://registry.docker-cn.com&quot; ]&#125;[root@localhost ~]# systemctl daemon-reload[root@localhost ~]# systemctl restart docker 3.1 Docker命令：search 用于从docker的官方公有镜像仓库查找镜像 （1）查看Docker Hub上公开的centos镜像 123456789101112131415[root@localhost ~]# docker search centosNAME DESCRIPTION STARS OFFICIAL AUTOMATEDcentos The official build of CentOS. 4805 [OK] ansible/centos7-ansible Ansible on Centos7 118 [OK]jdeathe/centos-ssh CentOS-6 6.10 x86_64 / CentOS-7 7.5.1804 x86… 99 [OK]consol/centos-xfce-vnc Centos container with &quot;headless&quot; VNC session… 65 [OK]...以下省略无数行.... 特别说明：Name：镜像的名字DESCRIPTION：描述STARS：星级（越高越好）OFFICIAL：是否是官方发布的AUTOMATED:是否自动化的 （2）查找星级多于100的centos镜像 12345678[root@localhost ~]# docker search centos -f stars=100NAME DESCRIPTION STARS OFFICIAL AUTOMATEDcentos The official build of CentOS. 4805 [OK] ansible/centos7-ansible Ansible on Centos7 118 [OK]特别提示：/：符号用于分割作者名称和镜像名称ansible/centos7-ansible：ansible是作者名称，centos7-ansible是镜像名称 （3）多条件查找–filter 1234#查找官方发布的，星级大于100的centos镜像[root@localhost ~]# docker search centos --filter is-official=true --filter stars=100NAME DESCRIPTION STARS OFFICIAL AUTOMATEDcentos The official build of CentOS. 4805 [OK] 3.2 Docker命令：pull 用于从Docker Hub上下载公有镜像 12345678910111213141516#查找符合条件的hello-world镜像[root@localhost ~]# docker search hello-world --filter is-official=true --filter stars=100NAME DESCRIPTION STARS OFFICIAL AUTOMATEDhello-world Hello World! (an example of minimal Dockeriz… 681 [OK] #下载目标hello-world镜像[root@localhost ~]# docker pull hello-worldUsing default tag: latestlatest: Pulling from library/hello-worldd1725b59e92d: Pull complete Digest: sha256:0add3ace90ecb4adbf7777e9aacf18357296e799f81cabc9fde470971e499788Status: Downloaded newer image for hello-world:latest #下载成功latest是标记tag#下载目标centos:7镜像[root@localhost ~]# docker pull centos:7 #网速不好的话，需要点时间耐心等待 3.3 Docker命令：images 用于本地镜像的查看 1234567891011[root@localhost ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEcentos 7 75835a67d134 3 days ago 200MBhello-world latest 4ab4c602aa5e 5 weeks ago 1.84kB特别说明：REPOSITORY：镜像仓库（下边罗列的都是本地已有镜像名称）TAG：镜像的标记（为了区分同名镜像）IMAGES ID：镜像的ID号CREATED：此镜像的创建时间SIZE：此镜像的大小 3.4 Docker命令：build 用于本地自定义镜像的构建，需要创建Dockerfile文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#创建Dockerfile文件的存储目录[root@localhost ~]# mkdir -p /root/dockerfile/lib/centos/7[root@localhost ~]# cd /root/dockerfile/lib/centos/7#创建docker.sh脚本[root@localhost 7]# vim docker.sh[root@localhost 7]# cat docker.sh #!/bin/bashwhile truedo echo &quot;welcome&quot; sleep 5done#创建Dockerfile配置文件，文件名称必须为Dockerfile，第一个字母必须大写[root@localhost 7]# vim Dockerfile[root@localhost 7]# cat Dockerfile FROM centos #从centos源镜像的基础上进行构建LABEL MAINTATNER=&quot;zhaoshuo&quot; #作者的名称RUN ln -sfv /usr/share/zoneinfo/Asia/Shanghai /etc/localtime #RUN：在镜像构建过程中运行命令ADD docker.sh /home/test/ #从本地系统中把docker.sh文件添加到构建中的镜像的/home/test/目录下RUN chmod +x /home/test/docker.sh #在镜像构建过程中运行命令CMD [&quot;/home/test/docker.sh&quot;] #构建镜像完成时，最后执行的命令#根据Dockfile配置文件构建一个自定义镜像[root@localhost 7]# docker build -t zhaoshuo/centos7:1 . #-t 指定镜像名称 ：1 设定镜像的tag标记Sending build context to Docker daemon 3.072kBStep 1/6 : FROM centoslatest: Pulling from library/centosDigest: sha256:67dad89757a55bfdfabec8abd0e22f8c7c12a1856514726470228063ed86593bStatus: Downloaded newer image for centos:latest ---&gt; 75835a67d134Step 2/6 : LABEL MAINTATNER=&quot;zhaoshuo&quot; ---&gt; Running in 07b9ec2c2b77Removing intermediate container 07b9ec2c2b77 ---&gt; 8af9c0b32fecStep 3/6 : RUN ln -sfv /usr/share/zoneinfo/Asia/Shanghai /etc/localtime ---&gt; Running in aeb9baee5588&#x27;/etc/localtime&#x27; -&gt; &#x27;/usr/share/zoneinfo/Asia/Shanghai&#x27;Removing intermediate container aeb9baee5588 ---&gt; 3e7396341559Step 4/6 : ADD docker.sh /home/test/ ---&gt; 55b4caef0d35Step 5/6 : RUN chmod +x /home/test/docker.sh ---&gt; Running in 6d8035f7ddebRemoving intermediate container 6d8035f7ddeb ---&gt; 69498dec7faeStep 6/6 : CMD [&quot;/home/test/docker.sh&quot;] ---&gt; Running in 2a823763ffe9Removing intermediate container 2a823763ffe9 ---&gt; a456dd68d802Successfully built a456dd68d802 #构建成功Successfully tagged zhaoshuo/centos7:1[root@localhost 7]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos7 1 a456dd68d802 57 seconds ago 200MB #TAG为1centos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MBhello-world latest 4ab4c602aa5e 5 weeks ago 1.84kB 3.5 Docker命令：run 运行一个本地镜像 123456789101112131415[root@localhost 7]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos7 1 a456dd68d802 57 seconds ago 200MB centos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MBhello-world latest 4ab4c602aa5e 5 weeks ago 1.84kB[root@localhost 7]# docker run -d -it a456dd68d802 /bin/bash5e993afbfc0f4b1b421f2414cb890a492c562a2306401e03796dfccc6b0f7c4b特别提示：docker run：运行一个指定的images id-d：放在后台运行-i：可以进行命令交互-t：制作一个伪终端用于登陆a456dd68d802:镜像的ID，可以简写成a45 3.6 Docker命令：ps 查看已经运行的镜像的进程 123456[root@localhost 7]# docker ps -a #查看所有运行的镜像进程（包含退出的exit）CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5e993afbfc0f a456dd68d802 &quot;/bin/bash&quot; 2 minutes ago Up 2 minutes reverent_euclid特别提示：STATUS：进程的状态，UP表示正在运行中，EXIT表示已经退出了。 3.7 Docker命令：attach 从本地系统中切入到某个STATUS状态是UP的镜像进程里 1234567891011121314[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5e993afbfc0f a456dd68d802 &quot;/bin/bash&quot; 2 minutes ago Up 2 minutes reverent_euclid[root@localhost 7]# docker attach 5e993afbfc0f #切入到容器号为5e993afbfc0f的镜像进程里[root@5e993afbfc0f /]# ls #已经进入容器里了anaconda-post.log dev home lib64 mnt proc run srv tmp varbin etc lib media opt root sbin sys usr[root@5e993afbfc0f /]# exit #退出容器exit[root@localhost 7]# docker ps -a #容器的进程的STATUS已经处于EXIT状态（之前是后台运行的，切入进去后执行exit就等于手动退出了）CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5e993afbfc0f a456dd68d802 &quot;/bin/bash&quot; 5 minutes ago Exited (0) 10 seconds ago reverent_euclid 3.8 Docker命令：stop 用于停止一个正在运行着的容器进程 1234567891011121314151617181920[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5e993afbfc0f a456dd68d802 &quot;/bin/bash&quot; 6 minutes ago Exited (0) About a minute ago reverent_euclid#再次在后台启动一个镜像[root@localhost 7]# docker run -d -it a456dd68d802 /bin/bashaf5a8cfcfe5ba25d2149441af123730db07105953dd9afa79827457d6e2c2e59[root@localhost 7]# docker ps -a #增加了一个容器进程CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 3 seconds ago Up 2 seconds xenodochial_colden5e993afbfc0f a456dd68d802 &quot;/bin/bash&quot; 7 minutes ago Exited (0) 2 minutes ago reverent_euclid#停止一个运行着的容器进程[root@localhost 7]# docker stop af5a8cfcfe5baf5a8cfcfe5b[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; About a minute ago Exited (137) 3 seconds ago xenodochial_colden5e993afbfc0f a456dd68d802 &quot;/bin/bash&quot; 8 minutes ago Exited (0) 3 minutes ago reverent_euclid 3.9 Docker命令：start 用于启动一个已经停止了的容器进程 1234567891011[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 2 minutes ago Exited (137) 44 seconds ago xenodochial_colden5e993afbfc0f a456dd68d802 &quot;/bin/bash&quot; 9 minutes ago Exited (0) 4 minutes ago reverent_euclid[root@localhost 7]# docker start af5a8cfcfe5baf5a8cfcfe5b[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 2 minutes ago Up 2 seconds xenodochial_colden5e993afbfc0f a456dd68d802 &quot;/bin/bash&quot; 9 minutes ago Exited (0) 4 minutes ago reverent_euclid 3.10 Docker命令：rm 用于删除一个已经停止了的容器进程 123456789101112[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 3 minutes ago Up About a minute xenodochial_colden5e993afbfc0f a456dd68d802 &quot;/bin/bash&quot; 10 minutes ago Exited (0) 5 minutes ago reverent_euclid[root@localhost ~]# docker rm 5e993afbfc0f5e993afbfc0f[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 3 minutes ago Up About a minute xenodochial_colden[root@localhost ~]# docker rm af5a8cfcfe5b #注意运行中的容器进程需要先stop，才能删除Error response from daemon: You cannot remove a running container af5a8cfcfe5ba25d2149441af123730db07105953dd9afa79827457d6e2c2e59. Stop the container before attempting removal or force remove 3.11 Docker命令：rmi 用于删除一个未用作容器启动的本地镜像 123456789101112131415161718192021222324[root@localhost ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos7 1 a456dd68d802 36 minutes ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MBhello-world latest 4ab4c602aa5e 5 weeks ago 1.84kB[root@localhost ~]# docker rmi -f 4ab4c602aa5e #-f 强制删除（即便被占用）Untagged: hello-world:latestUntagged: hello-world@sha256:0add3ace90ecb4adbf7777e9aacf18357296e799f81cabc9fde470971e499788Deleted: sha256:4ab4c602aa5eed5528a6620ff18a1dc4faef0e1ab3a5eddeddb410714478c67fDeleted: sha256:428c97da766c4c13b19088a471de6b622b038f3ae8efa10ec5a37d6d31a2df0b[root@localhost ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos7 1 a456dd68d802 37 minutes ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB[root@localhost ~]# docker rmi a456dd68d802 #但要注意，被用作容器启动的镜像是不能删除的（需先rm删除容器进程）Error response from daemon: conflict: unable to delete a456dd68d802 (cannot be forced) - image is being used by running container af5a8cfcfe5b[root@localhost ~]# docker rmi -f a456dd68d802 #强行删除被容器进程占用的镜像也是不行的Error response from daemon: conflict: unable to delete a456dd68d802 (cannot be forced) - image is being used by running container af5a8cfcfe5b[root@localhost ~]# docker ps -a #查看容器进程，被占用中CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 6 minutes ago Up 3 minutes xenodochial_colden 3.12 Docker命令：commit 将一个更改过的容器进程的容器状态保存为一个新的镜像 123456789101112131415161718192021222324252627282930313233343536373839[root@localhost ~]# docker ps -a #查看启动的容器进程CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 10 minutes ago Up 8 minutes xenodochial_colden[root@localhost ~]# docker attach af5a8cfcfe5b #切入容器进程[root@af5a8cfcfe5b /]# lsanaconda-post.log dev home lib64 mnt proc run srv tmp varbin etc lib media opt root sbin sys usr[root@af5a8cfcfe5b /]# mkdir yunjisuan #在容器进程里创建yunjisuan目录[root@af5a8cfcfe5b /]# lsanaconda-post.log dev home lib64 mnt proc run srv tmp varbin etc lib media opt root sbin sys usr yunjisuan[root@af5a8cfcfe5b /]# exit #退出容器进程exit[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 25 minutes ago Exited (0) 4 seconds ago xenodochial_colden[root@localhost ~]# docker commit af5a8cfcfe5b zhaoshuo/centos:2 #将更改后的容器进程保存为一个新的镜像sha256:d396465fd814d049063325b5f6c752e9cd0da0821ff42d9400c2a90ba2a42c5b[root@localhost ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 2 d396465fd814 5 seconds ago 200MBzhaoshuo/centos7 1 a456dd68d802 About an hour ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB#启动新保存的镜像[root@localhost ~]# docker run -d -it d396465fd814 /bin/bash06b28d43f70046b002a37a6431b658b0455dc261c4f55dd4f010c5aadde31f0e[root@localhost ~]# docker ps -a #查看新镜像的容器进程CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES06b28d43f700 d396465fd814 &quot;/bin/bash&quot; 9 seconds ago Up 9 seconds loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 27 minutes ago Exited (0) 2 minutes ago xenodochial_colden[root@localhost ~]# docker attach 06b28d43f700 #切入新镜像的容器进程[root@06b28d43f700 /]# ls #之前创建的目录仍旧存在anaconda-post.log dev home lib64 mnt proc run srv tmp varbin etc lib media opt root sbin sys usr yunjisuan 3.13 Docker命令：exec 用于从本地操作系统上直接向容器进程发布执行命令并返回结果 1234567891011[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES06b28d43f700 d396465fd814 &quot;/bin/bash&quot; 2 minutes ago Up 7 seconds loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 29 minutes ago Exited (0) 4 minutes ago xenodochial_colden[root@localhost ~]# docker exec 06b28d43f700 ls /tmp #查看容器进程里的/tmp目录下所有内容ks-script-7RxiSxyum.log[root@localhost ~]# docker exec 06b28d43f700 ls -d /yunjisuan #查看容器进程里/yunjisuan目录/yunjisuandocker exec -it ee782b37bb82 bash #进入到容器里面 3.14 Docker命令：cp 用于在容器进程和本地系统之间复制文件 12345678910111213141516[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES06b28d43f700 d396465fd814 &quot;/bin/bash&quot; 31 minutes ago Up 29 minutes loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; About an hour ago Exited (0) 33 minutes ago xenodochial_colden[root@localhost ~]# docker exec 06b28d43f700 ls /tmp #向容器进程发布命令ks-script-7RxiSxyum.log[root@localhost ~]# docker cp 06b28d43f700:/tmp/yum.log . #将指定容器进程的/tmp/yum.log复制到当前目录下[root@localhost ~]# ls #已经复制过来了anaconda-ks.cfg dockerfile wordpress-4.7.4-zh_CN.tar.gz yum.log[root@localhost ~]# docker cp anaconda-ks.cfg 06b28d43f700:/tmp/ #将本地文件复制到容器进程里[root@localhost ~]# docker exec 06b28d43f700 ls /tmpanaconda-ks.cfg #复制成功 ks-script-7RxiSxyum.log 3.15 Docker命令：create 用于创建一个容器进程，但是并不启动它 123456789101112131415161718192021[root@localhost ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 2 d396465fd814 About an hour ago 200MBzhaoshuo/centos7 1 a456dd68d802 2 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB[root@localhost ~]# docker create -it d396465fd814 #创建一个镜像的容器进程，但不直接启动5969d37135488e62ca32073163b86eb8bafc2d86854b16df58ae01ce858d3c7a[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5969d3713548 d396465fd814 &quot;/bin/bash&quot; 8 seconds ago Created optimistic_lewin #创建状态并未启动06b28d43f700 d396465fd814 &quot;/bin/bash&quot; About an hour ago Up About an hour loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 2 hours ago Exited (0) About an hour ago xenodochial_colden[root@localhost ~]# docker start 5969d3713548 #启动容器进程5969d3713548[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5969d3713548 d396465fd814 &quot;/bin/bash&quot; 46 seconds ago Up 1 second optimistic_lewin06b28d43f700 d396465fd814 &quot;/bin/bash&quot; About an hour ago Up About an hour loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 2 hours ago Exited (0) About an hour ago xenodochial_colden 3.16 Docker命令：diff 查看容器进程与源镜像做对比，发生了改变的文件或文件夹 1234567891011121314151617181920212223242526272829[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5969d3713548 d396465fd814 &quot;/bin/bash&quot; 2 minutes ago Up About a minute optimistic_lewin06b28d43f700 d396465fd814 &quot;/bin/bash&quot; About an hour ago Up About an hour loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 2 hours ago Exited (0) About an hour ago xenodochial_colden[root@localhost ~]# docker attach 5969d3713548 #切入容器进程[root@5969d3713548 /]# cd /yunjisuan/[root@5969d3713548 yunjisuan]# pwd /yunjisuan[root@5969d3713548 yunjisuan]# touch &#123;1..10&#125; #在容器进程中创建文件[root@5969d3713548 yunjisuan]# ls1 10 2 3 4 5 6 7 8 9[root@5969d3713548 yunjisuan]# exit #退出容器进程exit [root@localhost ~]# docker diff 5969d3713548 #查看容器进程的变化C /rootC /root/.bash_historyC /yunjisuanA /yunjisuan/1A /yunjisuan/7A /yunjisuan/3A /yunjisuan/4A /yunjisuan/5A /yunjisuan/6A /yunjisuan/8A /yunjisuan/9A /yunjisuan/10A /yunjisuan/2 3.17 Docker命令：events 时时监测容器的变化情况 123[root@localhost ~]# docker events2018-10-14T00:23:29.522641941+08:00 container kill 06b28d43f70046b002a37a6431b658b0455dc261c4f55dd4f010c5aadde31f0e (MAINTATNER=zhaoshuo, image=d396465fd814, name=loving_jepsen, org.label-schema.build-date=20181006, org.label-schema.license=GPLv2, org.label-schema.name=CentOS Base Image, org.label-schema.schema-version=1.0, org.label-schema.vendor=CentOS, signal=15) 前台时时监控容器的变化若要检测，需要另外再起一个窗口进行操作 3.18 Docker命令：export 将容器进程的文件系统导出到本地 12345678910111213141516[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5969d3713548 d396465fd814 &quot;/bin/bash&quot; 7 minutes ago Exited (0) 3 minutes ago optimistic_lewin06b28d43f700 d396465fd814 &quot;/bin/bash&quot; About an hour ago Exited (137) 37 seconds ago loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 2 hours ago Exited (0) About an hour ago xenodochial_colden[root@localhost ~]# docker start 596 #启动一个容器进程596[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5969d3713548 d396465fd814 &quot;/bin/bash&quot; 7 minutes ago Up 3 seconds optimistic_lewin06b28d43f700 d396465fd814 &quot;/bin/bash&quot; About an hour ago Exited (137) 54 seconds ago loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 2 hours ago Exited (0) About an hour ago xenodochial_colden[root@localhost ~]# docker export 5969d3713548 &gt; test.tar #将容器进程导出成一个tar包[root@localhost ~]# lsanaconda-ks.cfg dockerfile test.tar wordpress-4.7.4-zh_CN.tar.gz yum.log 3.19 Docker命令：import 用于将export导出的文件系统创建为一个镜像 123456789101112[root@localhost ~]# lsanaconda-ks.cfg dockerfile test.tar wordpress-4.7.4-zh_CN.tar.gz yum.log[root@localhost ~]# docker import test.tar zhaoshuo/centos:3sha256:0ac5e5b6feadedc5f47130d165cc911fc6170cc1423abe45d74abf67ec3c29b6[root@localhost ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 3 0ac5e5b6fead 12 seconds ago 200MBzhaoshuo/centos 2 d396465fd814 About an hour ago 200MBzhaoshuo/centos7 1 a456dd68d802 2 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB 3.20 Docker命令：history 用于查看一个镜像的历史修改纪录 123456789101112131415161718192021[root@localhost ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 3 0ac5e5b6fead 40 seconds ago 200MBzhaoshuo/centos 2 d396465fd814 About an hour ago 200MBzhaoshuo/centos7 1 a456dd68d802 2 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB[root@localhost ~]# docker history 0ac5e5b6feadIMAGE CREATED CREATED BY SIZE COMMENT0ac5e5b6fead About a minute ago 200MB Imported from -[root@localhost ~]# docker history d396465fd814IMAGE CREATED CREATED BY SIZE COMMENTd396465fd814 About an hour ago /bin/bash 27B a456dd68d802 2 hours ago /bin/sh -c #(nop) CMD [&quot;/home/test/docker.s… 0B 69498dec7fae 2 hours ago /bin/sh -c chmod +x /home/test/docker.sh 63B 55b4caef0d35 2 hours ago /bin/sh -c #(nop) ADD file:5bd1c2d39928ac84c… 63B 3e7396341559 2 hours ago /bin/sh -c ln -sfv /usr/share/zoneinfo/Asia/… 33B 8af9c0b32fec 2 hours ago /bin/sh -c #(nop) LABEL MAINTATNER=zhaoshuo 0B 75835a67d134 3 days ago /bin/sh -c #(nop) CMD [&quot;/bin/bash&quot;] 0B &lt;missing&gt; 3 days ago /bin/sh -c #(nop) LABEL org.label-schema.sc… 0B &lt;missing&gt; 3 days ago /bin/sh -c #(nop) ADD file:fbe9badfd2790f074… 200MB 3.21 Docker命令：info 用于查看当前操作系统的docker运行信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@localhost ~]# docker infoContainers: 3 #容器进程3个 Running: 1 #正在运行状态的容器1个 Paused: 0 Stopped: 2Images: 8 #一共有8个镜像Server Version: 18.06.1-ceStorage Driver: overlay2 Backing Filesystem: xfs Supports d_type: true Native Overlay Diff: trueLogging Driver: json-fileCgroup Driver: cgroupfsPlugins: Volume: local Network: bridge host macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslogSwarm: inactiveRuntimes: runcDefault Runtime: runcInit Binary: docker-initcontainerd version: 468a545b9edcd5932818eb9de8e72413e616e86erunc version: 69663f0bd4b60df09991c08812a60108003fa340init version: fec3683Security Options: seccomp Profile: defaultKernel Version: 3.10.0-862.el7.x86_64Operating System: CentOS Linux 7 (Core)OSType: linuxArchitecture: x86_64CPUs: 1Total Memory: 974.6MiBName: localhost.localdomainID: MUQB:NDRG:NF4L:KCWN:2WKA:DVZ2:CJKJ:CBFO:MP5J:XSIK:CTZO:SJJYDocker Root Dir: /var/lib/dockerDebug Mode (client): falseDebug Mode (server): falseRegistry: https://index.docker.io/v1/Labels:Experimental: falseInsecure Registries: 127.0.0.0/8Registry Mirrors: https://registry.docker-cn.com/Live Restore Enabled: false 3.22 Docker命令：inspect 查看某个镜像的详细信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283[root@localhost ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 3 0ac5e5b6fead 2 minutes ago 200MBzhaoshuo/centos 2 d396465fd814 About an hour ago 200MBzhaoshuo/centos7 1 a456dd68d802 2 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB[root@localhost ~]# docker inspect 0ac5e5b6fead[ &#123; &quot;Id&quot;: &quot;sha256:0ac5e5b6feadedc5f47130d165cc911fc6170cc1423abe45d74abf67ec3c29b6&quot;, &quot;RepoTags&quot;: [ &quot;zhaoshuo/centos:3&quot; ], &quot;RepoDigests&quot;: [], &quot;Parent&quot;: &quot;&quot;, &quot;Comment&quot;: &quot;Imported from -&quot;, &quot;Created&quot;: &quot;2018-10-13T16:26:34.706635044Z&quot;, &quot;Container&quot;: &quot;&quot;, &quot;ContainerConfig&quot;: &#123; &quot;Hostname&quot;: &quot;&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: false, &quot;AttachStdout&quot;: false, &quot;AttachStderr&quot;: false, &quot;Tty&quot;: false, &quot;OpenStdin&quot;: false, &quot;StdinOnce&quot;: false, &quot;Env&quot;: null, &quot;Cmd&quot;: null, &quot;Image&quot;: &quot;&quot;, &quot;Volumes&quot;: null, &quot;WorkingDir&quot;: &quot;&quot;, &quot;Entrypoint&quot;: null, &quot;OnBuild&quot;: null, &quot;Labels&quot;: null &#125;, &quot;DockerVersion&quot;: &quot;18.06.1-ce&quot;, &quot;Author&quot;: &quot;&quot;, &quot;Config&quot;: &#123; &quot;Hostname&quot;: &quot;&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: false, &quot;AttachStdout&quot;: false, &quot;AttachStderr&quot;: false, &quot;Tty&quot;: false, &quot;OpenStdin&quot;: false, &quot;StdinOnce&quot;: false, &quot;Env&quot;: null, &quot;Cmd&quot;: null, &quot;Image&quot;: &quot;&quot;, &quot;Volumes&quot;: null, &quot;WorkingDir&quot;: &quot;&quot;, &quot;Entrypoint&quot;: null, &quot;OnBuild&quot;: null, &quot;Labels&quot;: null &#125;, &quot;Architecture&quot;: &quot;amd64&quot;, &quot;Os&quot;: &quot;linux&quot;, &quot;Size&quot;: 200375235, &quot;VirtualSize&quot;: 200375235, &quot;GraphDriver&quot;: &#123; &quot;Data&quot;: &#123; &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/1ce82fb0cc05791fa02aa4d77f4bc34001c725f16d5389b36540e9bef1fb05b5/merged&quot;, &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/1ce82fb0cc05791fa02aa4d77f4bc34001c725f16d5389b36540e9bef1fb05b5/diff&quot;, &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/1ce82fb0cc05791fa02aa4d77f4bc34001c725f16d5389b36540e9bef1fb05b5/work&quot; &#125;, &quot;Name&quot;: &quot;overlay2&quot; &#125;, &quot;RootFS&quot;: &#123; &quot;Type&quot;: &quot;layers&quot;, &quot;Layers&quot;: [ &quot;sha256:2a18295dfc1f31d9fc66201ce7c08e5a84e65f514bae0ae4fcbcf38cdd541659&quot; ] &#125;, &quot;Metadata&quot;: &#123; &quot;LastTagTime&quot;: &quot;2018-10-14T00:26:36.091455174+08:00&quot; &#125; &#125;] 3.23 Docker命令：kill 强行停止一个或多个正在运行状态的容器进程 12345678910111213[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5969d3713548 d396465fd814 &quot;/bin/bash&quot; 13 minutes ago Up 5 minutes optimistic_lewin06b28d43f700 d396465fd814 &quot;/bin/bash&quot; About an hour ago Exited (137) 6 minutes ago loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 2 hours ago Exited (0) About an hour ago xenodochial_colden[root@localhost ~]# docker kill 5969d37135485969d3713548[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5969d3713548 d396465fd814 &quot;/bin/bash&quot; 14 minutes ago Exited (137) 1 second ago optimistic_lewin06b28d43f700 d396465fd814 &quot;/bin/bash&quot; About an hour ago Exited (137) 7 minutes ago loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 2 hours ago Exited (0) 2 hours ago xenodochial_colden 3.24 Docker命令：save 用于将一个镜像的文件系统导出到本地（export导出的是容器） 1234567891011121314151617181920212223242526272829303132333435#重新build一个镜像[root@localhost ~]# cd dockerfile/lib/centos/7/[root@localhost 7]# docker build -t test:1 .Sending build context to Docker daemon 3.072kBStep 1/6 : FROM centos ---&gt; 75835a67d134Step 2/6 : LABEL MAINTATNER=&quot;zhaoshuo&quot; ---&gt; Using cache ---&gt; 8af9c0b32fecStep 3/6 : RUN ln -sfv /usr/share/zoneinfo/Asia/Shanghai /etc/localtime ---&gt; Using cache ---&gt; 3e7396341559Step 4/6 : ADD docker.sh /home/test/ ---&gt; Using cache ---&gt; 55b4caef0d35Step 5/6 : RUN chmod +x /home/test/docker.sh ---&gt; Using cache ---&gt; 69498dec7faeStep 6/6 : CMD [&quot;/home/test/docker.sh&quot;] ---&gt; Using cache ---&gt; a456dd68d802Successfully built a456dd68d802Successfully tagged test:1[root@localhost 7]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 3 0ac5e5b6fead 5 minutes ago 200MBzhaoshuo/centos 2 d396465fd814 2 hours ago 200MBtest 1 a456dd68d802 2 hours ago 200MBzhaoshuo/centos7 1 a456dd68d802 2 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB[root@localhost 7]# docker save test:1 &gt; test.tar #save导出test：1这个镜像（也可以用id号）[root@localhost 7]# lsDockerfile docker.sh test.tar 3.25 Docker命令：load 用于将save导出到本地的tar包，重新加载为镜像（和源镜像的名字标识完全一样） 12345678910111213141516171819202122232425262728[root@localhost 7]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 3 0ac5e5b6fead 7 minutes ago 200MBzhaoshuo/centos 2 d396465fd814 2 hours ago 200MBtest 1 a456dd68d802 3 hours ago 200MBzhaoshuo/centos7 1 a456dd68d802 3 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB[root@localhost 7]# docker rmi test:1 #删除镜像test：1Untagged: test:1[root@localhost 7]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 3 0ac5e5b6fead 8 minutes ago 200MBzhaoshuo/centos 2 d396465fd814 2 hours ago 200MBzhaoshuo/centos7 1 a456dd68d802 3 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB[root@localhost 7]# docker load &lt; test.tar #将之前test：1这个镜像的save备份导入系统Loaded image: test:1[root@localhost 7]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 3 0ac5e5b6fead 8 minutes ago 200MBzhaoshuo/centos 2 d396465fd814 2 hours ago 200MBtest 1 a456dd68d802 3 hours ago 200MB #和原来的镜像完全一样zhaoshuo/centos7 1 a456dd68d802 3 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB 3.26 Docker命令：logs 用于输出一个容器进程内的操作日志 1234567891011121314151617181920212223242526272829303132333435[root@localhost 7]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 3 0ac5e5b6fead 19 minutes ago 200MBzhaoshuo/centos 2 d396465fd814 2 hours ago 200MBzhaoshuo/centos7 1 a456dd68d802 3 hours ago 200MBtest 1 a456dd68d802 3 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5969d3713548 d396465fd814 &quot;/bin/bash&quot; 29 minutes ago Exited (137) 15 minutes ago optimistic_lewin06b28d43f700 d396465fd814 &quot;/bin/bash&quot; 2 hours ago Exited (137) 22 minutes ago loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 2 hours ago Exited (0) 2 hours ago xenodochial_colden[root@localhost 7]# docker run -d -it test:128eea5848ffcdd22ff144cdaf31732efd86aa23dcef4da85225face907dcf612[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES28eea5848ffc test:1 &quot;/home/test/docker.sh&quot; 2 seconds ago Up 1 second dazzling_ride5969d3713548 d396465fd814 &quot;/bin/bash&quot; 29 minutes ago Exited (137) 15 minutes ago optimistic_lewin06b28d43f700 d396465fd814 &quot;/bin/bash&quot; 2 hours ago Exited (137) 22 minutes ago loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 2 hours ago Exited (0) 2 hours ago xenodochial_colden[root@localhost 7]# docker logs 28eea5848ffcwelcomewelcomewelcomewelcomewelcome[root@localhost 7]# docker logs --tail 5 28eea5848ffc #只显示容器日志的后5行welcomewelcomewelcomewelcomewelcome 3.27 Docker命令：pause &amp;&amp; unpause 用于将一个或多个容器的进程暂停和恢复 1234567891011121314151617181920212223[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES28eea5848ffc test:1 &quot;/home/test/docker.sh&quot; 6 minutes ago Up 6 minutes dazzling_ride5969d3713548 d396465fd814 &quot;/bin/bash&quot; 35 minutes ago Exited (137) 21 minutes ago optimistic_lewin06b28d43f700 d396465fd814 &quot;/bin/bash&quot; 2 hours ago Exited (137) 28 minutes ago loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 2 hours ago Exited (0) 2 hours ago xenodochial_colden[root@localhost 7]# docker pause 28eea5848ffc #暂停容器进程28eea5848ffc[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES28eea5848ffc test:1 &quot;/home/test/docker.sh&quot; 6 minutes ago Up 6 minutes (Paused) dazzling_ride5969d3713548 d396465fd814 &quot;/bin/bash&quot; 36 minutes ago Exited (137) 22 minutes ago optimistic_lewin06b28d43f700 d396465fd814 &quot;/bin/bash&quot; 2 hours ago Exited (137) 29 minutes ago loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 2 hours ago Exited (0) 2 hours ago xenodochial_colden[root@localhost 7]# docker unpause 28eea5848ffc #恢复容器进程28eea5848ffc[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES28eea5848ffc test:1 &quot;/home/test/docker.sh&quot; 7 minutes ago Up 7 minutes dazzling_ride5969d3713548 d396465fd814 &quot;/bin/bash&quot; 36 minutes ago Exited (137) 22 minutes ago optimistic_lewin06b28d43f700 d396465fd814 &quot;/bin/bash&quot; 2 hours ago Exited (137) 29 minutes ago loving_jepsenaf5a8cfcfe5b a456dd68d802 &quot;/bin/bash&quot; 2 hours ago Exited (0) 2 hours ago xenodochial_colden 3.28 Docker命令：port 用于列出一个容器的端口映射及协议 12345678910111213141516[root@localhost 7]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 3 0ac5e5b6fead 31 minutes ago 200MBzhaoshuo/centos 2 d396465fd814 2 hours ago 200MBtest 1 a456dd68d802 3 hours ago 200MBzhaoshuo/centos7 1 a456dd68d802 3 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB[root@localhost 7]# docker run -d -it -p 2222:22 test:1 #启动一个镜像的容器进程 -p 指定本地2222端口映射到容器的22端口30cf521af35820e9bf3803ab526f02065bdb2935a4f58c7278fb17a99ea71163[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES30cf521af358 test:1 &quot;/home/test/docker.sh&quot; 4 seconds ago Up 3 seconds 0.0.0.0:2222-&gt;22/tcp hardcore_cori [root@localhost 7]# docker port 30cf521af358 #查看容器进程的端口映射及协议22/tcp -&gt; 0.0.0.0:2222 3.29 Docker命令：rename 给容器进程重命名 12345678[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES30cf521af358 test:1 &quot;/home/test/docker.sh&quot; About a minute ago Up About a minute 0.0.0.0:2222-&gt;22/tcp hardcore_cori[root@localhost 7]# docker rename hardcore_cori zhaoshuo[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES30cf521af358 test:1 &quot;/home/test/docker.sh&quot; 2 minutes ago Up 2 minutes 0.0.0.0:2222-&gt;22/tcp zhaoshuo 3.30 Docker命令：restart 重启一个容器进程 123456789[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES30cf521af358 test:1 &quot;/home/test/docker.sh&quot; 3 minutes ago Up 3 minutes 0.0.0.0:2222-&gt;22/tcp zhaoshuo[root@localhost 7]# docker restart 30cf521af35830cf521af358[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES30cf521af358 test:1 &quot;/home/test/docker.sh&quot; 3 minutes ago Up 1 second 0.0.0.0:2222-&gt;22/tcp zhaoshuo 3.31 Docker命令：stats 用于时时输出容器的资源使用情况 12345[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES30cf521af358 test:1 &quot;/home/test/docker.sh&quot; 5 minutes ago Up 2 minutes 0.0.0.0:2222-&gt;22/tcp zhaoshuo[root@localhost 7]# docker stats 30cf521af358 12345#--no-tream只输出一次[root@localhost 7]# docker stats 30cf521af358 --no-streamCONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS30cf521af358 zhaoshuo 0.00% 288KiB / 974.6MiB 0.03% 648B / 0B 0B / 0B 2 3.32 Docker命令：tag 用于从一个指定的镜像创建另外一个镜像 123456789101112131415161718192021222324252627282930313233343536373839[root@localhost 7]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 3 0ac5e5b6fead 39 minutes ago 200MBzhaoshuo/centos 2 d396465fd814 2 hours ago 200MBtest 1 a456dd68d802 3 hours ago 200MBzhaoshuo/centos7 1 a456dd68d802 3 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB[root@localhost 7]# docker tag test:1 test:2 #我们可以指定名字：标志来创建[root@localhost 7]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 3 0ac5e5b6fead 39 minutes ago 200MBzhaoshuo/centos 2 d396465fd814 2 hours ago 200MBzhaoshuo/centos7 1 a456dd68d802 3 hours ago 200MBtest 1 a456dd68d802 3 hours ago 200MBtest 2 a456dd68d802 3 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB[root@localhost 7]# docker rmi test:2Untagged: test:2[root@localhost 7]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 3 0ac5e5b6fead 39 minutes ago 200MBzhaoshuo/centos 2 d396465fd814 2 hours ago 200MBtest 1 a456dd68d802 3 hours ago 200MBzhaoshuo/centos7 1 a456dd68d802 3 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB[root@localhost 7]# docker tag a456dd68d802 test:2 #我们也可以指定image id来创建[root@localhost 7]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos 3 0ac5e5b6fead 40 minutes ago 200MBzhaoshuo/centos 2 d396465fd814 2 hours ago 200MBtest 1 a456dd68d802 3 hours ago 200MBtest 2 a456dd68d802 3 hours ago 200MBzhaoshuo/centos7 1 a456dd68d802 3 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MB 3.33 Docker命令：top 用于显示指定容器的进程信息 12345678[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES30cf521af358 test:1 &quot;/home/test/docker.sh&quot; 9 minutes ago Up 5 minutes 0.0.0.0:2222-&gt;22/tcp zhaoshuo[root@localhost 7]# docker top 30cf521af358UID PID PPID C STIME TTY TIME CMDroot 4307 4293 0 01:02 pts/0 00:00:00 /bin/bash /home/test/docker.shroot 4484 4307 0 01:08 pts/0 00:00:00 sleep 5 3.34 Docker命令：update 用于调整一个或多个容器的启动配置 1234567891011121314151617181920212223[root@localhost 7]# docker update --helpUsage: docker update [OPTIONS] CONTAINER [CONTAINER...]Update configuration of one or more containersOptions: --blkio-weight uint16 Block IO (relative weight), between 10 and 1000, or 0 to disable (default 0) --cpu-period int Limit CPU CFS (Completely Fair Scheduler) period --cpu-quota int Limit CPU CFS (Completely Fair Scheduler) quota --cpu-rt-period int Limit the CPU real-time period in microseconds --cpu-rt-runtime int Limit the CPU real-time runtime in microseconds -c, --cpu-shares int CPU shares (relative weight) --cpus decimal Number of CPUs --cpuset-cpus string CPUs in which to allow execution (0-3, 0,1) --cpuset-mems string MEMs in which to allow execution (0-3, 0,1) --kernel-memory bytes Kernel memory limit -m, --memory bytes Memory limit --memory-reservation bytes Memory soft limit --memory-swap bytes Swap limit equal to memory plus swap: &#x27;-1&#x27; to enable unlimited swap --restart string Restart policy to apply when a container exits 3.35 Docker命令：version and wait version用于显示docker的版本信息wait用于捕捉一个或多个容器的退出状态,并返回退出状态码 1234567891011121314151617181920212223242526272829#显示docker版本信息[root@localhost 7]# docker versionClient: Version: 18.06.1-ce API version: 1.38 Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17:23:03 2018 OS/Arch: linux/amd64 Experimental: falseServer: Engine: Version: 18.06.1-ce API version: 1.38 (minimum version 1.12) Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17:25:29 2018 OS/Arch: linux/amd64 Experimental: false #监听容器的退出状态并返回状态码[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES30cf521af358 test:1 &quot;/home/test/docker.sh&quot; 11 minutes ago Up 8 minutes 0.0.0.0:2222-&gt;22/tcp zhaoshuo[root@localhost 7]# docker wait 30cf521af358 #需要在开一个窗口stop这个容器进程再查看137 3.36 Docker命令：login &amp;&amp; logout &amp;&amp; push login用于登陆docker hub官方公有仓库 logout用于登出docker hub官方公有仓库 push用于将本地镜像提交到docker hub DockerHub官方公有镜像仓库：https://hub.docker.com/ 四，管理应用程序数据4.1 Volume和Bind Mount将Docker主机数据挂载到容器 Docker提供三种不同方式将数据从宿主机挂载到容器中：volumes，bind mounts和tmpfs。 volumes：Docker管理宿主机文件系统的一部分（&#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes） bind mounts:可以存储在宿主机系统的任意位置 tmpfs：挂载存储在宿主机系统的内存中，而不会写入宿主机的文件系统 4.1.1 Volume123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#创建一个卷[root@localhost 7]# docker volume create nginx-volnginx-vol[root@localhost 7]# docker volume lsDRIVER VOLUME NAMElocal nginx-vol[root@localhost 7]# docker volume inspect nginx-vol[ &#123; &quot;CreatedAt&quot;: &quot;2018-10-14T01:14:25+08:00&quot;, #创建时间 &quot;Driver&quot;: &quot;local&quot;, #驱动 &quot;Labels&quot;: &#123;&#125;, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/nginx-vol/_data&quot;, #挂载点 &quot;Name&quot;: &quot;nginx-vol&quot;, #卷名 &quot;Options&quot;: &#123;&#125;, &quot;Scope&quot;: &quot;local&quot; &#125;]#下载一个nginx官方镜像[root@localhost 7]# docker pull nginx[root@localhost 7]# docker pull nginxUsing default tag: latestlatest: Pulling from library/nginx802b00ed6f79: Pull complete 5291925314b3: Pull complete bd9f53b2c2de: Pull complete Digest: sha256:9ad0746d8f2ea6df3a17ba89eca40b48c47066dfab55a75e08e2b70fc80d929eStatus: Downloaded newer image for nginx:latest[root@localhost 7]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos7 1 a456dd68d802 3 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MBnginx latest be1f31be9a87 10 days ago 109MB[root@localhost 7]# docker run -dit --name=nginx-test --mount src=nginx-vol,dst=/usr/share/nginx/html nginx499069e77ce67f55cb1c716d55fc08d783e3a2f2b7fbba9da9d9ed7d010cf1a9特别说明：--name：容器的名字--mount：挂载src：源卷的名字dst：挂载到容器中的路径[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES499069e77ce6 nginx &quot;nginx -g &#x27;daemon of…&quot; 15 seconds ago Up 14 seconds 80/tcp nginx-test#向容器中的挂载目录创建文件，查看是否挂载成功[root@localhost 7]# docker exec nginx-test touch /usr/share/nginx/html/test.txt[root@localhost 7]# docker exec nginx-test ls /usr/share/nginx/html/50x.htmlindex.htmltest.txt #有了[root@localhost 7]# ls /var/lib/docker/volumes/nginx-vol/_data/50x.html index.html test.txt #成功#清理容器进程[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES499069e77ce6 nginx &quot;nginx -g &#x27;daemon of…&quot; About a minute ago Up About a minute 80/tcp nginx-test[root@localhost 7]# docker stop nginx-testnginx-test[root@localhost 7]# docker rm nginx-testnginx-test[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES[root@localhost 7]# ls /var/lib/docker/volumes/nginx-vol/_data/50x.html index.html test.txt #清理容器后，挂载的卷的数据仍旧存在#重新启动镜像nginx的容器进程[root@localhost 7]# docker run -dit --name nginx-test -p 88:80 --mount src=nginx-vol,dst=/usr/share/nginx/html nginxba61800faa00559c66a557ee6e9e584c29d7af5f73f635fcbd9b83066b235331[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba61800faa00 nginx &quot;nginx -g &#x27;daemon of…&quot; 4 seconds ago Up 3 seconds 0.0.0.0:88-&gt;80/tcp nginx-test特别提示：docker run的-p参数：指定端口的映射，88：80的意思就是将宿主机88端口的访问映射到容器进程的80端口 现在通过浏览器访问宿主机的88端口，进而访问容器进程的80端口 1234#在数据卷nginx-vol里换一个网页在访问[root@localhost 7]# rm -rf /var/lib/docker/volumes/nginx-vol/_data/*[root@localhost 7]# echo &quot;welcome to yunjisuan&quot; &gt; /var/lib/docker/volumes/nginx-vol/_data/index.html 在启动一个镜像nginx的进程，让两个nginx的容器进程公用一个数据卷nginx-vol 1234567[root@localhost 7]# docker run -dit --name nginx-test2 -p 89:80 --mount src=nginx-vol,dst=/usr/share/nginx/html nginx78a9c5517e94527c6822550781cfce5e4c434c322a75541f37cd78333e3c9741[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES78a9c5517e94 nginx &quot;nginx -g &#x27;daemon of…&quot; 2 seconds ago Up 2 seconds 0.0.0.0:89-&gt;80/tcp nginx-test2ba61800faa00 nginx &quot;nginx -g &#x27;daemon of…&quot; 3 minutes ago Up 3 minutes 0.0.0.0:88-&gt;80/tcp nginx-test 用浏览器访问docker宿主机的89端口 4.1.2 Bind Mounts123456789101112[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES78a9c5517e94 nginx &quot;nginx -g &#x27;daemon of…&quot; About a minute ago Up About a minute 0.0.0.0:89-&gt;80/tcp nginx-test2ba61800faa00 nginx &quot;nginx -g &#x27;daemon of…&quot; 5 minutes ago Up 5 minutes 0.0.0.0:88-&gt;80/tcp nginx-test[root@localhost 7]# docker run -dit --name nginx-test3 -p 90:80 --mount type=bind,src=/var/lib/docker/volumes/nginx-vol/_data,dst=/usr/share/nginx/html nginx9cd50b218764905cdc1d37c1c8f317938cc4d8e33cf762dabd0036846cc529fb[root@localhost 7]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES9cd50b218764 nginx &quot;nginx -g &#x27;daemon of…&quot; 2 seconds ago Up 1 second 0.0.0.0:90-&gt;80/tcp nginx-test378a9c5517e94 nginx &quot;nginx -g &#x27;daemon of…&quot; About a minute ago Up About a minute 0.0.0.0:89-&gt;80/tcp nginx-test2ba61800faa00 nginx &quot;nginx -g &#x27;daemon of…&quot; 5 minutes ago Up 5 minutes 0.0.0.0:88-&gt;80/tcp nginx-test 用浏览器访问docker宿主机的90端口 特别提示：bind mounts可以挂载宿主机上的任意目录，而volume先得创建后才能挂载 4.2 实战容器部署LNMP网站平台 首先下载一个wordpress博客wget https://cn.wordpress.org/wordpress-4.7.4-zh_CN.tar.gz 4.2.1 创建MySQL数据库容器123456789101112131415161718192021222324252627282930#下载MySQL5.6版本镜像[root@localhost ~]# docker pull mysql:5.6[root@localhost ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos7 1 a456dd68d802 3 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MBnginx latest be1f31be9a87 10 days ago 109MBmysql 5.6 1f47fade220d 5 weeks ago 256MB#创建一个自定义网络[root@localhost ~]# docker network create lnmpe9c6f3aab48b1048af23ad5347408ef0b460c3dbb1917f04565dcd7d4f8498b7[root@localhost ~]# docker network lsNETWORK ID NAME DRIVER SCOPEbb056f017977 bridge bridge local0f75cc017204 host host locale9c6f3aab48b lnmp bridge local #有了（默认驱动为网桥）c71020a2992f none null local#启动MySQL数据库容器[root@localhost ~]# docker run -dit --name lnmp_mysql --network lnmp -p 3306:3306 --mount src=mysql-vol,dst=/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123123 mysql:5.6 --character-set-server=utf86acbcd435954d0aef1ffd07a5ddb42581200e85d2c76e160cfc1bd2ee9ebe7fb[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES6acbcd435954 mysql:5.6 &quot;docker-entrypoint.s…&quot; 5 seconds ago Up 4 seconds 0.0.0.0:3306-&gt;3306/tcp lnmp_mysql9cd50b218764 nginx &quot;nginx -g &#x27;daemon of…&quot; 5 minutes ago Exited (0) 0.0.0.0:90-&gt;80/tcp nginx-test378a9c5517e94 nginx &quot;nginx -g &#x27;daemon of…&quot; 6 minutes ago Exited (0) 0.0.0.0:89-&gt;80/tcp nginx-test2ba61800faa00 nginx &quot;nginx -g &#x27;daemon of…&quot; 10 minutes ago Exited (0) 0.0.0.0:88-&gt;80/tcp nginx-test 特别提示： 自定义网络lnmp如果不提前创建的话，在启动容器进程时会报错 如果没有提前pull好mysql:5.6那么容器在启动时会自动下载对应镜像 如果没有提前docker volume create mysql-vol，那么容器启动时会自动创建 12345678#查看容器lnmp_mysql的日志输出[root@localhost ~]# docker logs lnmp_mysql#查看容器里启动的进程[root@localhost ~]# docker top lnmp_mysqlUID PID PPID C STIME TTY TIME CMDpolkitd 5328 5315 0 01:33 pts/0 00:00:00 mysqld --character-set-server=utf8 4.2.2 向容器里的Mysql创建一个库123[root@localhost ~]# docker exec lnmp_mysql sh -c &#x27;exec mysql -uroot -p&quot;$MYSQL_ROOT_PASSWORD&quot; -e&quot;create database wp&quot;&#x27;Warning: Using a password on the command line interface can be insecure. 4.2.3 在docker宿主机上安装mysql的客户端通过端口映射访问容器内的mysql 因为已经将mysql容器的3306端口映射到了docker宿主机的3306，因此访问本地即可 1234567891011121314151617181920212223242526272829#安装mysql客户端[root@localhost ~]# yum -y install mysql#查看本机IP地址[root@localhost ~]# hostname -I | xargs -n1 | head -1192.168.131.162xargs: echo：因信号 13 而终止#远程方式连接docker宿主机的3306端口[root@localhost ~]# mysql -h192.168.131.162 -P3306 -uroot -p123123Welcome to the MariaDB monitor. Commands end with ; or \\g.Your MySQL connection id is 2Server version: 5.6.41 MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.MySQL [(none)]&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || wp | #已经创建wp库了+--------------------+4 rows in set (0.10 sec) 4.2.4 创建nginx+PHP环境容器1234567891011121314151617181920212223242526#创建一个网页目录[root@localhost ~]# mkdir -p /app/wwwroot[root@localhost ~]# ll -d /app/wwwrootdrwxr-xr-x 2 root root 6 10月 14 01:39 /app/wwwroot#下载richarvey/nginx-php-fpm镜像[root@localhost ~]# docker pull richarvey/nginx-php-fpm[root@localhost ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEzhaoshuo/centos7 1 a456dd68d802 4 hours ago 200MBcentos 7 75835a67d134 3 days ago 200MBcentos latest 75835a67d134 3 days ago 200MBnginx latest be1f31be9a87 10 days ago 109MBricharvey/nginx-php-fpm latest de1554d0b081 2 weeks ago 302MBmysql 5.6 1f47fade220d 5 weeks ago 256MB#启动richarvey/nginx-php-fpm镜像的容器[root@localhost ~]# docker run -dit --name lnmp_web --network lnmp -p 88:80 --mount type=bind,src=/app/wwwroot,dst=/var/www/html richarvey/nginx-php-fpm[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES677a6ddd712e richarvey/nginx-php-fpm &quot;docker-php-entrypoi…&quot; 5 minutes ago Up 3 seconds 443/tcp, 9000/tcp, 0.0.0.0:88-&gt;80/tcp lnmp_web6acbcd435954 mysql:5.6 &quot;docker-entrypoint.s…&quot; 24 minutes ago Up 12 seconds 0.0.0.0:3306-&gt;3306/tcp lnmp_mysql9cd50b218764 nginx &quot;nginx -g &#x27;daemon of…&quot; 29 minutes ago Exited (0) 8 minutes ago nginx-test378a9c5517e94 nginx &quot;nginx -g &#x27;daemon of…&quot; 30 minutes ago Exited (0) 8 minutes ago nginx-test2ba61800faa00 nginx &quot;nginx -g &#x27;daemon of…&quot; 34 minutes ago Exited (0) 47 seconds ago nginx-test 4.2.5 解压wordpress到网页目录&#x2F;app&#x2F;wwwroot下12345[root@localhost ~]# tar xf wordpress-4.7.4-zh_CN.tar.gz -C /app/wwwroot/[root@localhost ~]# cd /app/wwwroot/[root@localhost wwwroot]# lswordpress 4.2.6 博客wordpress访问测试 通过浏览器进行docker宿主机的88端口的访问测试http://IP:88/wordpress 特别提示： 如果在访问时，出现以下情况 12345678#如果出现连接不上的情况，那么请按顺序执行以下命令一遍[root@docker wwwroot]# systemctl stop firewalld[root@docker wwwroot]# systemctl stop iptables.serviceFailed to stop iptables.service: Unit iptables.service not loaded.[root@docker wwwroot]# iptables -F[root@docker wwwroot]# iptables -P FORWARD ACCEPT[root@docker wwwroot]# iptables -P INPUT ACCEPT[root@docker wwwroot]# iptables -P OUTPUT ACCEPT 再次访问 成功！！特别提示：如果多次连续访问同一网页，那么浏览器有可能默认去掉指定的端口因此，若访问不到，请查看是否指定了88端口","categories":[{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://kkabuzs.github.io/tags/Docker/"}]},{"title":"Tomcat详解","slug":"tomcatxiangjie","date":"2018-08-31T10:20:44.000Z","updated":"2018-08-31T10:20:44.000Z","comments":true,"path":"articles/2018/08/31/tomcatxiangjie/","permalink":"https://kkabuzs.github.io/articles/2018/08/31/tomcatxiangjie/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Tomcat详解一，Tomcat简介 Tomcat是Apache软件基金会（Apache Software Foundation）的Jakarta项目中的一个核心项目，由Apache，Sun和其他一些公司及个人共同开发而成。 Tomcat服务器是一个免费的开放源代码的Web应用服务器，属于轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下被普遍使用，是开发和调试JSP程序的首选。 Tomcat和Nginx，APache（httpd），lighttpd等Web服务器一样，具有处理HTML页面的功能，另外它还是一个Servlet和JSP容器，独立的Servlet容器是Tomcat的默认模式。不过，Tomcat处理静态HTML的能力不如Nginx／Apache服务器。 二，Tomcat安装2.1 软件准备JDK下载：https://github.com/frekele/oracle-java/releases Tomcat下载：http://tomcat.apache.org/ 注意：tomcat服务有些功能会和主机名有关。所以，当修改主机名的时候，要把（主机名 127.0.0.1）映射进hosts文件。 2.2 部署java环境jdk123456789101112131415161718#jdk的解压安装[root@localhost ~]# tar xf jdk-8u60-linux-x64.tar.gz -C /usr/local/[root@localhost ~]# ln -s /usr/local/jdk1.8.0_60/ /usr/local/jdk#配置java环境变量[root@localhost ~]# sed -i.ori &#x27;$a export JAVA_HOME=/usr/local/jdk\\nexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH\\nexport CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar&#x27; /etc/profile[root@localhost ~]# tail -3 /etc/profileexport JAVA_HOME=/usr/local/jdkexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATHexport CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar[root@localhost ~]# source /etc/profile #让java环境变量立刻生效[root@localhost ~]# which java/usr/local/jdk/bin/java[root@localhost ~]# java -version #查看版本java version &quot;1.8.0_60&quot;Java(TM) SE Runtime Environment (build 1.8.0_60-b27)Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode) 说明：sed -i.ori :-i表示对文件本身操作，.ori表示修改的同时备份源文件a：表示文件内容的最后一行，a表示在下面进行数据插入\\n :表示插入数据时换行 2.3 安装Tomcat123456789[root@localhost ~]# tar xf apache-tomcat-8.0.27.tar.gz -C /usr/local/[root@localhost ~]# ln -s /usr/local/apache-tomcat-8.0.27/ /usr/local/tomcat#配置Tomcat环境变量[root@localhost ~]# echo &#x27;export TOMCAT_HOME=/usr/local/tomcat&#x27; &gt;&gt; /etc/profile[root@localhost ~]# source /etc/profile#对jdk及Tomcat安装目录递归授权root[root@localhost ~]# chown -R root.root /usr/local/jdk/ /usr/local/tomcat/ 2.4 Tomcat目录介绍123456789101112131415161718192021222324[root@localhost tomcat]# cd /usr/local/tomcat/[root@localhost tomcat]# tree -L 1 #-L 1：代表只显示第一级目录.├── bin #用以启动，关闭Tomcat或者其他功能的脚本（.bat文件和.sh文件）├── conf #用以配置Tomcat的XML及DTD文件├── lib #存放web应用能访问的JAR包├── LICENSE├── logs #Catalina和其他Web应用程序的日志文件├── NOTICE├── RELEASE-NOTES├── RUNNING.txt├── temp #临时文件├── webapps #Web网页根目录└── work #用以产生有JSP编译出的Servlet的.java和.class文件7 directories, 4 files[root@localhost tomcat]# cd webapps/[root@localhost webapps]# lltotal 20drwxr-xr-x. 14 root root 4096 Oct 24 09:07 docs #tomcat帮助文档drwxr-xr-x. 6 root root 4096 Oct 24 09:07 examples #web应用实例drwxr-xr-x. 5 root root 4096 Oct 24 09:07 host-manager #管理drwxr-xr-x. 5 root root 4096 Oct 24 09:07 manager #管理drwxr-xr-x. 3 root root 4096 Oct 24 09:07 ROOT #默认网站根目录 2.5 启动Tomcat启动程序：/usr/local/tomcat/bin/startup.sh关闭程序：/usr/local/tomcat/bin/shutdown.sh 1234567891011[root@localhost webapps]# /usr/local/tomcat/bin/startup.sh #程序启动Using CATALINA_BASE: /usr/local/tomcat #检查环境变量CATALINA_BASEUsing CATALINA_HOME: /usr/local/tomcat #检查环境变量CATALINA_HOMEUsing CATALINA_TMPDIR: /usr/local/tomcat/temp #检查环境变量CATALINA_TMPDIRUsing JRE_HOME: /usr/local/jdk #检查环境变量JRE_HOMEUsing CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jarTomcat started.[root@localhost webapps]# netstat -antup | grep javatcp 0 0 :::8080 :::* LISTEN 1203/java tcp 0 0 ::ffff:127.0.0.1:8005 :::* LISTEN 1203/java tcp 0 0 :::8009 :::* LISTEN 1203/java 2.6 访问网站 网址为服务器ip：192.168.131.132:8080（访问时请注意关闭iptables） 1234567891011121314151617181920212223242526272829303132333435363738394041#查看日志[root@localhost webapps]# cat /usr/local/tomcat/logs/catalina.out 01-Sep-2018 19:45:02.874 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version: Apache Tomcat/8.0.2701-Sep-2018 19:45:02.882 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server built: Sep 28 2015 08:17:25 UTC01-Sep-2018 19:45:02.882 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server number: 8.0.27.001-Sep-2018 19:45:02.882 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log OS Name: Linux01-Sep-2018 19:45:02.882 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log OS Version: 2.6.32-431.el6.x86_6401-Sep-2018 19:45:02.883 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Architecture: amd6401-Sep-2018 19:45:02.883 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Java Home: /usr/local/jdk1.8.0_60/jre01-Sep-2018 19:45:02.884 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log JVM Version: 1.8.0_60-b2701-Sep-2018 19:45:02.884 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log JVM Vendor: Oracle Corporation01-Sep-2018 19:45:02.884 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log CATALINA_BASE: /usr/local/apache-tomcat-8.0.2701-Sep-2018 19:45:02.884 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log CATALINA_HOME: /usr/local/apache-tomcat-8.0.2701-Sep-2018 19:45:02.886 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties01-Sep-2018 19:45:02.886 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager01-Sep-2018 19:45:02.887 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.endorsed.dirs=/usr/local/tomcat/endorsed01-Sep-2018 19:45:02.887 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Dcatalina.base=/usr/local/tomcat01-Sep-2018 19:45:02.891 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Dcatalina.home=/usr/local/tomcat01-Sep-2018 19:45:02.891 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.io.tmpdir=/usr/local/tomcat/temp01-Sep-2018 19:45:02.891 INFO [main] org.apache.catalina.core.AprLifecycleListener.lifecycleEvent The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: /usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib01-Sep-2018 19:45:03.446 INFO [main] org.apache.coyote.AbstractProtocol.init Initializing ProtocolHandler [&quot;http-nio-8080&quot;]01-Sep-2018 19:45:03.480 INFO [main] org.apache.tomcat.util.net.NioSelectorPool.getSharedSelector Using a shared selector for servlet write/read01-Sep-2018 19:45:03.489 INFO [main] org.apache.coyote.AbstractProtocol.init Initializing ProtocolHandler [&quot;ajp-nio-8009&quot;]01-Sep-2018 19:45:03.493 INFO [main] org.apache.tomcat.util.net.NioSelectorPool.getSharedSelector Using a shared selector for servlet write/read01-Sep-2018 19:45:03.496 INFO [main] org.apache.catalina.startup.Catalina.load Initialization processed in 1541 ms01-Sep-2018 19:45:03.559 INFO [main] org.apache.catalina.core.StandardService.startInternal Starting service Catalina01-Sep-2018 19:45:03.559 INFO [main] org.apache.catalina.core.StandardEngine.startInternal Starting Servlet Engine: Apache Tomcat/8.0.2701-Sep-2018 19:45:03.583 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deploying web application directory /usr/local/apache-tomcat-8.0.27/webapps/manager01-Sep-2018 19:45:04.155 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory /usr/local/apache-tomcat-8.0.27/webapps/manager has finished in 572 ms01-Sep-2018 19:45:04.155 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deploying web application directory /usr/local/apache-tomcat-8.0.27/webapps/docs01-Sep-2018 19:45:04.192 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory /usr/local/apache-tomcat-8.0.27/webapps/docs has finished in 37 ms01-Sep-2018 19:45:04.195 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deploying web application directory /usr/local/apache-tomcat-8.0.27/webapps/examples01-Sep-2018 19:45:04.923 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory /usr/local/apache-tomcat-8.0.27/webapps/examples has finished in 728 ms01-Sep-2018 19:45:04.924 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deploying web application directory /usr/local/apache-tomcat-8.0.27/webapps/host-manager01-Sep-2018 19:45:05.134 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory /usr/local/apache-tomcat-8.0.27/webapps/host-manager has finished in 209 ms01-Sep-2018 19:45:05.134 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deploying web application directory /usr/local/apache-tomcat-8.0.27/webapps/ROOT01-Sep-2018 19:45:05.288 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory /usr/local/apache-tomcat-8.0.27/webapps/ROOT has finished in 154 ms01-Sep-2018 19:45:05.291 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler [&quot;http-nio-8080&quot;]01-Sep-2018 19:45:05.310 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler [&quot;ajp-nio-8009&quot;]01-Sep-2018 19:45:05.316 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in 1819 ms 三，Tomcat的配置文件3.1 Tomcat配置文件123456789101112[root@localhost logs]# cd /usr/local/tomcat/conf/[root@localhost conf]# ll -htotal 216Kdrwxr-xr-x. 3 root root 4.0K Oct 24 09:20 Catalina-rw-------. 1 root root 13K Sep 28 2015 catalina.policy-rw-------. 1 root root 7.0K Sep 28 2015 catalina.properties-rw-------. 1 root root 1.6K Sep 28 2015 context.xml-rw-------. 1 root root 3.4K Sep 28 2015 logging.properties-rw-------. 1 root root 6.4K Sep 28 2015 server.xml #主配置文件-rw-------. 1 root root 1.8K Sep 28 2015 tomcat-users.xml #Tomcat管理用户配置文件-rw-------. 1 root root 1.9K Sep 28 2015 tomcat-users.xsd-rw-------. 1 root root 164K Sep 28 2015 web.xml 3.2 Tomcat管理 测试功能，生产环境不要用：Tomcat管理功能用于对Tomcat自身以及部署在Tomcat上的应用进行管理的Web应用。在默认情况下是处于禁用状态的。如果需要开启这个功能，就需要配置管理用户，即配置前面说过的tomcat-users.xml。 1234567891011121314151617181920212223#找到配置文件的第38行[root@localhost conf]# cat -n /usr/local/tomcat/conf/tomcat-users.xml | sed -n &#x27;38p&#x27; 38 &lt;/tomcat-users&gt;#在38行上加入如下三行代码[root@localhost conf]# tail -4 /usr/local/tomcat/conf/tomcat-users.xml&lt;role rolename=&quot;manager-gui&quot;/&gt; #加入此行&lt;role rolename=&quot;admin-gui&quot;/&gt; #加入此行&lt;user username=&quot;tomcat&quot; password=&quot;tomcat&quot; roles=&quot;manager-gui,admin-gui&quot;/&gt; #加入此行&lt;/tomcat-users&gt;#重启tomcat服务[root@localhost conf]# /usr/local/tomcat/bin/shutdown.sh Using CATALINA_BASE: /usr/local/tomcatUsing CATALINA_HOME: /usr/local/tomcatUsing CATALINA_TMPDIR: /usr/local/tomcat/tempUsing JRE_HOME: /usr/local/jdkUsing CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar[root@localhost conf]# /usr/local/tomcat/bin/startup.sh Using CATALINA_BASE: /usr/local/tomcatUsing CATALINA_HOME: /usr/local/tomcatUsing CATALINA_TMPDIR: /usr/local/tomcat/tempUsing JRE_HOME: /usr/local/jdkUsing CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jarTomcat started. 在浏览器里输入http://192.168.131.132:8080/manager/status进行TOMCAT管理页面 登陆验证信息：账号：tomcat 密码：tomcat 3.3 Tomcat主配置文件Server.xml3.3.1 Server.xml组件类别 顶级组件：位于整个配置的顶层，如server。 容器类组件：可以包含其他组件的组件，如service，engine，host，context 连接器组件：连接用户请求至tomcat，如connector。 被嵌套类组件：位于一个容器当中，不能包含其他组件，如Valve，logger。 12345678910111213&lt;server&gt; &lt;service&gt; &lt;connector /&gt; &lt;engine&gt; &lt;host&gt; &lt;context&gt;&lt;/context&gt; &lt;/host&gt; &lt;host&gt; &lt;context&gt;&lt;/context&gt; &lt;/host&gt; &lt;/engine&gt; &lt;/service&gt;&lt;/server&gt; 3.3.2 组件详解 engine:核心容器组件，catalina引擎，负责通过connector接收用户请求，并处理请求，将请求转至对应的虚拟主机host。 host：类似于httpd中的虚拟主机，一般而言支持基于FQDN的虚拟主机。 context：定义一个应用程序，是一个最内层的容器类组件（不能再嵌套）。配置context的主要目的指定对应对的webapp的根目录，类似于httpd的alias，其还能为webapp指定额外的属性，如部署方式等。 connector：接收用户请求，类似于httpd的listen配置监听端口。 service（服务）：将connector关联至engine，因此一个service内部可以有多个connector，但只能又一个引擎engine。service内部有两个connector，一个engine。因此，一般情况下一个server内部只有一个service，一个service内部只有一个engine，但一个service内部可以有多个connector。 server：表示一个运行于JVM中的tomcat实例。 Valve：阀门，拦截请求并在将其转至对应的webapp前进行某种处理操作，可以用于任何容器中，比如记录日志（access log valve），基于IP做访问控制（remote address filer valve）。 logger：日志记录器，用于记录组件内部的状态信息，可以用于除context外的任何容器中。 realm：可以用于任意容器类的组件中，关联一个用户认证库，实现认证和授权。可以关联的认证库有两种：UserDatabaseRealm，MemoryRealm和JDBCRealm。 UserDatabaseRealm：使用JNDI自定义的用户认证库。 MemoryRealm：认证信息定义在tomcat-users.xml中。 JDBCRealm：认证信息定义在数据库中，并通过JDBC连接至数据库中查找认证用户。 ####3.3.3 配置文件注释 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version=&#x27;1.0&#x27; encoding=&#x27;utf-8&#x27;?&gt;&lt;!--&lt;Server&gt;元素代表整个容器,是Tomcat实例的顶层元素.由org.apache.catalina.Server接口来定义.它包含一个&lt;Service&gt;元素.并且它不能做为任何元素的子元素. port指定Tomcat监听shutdown命令端口.终止服务器运行时,必须在Tomcat服务器所在的机器上发出shutdown命令.该属性是必须的. shutdown指定终止Tomcat服务器运行时,发给Tomcat服务器的shutdown监听端口的字符串.该属性必须设置--&gt;&lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt; &lt;Listener className=&quot;org.apache.catalina.startup.VersionLoggerListener&quot; /&gt; &lt;Listener className=&quot;org.apache.catalina.core.AprLifecycleListener&quot; SSLEngine=&quot;on&quot; /&gt; &lt;Listener className=&quot;org.apache.catalina.core.JreMemoryLeakPreventionListener&quot; /&gt; &lt;Listener className=&quot;org.apache.catalina.mbeans.GlobalResourcesLifecycleListener&quot; /&gt; &lt;Listener className=&quot;org.apache.catalina.core.ThreadLocalLeakPreventionListener&quot; /&gt; &lt;GlobalNamingResources&gt; &lt;Resource name=&quot;UserDatabase&quot; auth=&quot;Container&quot; type=&quot;org.apache.catalina.UserDatabase&quot; description=&quot;User database that can be updated and saved&quot; factory=&quot;org.apache.catalina.users.MemoryUserDatabaseFactory&quot; pathname=&quot;conf/tomcat-users.xml&quot; /&gt; &lt;/GlobalNamingResources&gt; &lt;!--service服务组件--&gt; &lt;Service name=&quot;Catalina&quot;&gt; &lt;!-- connector：接收用户请求，类似于httpd的listen配置监听端口. port指定服务器端要创建的端口号，并在这个端口监听来自客户端的请求。 address：指定连接器监听的地址，默认为所有地址（即0.0.0.0） protocol连接器使用的协议，支持HTTP和AJP。AJP（Apache Jserv Protocol）专用于tomcat与apache建立通信的， 在httpd反向代理用户请求至tomcat时使用（可见Nginx反向代理时不可用AJP协议）。 minProcessors服务器启动时创建的处理请求的线程数 maxProcessors最大可以创建的处理请求的线程数 enableLookups如果为true，则可以通过调用request.getRemoteHost()进行DNS查询来得到远程客户端的实际主机名，若为false则不进行DNS查询，而是返回其ip地址 redirectPort指定服务器正在处理http请求时收到了一个SSL传输请求后重定向的端口号 acceptCount指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理 connectionTimeout指定超时的时间数(以毫秒为单位) --&gt; &lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; &lt;Connector port=&quot;8009&quot; protocol=&quot;AJP/1.3&quot; redirectPort=&quot;8443&quot; /&gt; &lt;!--engine,核心容器组件,catalina引擎,负责通过connector接收用户请求,并处理请求,将请求转至对应的虚拟主机host defaultHost指定缺省的处理请求的主机名，它至少与其中的一个host元素的name属性值是一样的 --&gt; &lt;Engine name=&quot;Catalina&quot; defaultHost=&quot;localhost&quot;&gt; &lt;!--Realm表示存放用户名，密码及role的数据库--&gt; &lt;Realm className=&quot;org.apache.catalina.realm.LockOutRealm&quot;&gt; &lt;Realm className=&quot;org.apache.catalina.realm.UserDatabaseRealm&quot; resourceName=&quot;UserDatabase&quot;/&gt; &lt;/Realm&gt; &lt;!-- host表示一个虚拟主机 name指定主机名 appBase应用程序基本目录，即存放应用程序的目录.一般为appBase=&quot;webapps&quot; ，相对于CATALINA_HOME而言的，也可以写绝对路径。 unpackWARs如果为true，则tomcat会自动将WAR文件解压，否则不解压，直接从WAR文件中运行应用程序 autoDeploy：在tomcat启动时，是否自动部署。 xmlValidation：是否启动xml的校验功能，一般xmlValidation=&quot;false&quot;。 xmlNamespaceAware：检测名称空间，一般xmlNamespaceAware=&quot;false&quot;。 --&gt; &lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;&gt; &lt;!-- Context表示一个web应用程序，通常为WAR文件 docBase应用程序的路径或者是WAR文件存放的路径,也可以使用相对路径，起始路径为此Context所属Host中appBase定义的路径。 path表示此web应用程序的url的前缀，这样请求的url为http://localhost:8080/path/**** reloadable这个属性非常重要，如果为true，则tomcat会自动检测应用程序的/WEB-INF/lib 和/WEB-INF/classes目录的变化，自动装载新的应用程序，可以在不重启tomcat的情况下改变应用程序 --&gt; &lt;Context path=&quot;&quot; docBase=&quot;&quot; debug=&quot;&quot;/&gt; &lt;Valve className=&quot;org.apache.catalina.valves.AccessLogValve&quot; directory=&quot;logs&quot; prefix=&quot;localhost_access_log&quot; suffix=&quot;.txt&quot; pattern=&quot;%h %l %u %t &amp;quot;%r&amp;quot; %s %b&quot; /&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt;&lt;/Server&gt; 四，WEB站点部署 上线的代码有两种方式，第一种方式是直接将程序目录放在webapps目录下面。第二种方式是使用开发工具将程序打包成war包，然后上传到webapps目录下面。 4.1 使用war包部署web站点第二种代码上线方式： 123456789101112131415161718[root@localhost ~]# cp memtest.war /usr/local/tomcat/webapps/[root@localhost ~]# cp jpress-web-newest.war /usr/local/tomcat/webapps/[root@localhost ~]# /usr/local/tomcat/bin/shutdown.sh Using CATALINA_BASE: /usr/local/tomcatUsing CATALINA_HOME: /usr/local/tomcatUsing CATALINA_TMPDIR: /usr/local/tomcat/tempUsing JRE_HOME: /usr/local/jdkUsing CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar[root@localhost ~]# /usr/local/tomcat/bin/startup.sh Using CATALINA_BASE: /usr/local/tomcatUsing CATALINA_HOME: /usr/local/tomcatUsing CATALINA_TMPDIR: /usr/local/tomcat/tempUsing JRE_HOME: /usr/local/jdkUsing CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jarTomcat started.[root@localhost ~]# ls /usr/local/tomcat/webapps/docs examples host-manager jpress-web-newest jpress-web-newest.war manager memtest memtest.war ROOT 用浏览器访问：http://192.168.131.132:8080/memtest/meminfo.jsp如下： 4.2 自定义默认网站目录上面访问的网址为：http://192.168.131.132:8080/memtest/meminfo.jsp现在我想访问格式为：http://192.168.131.132:8080/meminfo.jsp 方法一： 将meminfo.jsp或其他程序放在tomcat&#x2F;webapps&#x2F;ROOT目录下即可。因为默认网站根目录为tomcat&#x2F;webapps&#x2F;ROOT 方法二： 123456[root@tomcat ~]# vim /application/tomcat/conf/server.xml &lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;&gt; &lt;Context path=&quot;&quot; docBase=&quot;/usr/local/tomcat/webapps/memtest&quot; debug=&quot;0&quot; reloadable=&quot;false&quot; crossContext=&quot;true&quot;/&gt; #在虚拟主机这里添加一行代码限定web站点的根目录路径[root@tomcat ~]# /application/tomcat/bin/shutdown.sh[root@tomcat ~]# /application/tomcat/bin/startup.sh 五，Tomcat多实例及集群架构5.1 Tomcat多实例5.1.1 复制Tomcat目录123456789[root@localhost ~]# cd /usr/local/[root@localhost local]# lsapache-tomcat-8.0.27 bin etc games include jdk jdk1.8.0_60 lib lib64 libexec sbin share src tomcat[root@localhost local]# cp -a apache-tomcat-8.0.27/ tomcat8_1[root@localhost local]# cp -a apache-tomcat-8.0.27/ tomcat8_2[root@localhost local]# lsapache-tomcat-8.0.27 etc include jdk1.8.0_60 lib64 sbin src tomcat8_1bin games jdk lib libexec share tomcat tomcat8_2 5.1.2 修改多实例配置文件123456789101112131415161718192021#创建多实例的网页根目录[root@localhost local]# mkdir -p /data/www/www/ROOT#将网页程序拷贝到，多实例根目录ROOT下[root@localhost local]# cp /usr/local/tomcat/webapps/memtest/meminfo.jsp /data/www/www/ROOT/#修改多实例配置文件的以下三行[root@localhost local]# cat -n /usr/local/tomcat/conf/server.xml | sed -n &#x27;22p;69p;123p&#x27; 22 &lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt; #管理端口及停止命令 69 &lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; #对外提供服务的端口 123 &lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; #网站域名及网页根目录路径#修改第一个多实例配置文件[root@localhost local]# sed -i &#x27;22s#8005#8011#;69s#8080#8081#;123s#appBase=&quot;.*&quot;#appBase=&quot;/data/www/www&quot;#&#x27; /usr/local/tomcat8_1/conf/server.xml [root@localhost local]# sed -n &#x27;22p;69p;123p&#x27; /usr/local/tomcat8_1/conf/server.xml&lt;Server port=&quot;8011&quot; shutdown=&quot;SHUTDOWN&quot;&gt; &lt;Connector port=&quot;8081&quot; protocol=&quot;HTTP/1.1&quot; &lt;Host name=&quot;localhost&quot; appBase=&quot;/data/www/www&quot;#修改第二个多实例配置文件[root@localhost local]# sed -i &#x27;22s#8005#8012#;69s#8080#8082#;123s#appBase=&quot;.*&quot;#appBase=&quot;/data/www/www&quot;#&#x27; /usr/local/tomcat8_2/conf/server.xml [root@localhost local]# sed -n &#x27;22p;69p;123p&#x27; /usr/local/tomcat8_2/conf/server.xml&lt;Server port=&quot;8012&quot; shutdown=&quot;SHUTDOWN&quot;&gt; &lt;Connector port=&quot;8082&quot; protocol=&quot;HTTP/1.1&quot; &lt;Host name=&quot;localhost&quot; appBase=&quot;/data/www/www&quot; 5.1.3 启动多实例12345678910111213141516171819202122232425#启动多实例[root@localhost local]# /usr/local/tomcat8_1/bin/startup.sh Using CATALINA_BASE: /usr/local/tomcat8_1Using CATALINA_HOME: /usr/local/tomcat8_1Using CATALINA_TMPDIR: /usr/local/tomcat8_1/tempUsing JRE_HOME: /usr/local/jdkUsing CLASSPATH: /usr/local/tomcat8_1/bin/bootstrap.jar:/usr/local/tomcat8_1/bin/tomcat-juli.jarTomcat started.[root@localhost local]# /usr/local/tomcat8_2/bin/startup.sh Using CATALINA_BASE: /usr/local/tomcat8_2Using CATALINA_HOME: /usr/local/tomcat8_2Using CATALINA_TMPDIR: /usr/local/tomcat8_2/tempUsing JRE_HOME: /usr/local/jdkUsing CLASSPATH: /usr/local/tomcat8_2/bin/bootstrap.jar:/usr/local/tomcat8_2/bin/tomcat-juli.jarTomcat started.#查看多实例进程启动情况[root@localhost local]# netstat -antup | grep javatcp 0 0 ::ffff:127.0.0.1:8011 :::* LISTEN 1671/java tcp 0 0 ::ffff:127.0.0.1:8012 :::* LISTEN 1697/java tcp 0 0 :::8080 :::* LISTEN 1434/java tcp 0 0 :::8081 :::* LISTEN 1671/java tcp 0 0 :::8082 :::* LISTEN 1697/java tcp 0 0 ::ffff:127.0.0.1:8005 :::* LISTEN 1434/java tcp 0 0 :::8009 :::* LISTEN 1434/java 浏览器可以分别访问http://192.168.131.132:8081/meminfo.jsp和http://192.168.131.132:8082/meminfo.jsp 5.2 Tomcat集群 使用nginx+Tomcat反向代理集群 5.2.1 安装nginx1234567[root@localhost ~]# yum -y install pcre-devel openssl-devel[root@localhost ~]# wget -q http://nginx.org/download/nginx-1.10.2.tar.gz[root@localhost ~]# useradd -s /sbin/nologin -M nginx[root@localhost ~]# tar xf nginx-1.10.2.tar.gz -C /usr/src/[root@localhost ~]# cd /usr/src/nginx-1.10.2/[root@localhost nginx-1.10.2]# ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module[root@localhost nginx-1.10.2]# make &amp;&amp; make install 5.2.2 修改nginx配置文件如下12345678910111213141516171819202122232425262728293031323334#创建配置文件模版[root@localhost nginx]# egrep -v &quot;#|^$&quot; /usr/local/nginx/conf/nginx.conf.default &gt; /usr/local/nginx/conf/nginx.conf#修改配置文件内容如下：[root@localhost nginx]# cat /usr/local/nginx/conf/nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; upstream web_pools &#123; server 127.0.0.1:8081; server 127.0.0.1:8082; &#125; server &#123; listen 80; server_name localhost; location / &#123; root html; index index.jsp index.html index.htm; proxy_pass http://web_pools; &#125; &#125;&#125;#检测语法并启动nginx[root@localhost nginx]# /usr/local/nginx/sbin/nginx -tnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful[root@localhost nginx]# /usr/local/nginx/sbin/nginx[root@localhost nginx]# netstat -antup | grep nginxtcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 4833/nginx 5.3 使用Tomcat安装Jpress JPress，一个wordpress的java代替版本，使用JFinal开发。需要maven支持 12345678910111213[root@localhost ~]# tar xf apache-maven-3.3.9-bin.tar.gz -C /usr/local/[root@localhost ~]# ln -s /usr/local/apache-maven-3.3.9 /usr/local/maven[root@localhost ~]# tail -2 /etc/profileexport MAVEN_HOME=/usr/local/mavenexport PATH=&quot;$MAVEN_HOME/bin:$PATH&quot;[root@localhost ~]# source /etc/profile[root@localhost ~]# mvn -version #出现这个表示成功Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-10T11:41:47-05:00)Maven home: /usr/local/mavenJava version: 1.8.0_60, vendor: Oracle CorporationJava home: /usr/local/jdk1.8.0_60/jreDefault locale: en_US, platform encoding: UTF-8OS name: &quot;linux&quot;, version: &quot;2.6.32-431.el6.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot; 将 jpress-web-newest.war包放到Tomcat网站根目录下 12345678910[root@localhost ~]# mv jpress-web-newest.war /data/www/www/ROOT/ #将war包放到网站根目录下#解压war包[root@localhost ~]# which jar/usr/local/jdk/bin/jar[root@localhost ~]# cd /data/www/www/ROOT/[root@localhost ROOT]# jar xf jpress-web-newest.war #jar是war包的解压命令[root@localhost ROOT]# lsjpress-web-newest.war meminfo.jsp META-INF robots.txt static templates WEB-INF 用浏览器访问：http://192.168.131.132:8080/install进入jpress安装向导 六，Tomcat安全优化和性能优化6.1 安全优化 最重要的优化为如下4项，但并不止这四种 降权启动 telnet管理端口保护 ajp连接端口保护 禁用管理端 具体操作如下： （1）降权启动（同nginx优化部分的监牢模式） 降权的原则就是利用普通用户来启动Tomcat（1）将Tomcat程序目录拷贝到普通用户家目录下（2）修改家目录下程序的配置文件（启动端口，检测端口等），并重新指定网页根目录路径。（3）递归授权拷贝后的Tomcat程序的属主属组为普通用户。（4）用su命令切换为普通用户，启动Tomcat进程（5）此时Tomcat进程的权限为普通用户权限（6）如果利用&#x2F;etc&#x2F;rc.local文件配置普通用户程序的开机启动，那么需要利用su -c临时切换身份启动。 （2）telnet管理端口保护 1234567891011[root@localhost ~]# sed -n &#x27;22p&#x27; /usr/local/tomcat/conf/server.xml&lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt; #表示通过8005端口来接受SHUTDOWN，用来停止Tomcat进程。默认的方式是非常危险的。需要进行修改[root@localhost ~]# netstat -antup | grep javatcp 0 0 ::ffff:127.0.0.1:8011 :::* LISTEN 2295/java #本地8011端口接收SHUTDOWN命令tcp 0 0 ::ffff:127.0.0.1:8012 :::* LISTEN 2321/java #本地8012端口接收SHUTDOWN命令tcp 0 0 :::8080 :::* LISTEN 2031/java tcp 0 0 :::8081 :::* LISTEN 2295/java tcp 0 0 :::8082 :::* LISTEN 2321/java tcp 0 0 ::ffff:127.0.0.1:8005 :::* LISTEN 2031/java #本地8005端口接收SHUTDOWN命令tcp 0 0 :::8009 :::* LISTEN 2031/java Tomcat默认通过8005端口来接收SHUTDOWN这个字符串来关闭Tomcat进程，但这是非常危险的，因此需要修改端口号来防护。否则，通过telnet命令即可强行关闭Tomcat进程 12345678910111213#利用Telnet来关闭Tomcat进程[root@localhost ~]# telnet 127.0.0.1 8005 #通过telnet连接本地8005端口Trying 127.0.0.1...Connected to 127.0.0.1.Escape character is &#x27;^]&#x27;.SHUTDOWN #发送SHUTDOWN字符串Connection closed by foreign host.[root@localhost ~]# netstat -antup | grep java #可以发现8005端口和8080端口的Tomcat进程没了tcp 0 0 ::ffff:127.0.0.1:8011 :::* LISTEN 2295/java tcp 0 0 ::ffff:127.0.0.1:8012 :::* LISTEN 2321/java tcp 0 0 :::8081 :::* LISTEN 2295/java tcp 0 0 :::8082 :::* LISTEN 2321/java [root@localhost ~]# （3）ajp连接端口保护 12[root@localhost ~]# sed -n &#x27;91p&#x27; /usr/local/tomcat/conf/server.xml &lt;Connector port=&quot;8009&quot; protocol=&quot;AJP/1.3&quot; redirectPort=&quot;8443&quot; /&gt; #这是AJP协议打开的端口，我们并不需要开启这个端口，因此注释掉本行 （4）禁用管理端 Tomcat默认在安装完成后的网页目录里有很多多余的目录，删除所有不需要用到的目录，并清空ROOT网页默认根目录下的所有东西，规避可能的代码漏洞 123456[root@localhost ~]# cd /usr/local/tomcat/webapps/[root@localhost webapps]# lsdocs examples host-manager manager memtest memtest.war ROOT #有很多多余的东西，只留下ROOT目录，其他都删掉或者mv移走[root@localhost webapps]# ls ROOT/ #很多多余的东西，因此清空本目录，或者都移走asf-logo.png bg-button.png bg-nav-item.png bg-upper.png favicon.ico meminfo.jsp tomcat.css tomcat.png tomcat.svgasf-logo-wide.gif bg-middle.png bg-nav.png build.xml index.jsp RELEASE-NOTES.txt tomcat.gif tomcat-power.gif WEB-INF 6.2 性能优化6.2.1 屏蔽DNS查询enableLookups=&quot;false&quot; 1234#默认没有，需添加配置文件如下代码段，在Connector标签位置。表示禁止DNS查询 &lt;Connector port=&quot;8081&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;6000&quot; enableLookups=&quot;false&quot; acceptCount=&quot;800&quot; redirectPort=&quot;8443&quot; /&gt; 6.2.2 jvm调优 Tomcat最吃内存，只要内存足够，这只猫就跑的很快。如果系统资源有限，那就需要进行调优，提高资源使用率。 1234567891011121314151617181920#优化catalina.sh初始化脚本。在catalina.sh初始化脚本中添加以下代码：#catalina.sh的路径为:/usr/local/tomcat/bin/catalina.sh#此行优化代码需要加在脚本的最开始，声明位置。不要放在后边JAVA_OPTS=&quot;-Djava.awt.headless=true -Dfile.encoding=UTF-8 -server -Xms1024m -Xmx1024m -XX:NewSize=512m -XX:MaxNewSize=512m -XX:PermSize=512m -XX:MaxPermSize=512m&quot;#代码说明：server:一定要作为第一个参数，在多个CPU时性能佳-Xms：初始堆内存Heap大小，使用的最小内存,cpu性能高时此值应设的大一些-Xmx：初始堆内存heap最大值，使用的最大内存上面两个值是分配JVM的最小和最大内存，取决于硬件物理内存的大小，建议均设为物理内存的一半。-XX:PermSize:设定内存的永久保存区域-XX:MaxPermSize:设定最大内存的永久保存区域-XX:MaxNewSize:-Xss 15120 这使得JBoss每增加一个线程（thread)就会立即消耗15M内存，而最佳值应该是128K,默认值好像是512k.+XX:AggressiveHeap 会使得 Xms没有意义。这个参数让jvm忽略Xmx参数,疯狂地吃完一个G物理内存,再吃尽一个G的swap。-Xss：每个线程的Stack大小-verbose:gc 现实垃圾收集信息-Xloggc:gc.log 指定垃圾收集日志文件-Xmn：young generation的heap大小，一般设置为Xmx的3、4分之一-XX:+UseParNewGC ：缩短minor收集的时间-XX:+UseConcMarkSweepGC ：缩短major收集的时间 jvm拓展：http://www.cnblogs.com/xingzc/p/5756119.html 案例：Linux下java&#x2F;http进程高解决案例 生产环境下某台tomcat7服务器，在刚发布的时候一切都很正常，在运行一段时间后就出现CPU占用很高的问题，基本上是负载一天比一天高。诸如此类问题，请排查！ 问题分析： （1）程序属于CPU密集型，和开发沟通过，排除此类情况（2）程序代码有问题，出现死循环，可能性极大 问题解决： （1）开发那边无法排查代码某个模块有问题，从日志上也无法分析得出（2）我们可以尝试通过jstack命令来精确定位出现错误的代码段，从而拿给开发排查。 （1）首先查找进程高的PID号(先找到是哪个PID号的进程导致的) top -H （2）查看这个进程所有系统调用（再找到是哪个PID号的线程导致的） strace -p 进程的PID （3）如果是Web应用，可以继续打印该线程的堆栈信息（找出有问题的代码块） printf &quot;%x\\n&quot; 线程的PID #将有问题的线程的PID号转换成16进制格式 jstack 进程的PID | grep 线程PID号的十六进制格式 -A 30 #过滤出有问题的线程的堆栈信息，找出问题代码块，-A 30 是继续向后显示30行、-B 30 是向前显示30行、-C 30是上下个各15行 操作演示： 123456789101112131415161718192021222324252627282930[root@localhost ROOT]# pgrep -l java2031 java #java进程及对应PID号2295 java2321 java[root@localhost ROOT]# strace -p 2031 #查看PID号为2031的java进程的所有线程调用情况Process 2031 attached - interrupt to quitfutex(0x7f4cdd0e79d0, FUTEX_WAIT, 2032, NULL #只有一个线程，线程的PID号为2032&lt;unfinished ...&gt;Process 2031 detached[root@localhost ROOT]# printf &quot;%x\\n&quot; 2032 #将线程的PID号2032转换成十六进制格式7f0[root@localhost ROOT]# jstack 2031 | grep 7f0 -A 30 #追踪进称号为2031的进程的所有线程调用，从里面过滤出16进制为7f0的线程的代码调用情况&quot;main&quot; #1 prio=5 os_prio=0 tid=0x00007f4cd4008800 nid=0x7f0 runnable [0x00007f4cdd0e5000] java.lang.Thread.State: RUNNABLE at java.net.PlainSocketImpl.socketAccept(Native Method) at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) #圆括号里显示的是（代码类名:具体调用的代码行号） at java.net.ServerSocket.implAccept(ServerSocket.java:545) at java.net.ServerSocket.accept(ServerSocket.java:513) at org.apache.catalina.core.StandardServer.await(StandardServer.java:446) at org.apache.catalina.startup.Catalina.await(Catalina.java:713) at org.apache.catalina.startup.Catalina.start(Catalina.java:659) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:351) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:485)&quot;VM Thread&quot; os_prio=0 tid=0x00007f4cd406d000 nid=0x7f1 runnable &quot;VM Periodic Task Thread&quot; os_prio=0 tid=0x00007f4cd40b8800 nid=0x7f8 waiting on condition JNI global references: 244 ##jstack命令（Java stack Trace） （1）介绍 jstack用于打印出给定的java进程ID或core file或远程调试服务的Java堆栈信息，如果是在64位机器上，需要指定选项”-J-d64”，Windows的jstack使用方式只支持以下的这种方式：jstack [-l] pid 如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。另外，jstack工具还可以附属到正在运行的java程序中，看到当时运行的java程序的java stack和native stack的信息, 如果现在运行的java程序呈现hung的状态，jstack是非常有用的。 （2）命令格式 jstack [ option ] pidjstack [ option ] executable corejstack [ option ] [server-id@]remote-hostname-or-IP （3）常用参数说明 1)、options： executable Java executable from which the core dump was produced.(可能是产生core dump的java可执行程序)core 将被打印信息的core dump文件remote-hostname-or-IP 远程debug服务的主机名或ipserver-id 唯一id,假如一台主机上多个远程debug服务 2）、基本参数： -F：当’jstack [-l] pid’没有相应的时候强制打印栈信息-l：长列表. 打印关于锁的附加信息,例如属于java.util.concurrent的ownable synchronizers列表.-m：打印java和native c&#x2F;c++框架的所有栈信息.-h | -help：打印帮助信息pid ：需要被打印配置信息的java进程id,可以用jps查询.","categories":[{"name":"架构基础","slug":"架构基础","permalink":"https://kkabuzs.github.io/categories/%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Tomcat","slug":"Tomcat","permalink":"https://kkabuzs.github.io/tags/Tomcat/"}]},{"title":"Keepalived高可用集群","slug":"keepalivedgaokeyongjiqun","date":"2018-08-31T07:45:40.000Z","updated":"2018-08-31T07:45:40.000Z","comments":true,"path":"articles/2018/08/31/keepalivedgaokeyongjiqun/","permalink":"https://kkabuzs.github.io/articles/2018/08/31/keepalivedgaokeyongjiqun/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Keepalived高可用集群一，Keepalived高可用软件1.1 Keepalived介绍 Keepalived软件起初是专门为LVS负载均衡软件设计的，用来管理并监控LVS集群系统中各个服务节点的状态，后来又加入了可以实现高可用的VRRP功能。因此，Keepalived除了能够管理LVS软件外，还可以作为其他服务（例如：Nginx，Haproxy，MySQL等）的高可用解决方案软件。 Keepalived软件主要是通过VRRP协议实现高可用功能的。VRRP是Virtual Router Redundancy Protocol（虚拟路由器冗余协议）的缩写，VRRP出现的目的就是为了解决静态路由单点故障问题的，他能够保证当个别节点宕机时，整个网络可以不间断地运行。所以，Keepalived一方面具有配置管理LVS的功能，同时还具有对LVS下面节点进行健康检查的功能，另一方面也可实现系统网络服务的高可用功能。 1.2 Keepalived服务的三个重要功能（1）管理LVS负载均衡软件 （2）实现对LVS集群节点健康检查功能（healthcheck） （3）作为系统网络服务的高可用功能（failover） 1.3 Keepalived高可用故障切换转移原理 Keepalived高可用服务之间的故障切换转移，是通过VRRP（Virtual Router Redundancy Protocol，虚拟路由器冗余协议）来实现的。 在Keepalived服务正常工作时，主Master节点会不断地向备节点发送（多播的方式）心跳消息，用以告诉备Backup节点自己还活着，当主Master节点发生故障时，就无法发送心跳消息，备节点也就因此无法继续检测到来自主Master节点的心跳了，于是调用自身的接管程序，接管主Master节点的IP资源及服务。而当主Master节点恢复时，备Backup节点又会释放主节点故障时自身接管的IP资源及服务，恢复到原来的备用角色。 VRRP协议：VRRP，全称Virtual Router Redundancy Protocol，中文名为虚拟路由冗余协议，VRRP的出现就是为了解决静态路由的单点故障问题，VRRP是通过一种竞选机制来将路由的任务交给某台VRRP路由器的。 二，Keepalived高可用服务搭建准备2.1 安装Keepalived环境 nginx装高可用需要三块网卡，一块连接外网，一块连接内网，还有一块直连keepalive心跳线。 首先添加一块网卡，用于直连心跳线 12345678910111213[root@nginx-master ~]# cd /etc/sysconfig/network-scripts/[root@nginx-master network-scripts]# cp ifcfg-eth0 ifcfg-eth1[root@nginx-master network-scripts]# vim ifcfg-eth1DEVICE=eth1TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=dhcp #因为是仅主机模式#IPADDR=192.168.27.203#NETMASK=255.255.255.0#GATEAWY=192.168.27.1#DNS1=202.106.0.20 2.2 开始安装Keepalived软件1[root@nginx-master ~]# yum -y install keepalived #主备两台都安装 2.3 启动Keepalived服务并检查启动及检查Keepalived服务的命令如下： 12345678910111213141516171819[root@nginx-master ~]# /etc/init.d/keepalived startStarting keepalived: [ OK ][root@nginx-master ~]# ps -ef | grep keep | grep -v greproot 1330 1 0 01:15 ? 00:00:00 /usr/sbin/keepalived -Droot 1332 1330 0 01:15 ? 00:00:00 /usr/sbin/keepalived -Droot 1333 1330 0 01:15 ? 00:00:00 /usr/sbin/keepalived -D#提示：启动后有3个Keepalived进程表示安装正确[root@nginx-master ~]# ip add | grep 192.168 #提示：启动后有3个Keepalived进程表示安装正确 inet 192.168.131.132/24 brd 192.168.131.255 scope global eth0 inet 192.168.200.16/32 scope global eth0 inet 192.168.200.17/32 scope global eth0 inet 192.168.200.18/32 scope global eth0 inet 192.168.88.128/24 brd 192.168.88.255 scope global eth1[root@nginx-master ~]# /etc/init.d/keepalived stopStopping keepalived: [ OK ]#说明：主备两台都需要测试 2.4 Keepalived配置文件说明vim /etc/keepalived/keepalived.conf 这里的具备高可用功能的Keepalived.conf配置文件包含了两个重要区块 （1）全局定义（Global Definitions）部分 这部分主要用来设置Keepalived的故障通知机制和Router ID标识。示例代码如下： 1234567891011121314[root@nginx-master ~]# head -13 /etc/keepalived/keepalived.conf | cat -n 1 ! Configuration File for keepalived 2 3 global_defs &#123; 4 notification_email &#123; 5 acassen@firewall.loc 6 failover@firewall.loc 7 sysadmin@firewall.loc 8 &#125; 9 notification_email_from Alexandre.Cassen@firewall.loc 10 smtp_server 192.168.200.1 11 smtp_connect_timeout 30 12 router_id LVS_DEVEL 13 &#125; 基础参数说明： 第1行是注释，！开头和#号开发一样，都是注释。第2行是空行。第3~8行是定义服务故障报警的Email地址。作用是当服务发生切换或RS节点等有故障时，发报警邮件。这几行是可选配置，notification_email指定在Keepalived发生事件时，需要发送的Email地址，可以有多个，每行一个。第9行是指定发送邮件的发送人，即发件人地址，也是可选的配置。第10行smtp_server指定发送邮件的smtp服务器，如果本机开启了sendmail或postfix，就可以使用上面默认配置实现邮件发送，也是可选配置。第11行smtp_connect_timeout是连接smtp的超时时间，也是可选配置。 注意：第4~11行所有和邮件报警相关的参数均可以不配，在实际工作中会将监控的任务交给更加擅长监控报警的Nagios或Zabbix软件。第12行是Keepalived服务器的路由标识（router_id）.在一个局域网内，这个标识（router_id）应该是唯一的。大括号“{}”。用来分隔区块，要成对出现。如果漏写了半个大括号，Keepalived运行时，不会报错，但也不会得到预期的结果。另外，由于区块间存在多层嵌套关系，因此很容易遗漏区块结尾处的大括号，要特别注意。 （2）VRRP实例定义区块（VRRP instance（s））部分 1234567891011121314151617[root@nginx-master ~]# sed -n &#x27;15,30&#123;=;p&#125;&#x27; /etc/keepalived/keepalived.conf | xargs -L215 vrrp_instance VI_1 &#123;16 state MASTER17 interface eth018 virtual_router_id 5119 priority 10020 advert_int 121 authentication &#123;22 auth_type PASS23 auth_pass 111124 &#125;25 virtual_ipaddress &#123;26 192.168.200.1627 192.168.200.1728 192.168.200.1829 &#125;30 &#125; 参数说明： 第15行表示定义一个vrrp_instance实例，名字是VI_1,每个vrrp_instance实例可以认为是Keepalived服务的一个实例或者作为一个业务服务，在Keepalived服务配置中，这样的vrrp_instance实例可以有多个。注意，存在于主节点中的vrrp_instance实例在备节点中也要存在，这样才能实现故障切换接管。 第16行state MASTER表示当前实例VI_1的角色状态，当前角色为MASTER，这个状态只能有MASTER和BACKUP两种状态，并且需要大写这些字符。其中MASTER为正式工作的状态，BACKUP为备用的状态。当MASTER所在的服务器故障或失效时，BACKUP所在的服务器会接管故障的MASTER继续提供服务。 第17行interface为网络通信接口。为对外提供服务的网络接口，如eth0,eth1。当前主流的服务器都有2~4个网络接口，在选择服务接口时，要搞清楚了。 第18行virtual_router_id为虚拟路由ID标识，这个标识最好是一个数字，并且要在一个keepalived.conf配置中是唯一的。但是MASTER和BACKUP配置中相同实例的virtual_router_id又必须是一致的，否则将出现脑裂问题。 第19行priority为优先级，其后面的数值也是一个数字，数字越大，表示实例优先级越高。在同一个vrrp_instance实例里，MASTER的优先级配置要高于BACKUP的。若MASTER的priority值为150，那么BACKUP的priority必须小于150，一般建议间隔50以上为佳，例如：设置BACKUP的priority为100或更小的数值。 第20行advert_int为同步通知间隔。MASTER与BACKUP之间通信检查的时间间隔，单位为秒，默认为1. 第21~24行authentication为权限认证配置。包含认证类型（auth_type）和认证密码（auth_pass）。认证类型有PASS（Simple Passwd（suggested）），AH（IPSEC（not recommended））两种，官方推荐使用的类型为PASS。验证密码为明文方式，最好长度不要超过8个字符，建议用4位数字，同一vrrp实例的MASTER与BACKUP使用相同的密码才能正常通信。 第25 ~ 29 行virtual_ipaddress为虚拟IP地址。可以配置多个IP地址，每个地址占一行，配置时最好明确指定子网掩码以及虚拟IP绑定的网络接口。否则，子网掩码默认是32位，绑定的接口和前面的interface参数配置的一致。注意，这里的虚拟IP就是在工作中需要和域名绑定的IP，即和配置的高可用服务监听的IP要保持一致！ 三，Keepalived高可用服务单实例3.1 配置Keepalived实现单实例单IP自动漂移接管（1）配置Keepalived主服务器 12345678910111213141516171819202122232425262728[root@nginx-master ~]# vim /etc/keepalived/keepalived.conf#删掉已有的所有默认配置，加入经过修改好的如下配置：! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; 424219678@qq.com #邮箱随便写 &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 #邮件服务器IP smtp_connect_timeout 30 router_id nginx-master #id为nginx-master，不能和其他Keepalived节点相同（全局唯一）&#125;vrrp_instance VI_1 &#123; #实例名字为VI_1,相同实例的备节点名字要和这个相同 state MASTER #状态为MASTER，备节点状态需要为BACKUP interface eth1 #通信（心跳）接口为eth1，此参数备节点设置和主节点相同 virtual_router_id 55 #实例ID为55，要和备节点相同 priority 150 #优先级为150，备节点的优先级必须比此数字低 advert_int 1 #通信检查间隔时间1秒 authentication &#123; auth_type PASS #PASS认证类型，此参数备节点设置和主节点相同 auth_pass 1111 #密码1111，此参数备节点设置和主节点相同 &#125; virtual_ipaddress &#123; 192.168.131.240/24 dev eth0 label eth0:1 #虚拟IP，即VIP为192.168.131.240,子网掩码为24位，绑定接口为eth0，别名为eth0：1，此参数备节点设置和主节点相同 &#125;&#125; 配置完毕后，启动Keepalived服务 12345678910111213141516171819202122232425262728293031323334[root@nginx-master ~]# /etc/init.d/keepalived startStarting keepalived: [ OK ][root@nginx-master ~]# ifconfig eth0 Link encap:Ethernet HWaddr 00:0C:29:D5:71:25 inet addr:192.168.131.132 Bcast:192.168.131.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fed5:7125/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:7970 errors:0 dropped:0 overruns:0 frame:0 TX packets:2302 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:574674 (561.2 KiB) TX bytes:312605 (305.2 KiB)eth0:1 Link encap:Ethernet HWaddr 00:0C:29:D5:71:25 #成功了 inet addr:192.168.131.240 Bcast:0.0.0.0 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1eth1 Link encap:Ethernet HWaddr 00:0C:29:D5:71:2F inet addr:192.168.88.128 Bcast:192.168.88.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fed5:712f/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:55 errors:0 dropped:0 overruns:0 frame:0 TX packets:50 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:7316 (7.1 KiB) TX bytes:4848 (4.7 KiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 b) TX bytes:0 (0.0 b) （2）实战配置Keepalived备服务器lb02 BACKUP 123456789101112131415161718192021222324252627[root@nginx-slave ~]# vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; 424219678@qq.com &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id nginx-slave #此参数和nginx-master MASTER不同&#125;vrrp_instance VI_1 &#123; #此参数和nginx-master MASTER同 state BACKUP #此参数和nginx-master MASTER不同 interface eth1 #此参数和nginx-master MASTER同 virtual_router_id 55 #此参数和nginx-master MASTER同 priority 100 #此参数和nginx-master MASTER不同 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.131.240/24 dev eth0 label eth0:1 &#125;&#125; 配置完成后，启动Keepalived服务 123[root@nginx-slave ~]# /etc/init.d/keepalived startStarting keepalived: [ OK ] （3）进行高可用主备服务器切换实验 1234567891011121314151617[root@nginx-master ~]# ifconfig eth0:1eth0:1 Link encap:Ethernet HWaddr 00:0C:29:D5:71:25 inet addr:192.168.131.240 Bcast:0.0.0.0 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1[root@nginx-master ~]# /etc/init.d/keepalived stopStopping keepalived: [ OK ][root@nginx-master ~]# ifconfig eth0:1eth0:1 Link encap:Ethernet HWaddr 00:0C:29:D5:71:25 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1#这时候看slave服务器[root@nginx-slave ~]# ifconfig eth0:1 #漂移成功eth0:1 Link encap:Ethernet HWaddr 00:0C:29:B3:7E:B5 inet addr:192.168.131.240 Bcast:0.0.0.0 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 主节点启动Keepalived服务后，发现很快就又接管了VIP 192.168.131.240 1234567[root@nginx-master ~]# /etc/init.d/keepalived startStarting keepalived: [ OK ][root@nginx-master ~]# ifconfig eth0:1eth0:1 Link encap:Ethernet HWaddr 00:0C:29:D5:71:25 inet addr:192.168.131.240 Bcast:0.0.0.0 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 五，Keepalived双实例双主模式配置首先，配置nginx-master的Keepalived.conf，在单实例的基础上增加一个vrrp_instance VI_2实例 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@nginx-master ~]# vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; 424219678@qq.com &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id nginx-master&#125;vrrp_instance VI_1 &#123; state MASTER interface eth1 virtual_router_id 55 priority 150 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.131.240/24 dev eth0 label eth0:1 &#125;vrrp_instance VI_2 &#123; state BACKUP interface eth1 virtual_router_id 56 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.131.250/24 dev eth0 label eth0:2 &#125;&#125;#提示：以vrrp_instance VI_1在master服务器上的角色为主，vrrp_instance VI_2在slave服务器上的角色为备。 然后配置nginx-slave的Keepalived.conf，在单实例的基础上增加vrrp_instance VI_2实例 123456789101112131415161718192021222324252627282930313233343536373839[root@nginx-slave ~]# vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; 424219678@qq.com &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id nginx-slave&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth1 virtual_router_id 55 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.131.240/24 dev eth0 label eth0:1 &#125;vrrp_instance VI_2 &#123; state MASTER interface eth1 virtual_router_id 56 priority 150 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.131.250/24 dev eth0 label eth0:2&#125; 重启服务(两台)示例一台： 1234567891011121314151617181920212223242526272829303132333435363738394041[root@nginx-master ~]# /etc/init.d/keepalived restartStopping keepalived: [ OK ]Starting keepalived: [ OK ]#关闭slave服务器的keepalive服务，查看master服务器[root@nginx-master ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:0C:29:D5:71:25 inet addr:192.168.131.132 Bcast:192.168.131.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fed5:7125/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8916 errors:0 dropped:0 overruns:0 frame:0 TX packets:2574 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:642080 (627.0 KiB) TX bytes:348713 (340.5 KiB)eth0:1 Link encap:Ethernet HWaddr 00:0C:29:D5:71:25 inet addr:192.168.131.240 Bcast:0.0.0.0 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1eth0:2 Link encap:Ethernet HWaddr 00:0C:29:D5:71:25 #漂移成功 inet addr:192.168.131.250 Bcast:0.0.0.0 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1eth1 Link encap:Ethernet HWaddr 00:0C:29:D5:71:2F inet addr:192.168.88.128 Bcast:192.168.88.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fed5:712f/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:166 errors:0 dropped:0 overruns:0 frame:0 TX packets:1039 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:15552 (15.1 KiB) TX bytes:59022 (57.6 KiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 b) TX bytes:0 (0.0 b) 四，Keepalived高可用服务器的“裂脑”问题4.1 什么是裂脑 由于某些原因，导致两台高可用服务器对在指定时间内，无法检测到对方的心跳消息，各自取得资源及服务的所有权，而此时的两台高可用服务器对都还活着并在正常运行，这样就会导致同一个IP或服务在两端同时存在而发生冲突，最严重的是两台主机占用同一个VIP地址，当用户写入数据时可能会分别写入到两端，这可能会导致服务器两端的数据不一致或造成数据丢失，这种情况就被称为裂脑。 4.2，导致裂脑发生的原因一般来说，裂脑的发生，有以下几种原因： 高可用服务器对之间心跳线链路发生故障，导致无法正常通信。 心跳线坏了（包括断了,老化） 网卡及相关驱动坏了，IP配置及冲突问题（网卡直连）。 心跳线间连接的设备故障（网卡及交换机） 仲裁的机器出问题（采用仲裁的方案） 高可用服务器上开启了iptables防火墙阻挡了心跳消息传输 高可用服务器上心跳网卡地址等信息配置不正确，导致发送心跳失败。 其他服务配置不当等原因，如心跳方式不同，心跳广播冲突，软件BUG等 提示：Keepalived配置里同一VRRP实例如果virtual_router_id两端参数配置不一致，也会导致裂脑问题发生。 4.3 解决裂脑的常见方案在实际生产环境中，我们可以从以下几个方面来防止裂脑问题的发生： 同时使用串行电缆和以太网电缆连接，同时用两条心跳线路，这样一条线路坏了，另一个还是好的，依然能传送心跳消息。 当检测到裂脑时强行关闭一个心跳节点（这个功能需特殊设备支持，如Stonith，fence）。相当于备节点接收不到心跳消息，通过单独的线路发送关机命令关闭主节点的电源。 做好对裂脑的监控报警（如邮件及手机短信等或值班），在问题发生时人为第一时间介入仲裁，降低损失。例如，百度的监控报警短信就有上行和下行的区别。报警信息发送到管理员手机上，管理员可以通过手机回复对应数字或简单的字符串操作返回给服务器，让服务器根据指令自动处理相应故障，这样解决故障的时间更短。 当然，在实施高可用方案时，要根据业务实际需求确定是否能容忍这样的损失。对于一般的网站常规业务，这个损失是可容忍的。 4.4 解决Keepalived裂脑的常见方案 作为互联网应用服务器的高可用，特别是前端Web负载均衡器的高可用，裂脑的问题对普通业务的影响是可以忍受的，如果是数据库或者存储的业务，一般出现裂脑问题就非常严重了。因此，可以通过增加冗余心跳线路来避免裂脑问题的发生，同时加强对系统的监控，以便裂脑发生时人为快速介入解决问题。 如果开启防火墙，一定要让心跳消息通过，一般通过允许IP段的形式解决。 可以拉一条以太网网线或者串口线作为主被节点心跳线路的冗余。 开发检测程序通过监控软件（例如Nagios）检测裂脑。 生产场景检测裂脑故障的一些思路： 1)简单判断的思想：只要备节点出现VIP就报警，这个报警有两种情况，一是主机宕机了备机接管了；二是主机没宕，裂脑了。不管属于哪个情况，都进行报警，然后由人工查看判断及解决。 2）比较严谨的判断：备节点出现对应VIP，并且主节点及对应服务（如果能远程连接主节点看是否有VIP就更好了）还活着，就说明发生裂脑了。 五，开发检测Keepalived裂脑的脚本1）在nginx-slave备节点开发脚本并执行 12345678910111213141516171819[root@nginx-slave scripts]# cat check_split_brain.sh #!/bin/bashnginx_master_vip=192.168.131.240nginx_aster_ip=192.168.131.132while truedo ping -c 2 -W 3 $nginx_aster_ip &amp;&gt;/dev/null if [ $? -eq 0 -a `ip a | grep &quot;$nginx_master_vip&quot; | wc -l` -eq 1 ];then echo &quot;ha is split brain.warning.&quot; else echo &quot;ha is OK&quot; fi sleep 5done[root@nginx-slave scripts]# sh check_split_brain.sh ha is OKha is OKha is OK#正常情况下，主节点活着，VIP 192.168.131.132在主节点，因此不会报警，提示“ha is OK” 2）停止Keepalived服务看nginx-slave脚本执行情况。 nginx-master上： 123456789101112[root@nginx-master scripts]# /etc/init.d/keepalived stopStopping keepalived: [ OK ][root@nginx-master scripts]# ip a | grep 192.168.0.240[root@nginx-master scripts]# #在nginx-slave上观察即可，此前脚本已经执行。[root@nginx-slave scripts]# sh check_split_brain.sh ha is OKha is OKha is OKha is split brain.warning.ha is split brain.warning.ha is split brain.warning. 3）关掉nginx-master服务器，然后再观察nginx-slave脚本的输出。 1234567891011[root@nginx-slave scripts]# sh check_split_brain.sh ha is OKha is OKha is OKha is split brain.warning.ha is split brain.warning.ha is split brain.warning.ha is OKha is OKha is OK#裂脑报警恢复了。 六，解决高可用服务只针对物理服务器的问题第一种（主服务器）12345678910111213141516171819202122232425262728[root@lb01 scripts]# cat check_nginx.sh#!/bin/shwhile truedo if [ `netstat -antup | grep nginx | wc -l` -ne 1 ];then /etc/init.d/keepalived stop fi sleep 5done#此脚本的基本思想是若没有80端口存在，就停掉Keepalived服务实现释放本地的VIP。在后台执行上述脚本并检查：[root@lb01 scripts]# sh check_nginx.sh &amp;[1] 1521[root@lb01 scripts]# ps -ef | grep check | grep -v greproot 1521 1195 0 10:49 pts/0 00:00:00 sh check_nginx.sh#确认Nginx以及Keepalived服务是正常的[root@lb01 scripts]# netstat -antup | grep nginxtcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 1492/nginx [root@lb01 scripts]# /etc/init.d/keepalived statuskeepalived (pid 1512) is running...#然后模拟Nginx服务挂掉，看IP是否发生切换。[root@lb01 scripts]# /usr/local/nginx/sbin/nginx -s stop[root@lb01 scripts]# Stopping keepalived: [ OK ][root@lb01 scripts]# /etc/init.d/keepalived statuskeepalived is stopped[root@lb01 scripts]# netstat -antup | grep nginx#此时，备节点已接管：[root@lb02 ~]# ip a | grep 192.168.0.240 inet 192.168.0.240/24 scope global secondary eth0:1 第二种（主服务器）12345678[root@lb01 scripts]# cat chk_nginx_proxy.sh #!/bin/bashif [ `netstat -antup | grep nginx | wc -l` -ne 1 ];then /etc/init.d/keepalived stopfi[root@lb01 scripts]# chmod +x chk_nginx_proxy.sh [root@lb01 scripts]# ls -l chk_nginx_proxy.sh -rwxr-xr-x. 1 root root 102 Jul 31 10:59 chk_nginx_proxy.sh 此时，Keepalived服务的完整配置为： 123456789101112131415161718192021222324252627282930313233[root@lb01 scripts]# cat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; 215379068@qq.com &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id lb01&#125;vrrp_script chk_nginx_proxy &#123; #定义vrrp脚本，检测HTTP端口 script &quot;/server/scripts/chk_nginx_proxy.sh&quot; #执行脚本，当Nginx服务有问题，就停掉Keepalived服务 interval 2 #间隔2秒 weight 2&#125;vrrp_instance VI_1 &#123; state MASTER interface eth1 virtual_router_id 55 priority 150 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.0.240/24 dev eth0 label eth0:1 &#125; track_script &#123; chk_nginx_proxy #触发检查 &#125;&#125;","categories":[{"name":"架构基础","slug":"架构基础","permalink":"https://kkabuzs.github.io/categories/%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Keepalived","slug":"Keepalived","permalink":"https://kkabuzs.github.io/tags/Keepalived/"}]},{"title":"Ansible批量自动化管理工具","slug":"ansiblepiliangzidonghuaguanligongju","date":"2018-08-28T04:28:54.000Z","updated":"2018-08-28T04:28:54.000Z","comments":true,"path":"articles/2018/08/28/ansiblepiliangzidonghuaguanligongju/","permalink":"https://kkabuzs.github.io/articles/2018/08/28/ansiblepiliangzidonghuaguanligongju/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Ansible批量自动化管理工具一， ansible简介 批量管理服务器的工具，无需部署agent，通过ssh进行管理 jenkins简介 可视化运维（主要用在可视化部署）持续构建，可以和git，svn结合可结合ssh实现可视化运维可结合ansible实现可视化运维 二，Python3与ansible的安装2.1 使用源码安装Python3.51234567891011[root@localhost ~]# yum -y install lrzsz vim net-tools gcc gcc-c++ ncurses ncurses-devel unzip zlib-devel zlib openssl-devel openssl #安装支持包[root@localhost ~]# tar xf Python-3.5.2.tgz -C /usr/src/[root@localhost Python-3.5.2]# cd /usr/src/Python-3.5.2/[root@localhost Python-3.5.2]# ./configure --prefix=/usr/local/python[root@localhost Python-3.5.2]# make &amp;&amp; make install[root@localhost Python-3.5.2]# ln -s /usr/local/python/bin/python3 /usr/bin/python3[root@localhost Python-3.5.2]# which python3/usr/bin/python3[root@localhost Python-3.5.2]# python3 -VPython 3.5.2 2.2 使用pip3安装ansible123456789101112131415#安装ansible最新版本[root@localhost Python-3.5.2]# /usr/local/python/bin/pip3 install ansible等待安装...[root@localhost Python-3.5.2]# ln -s /usr/local/python/bin/ansible /usr/local/bin[root@localhost Python-3.5.2]# which ansible/usr/local/bin/ansible[root@localhost Python-3.5.2]# ansible --versionansible 2.6.3 config file = None configured module search path = [&#x27;/root/.ansible/plugins/modules&#x27;, &#x27;/usr/share/ansible/plugins/modules&#x27;] ansible python module location = /usr/local/python/lib/python3.5/site-packages/ansible executable location = /usr/local/bin/ansible python version = 3.5.2 (default, Aug 29 2018, 05:04:47) [GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] 2.3 ansible查看帮助[root@localhost ~]# /usr/local/python/bin/ansible-doc -l 查看总帮助[root@localhost ~]# /usr/local/python/bin/ansible-doc -s shell 查看shell模块的帮助[root@localhost ~]# /usr/local/python/bin/ansible-doc -s raw 三，使用公私钥实现ssh无密码登陆 host IP ansible 192.168.131.109 webA 192.168.131.132 webB 192.168.131.110 1234567891011121314151617181920212223242526272829303132333435[root@localhost ~]# yum -y install openssh-clients #安装ssh包[root@localhost ~]# ssh-keygen -t rsa -f ~/.ssh/id_rsa -P &quot;&quot; #生成密钥对Generating public/private rsa key pair.Created directory &#x27;/root/.ssh&#x27;.Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:d8:e8:42:7c:48:dc:84:5c:01:11:63:57:a7:09:10:9a root@localhost.localdomainThe key&#x27;s randomart image is:+--[ RSA 2048]----+| .OO*o. . || =o= . + || E o . o || o . + || + + S || . o || . . || . || |+-----------------+[root@localhost ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub &quot;-o StrictHostKeyChecking=no&quot; 192.168.131.132 #自动分发密钥Warning: Permanently added &#x27;192.168.131.132&#x27; (RSA) to the list of known hosts.root@192.168.131.132&#x27;s password: Now try logging into the machine, with &quot;ssh &#x27;-o StrictHostKeyChecking=no 192.168.131.132&#x27;&quot;, and check in: .ssh/authorized_keysto make sure we haven&#x27;t added extra keys that you weren&#x27;t expecting.[root@localhost ~]# ssh 192.168.131.132 #访问测试Last login: Thu Aug 30 04:58:46 2018 from 192.168.131.1[root@localhost ~]# hostname -I #访问成功192.168.131.132 四，ansible的简单配置和ping模块4.1 ansible的配置文件 过pip安装的ansible是没有配置文件的,需要手动创建 123456789101112[root@localhost ~]# mkdir -p /etc/ansible [root@localhost ~]# vim /etc/ansible/hosts #编辑配置文件[nginx] #被管理的主机组名称webA ansible_ssh_host=192.168.131.132 ansible_ssh_port=22 ansible_ssh_user=root #第一台主机webB ansible_ssh_host=192.168.131.110 ansible_ssh_port=22 ansible_ssh_user=root ansible_ssh_pass=123456 #第二台主机特别提示：WebA ===&gt; 主机名ansible_ssh_host ===&gt;主机IPansible_ssh_port ===&gt;ssh的默认端口ansible_ssh_user ===&gt;ssh的用户名ansible_ssh_pass ===&gt;ssh的用户的连接密码 如果已经设置了ssh免密钥了。那么就不需要写密码了。例如：webA要是没有设置免密钥，那么就需要安装sshpass工具，并在&#x2F;etc&#x2F;ansible&#x2F;hosts文件里写上主机的连接密码。例如webB 1234567891011#下载epel源安装sshpass[root@localhost ~]# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.repo[root@localhost ~]# yum -y clean allLoaded plugins: fastestmirrorCleaning repos: c6-media epelCleaning up EverythingCleaning up list of fastest mirrors[root@localhost ~]# yum makecache[root@localhost ~]# yum -y install sshpass[root@localhost ~]# which sshpass/usr/bin/sshpass 4.2 进行ansible远程执行命令测试语法： ansible all -m command -a &#39;uptime&#39; ansible 主机组 -m ansible内置功能模块名 -a 命令 进行命令测试： 1234567891011121314[root@localhost ~]# ansible nginx -m pingparamiko: The authenticity of host &#x27;192.168.131.110&#x27; can&#x27;t be established.The ssh-rsa key fingerprint is b&#x27;0900a44abeb172d26fb5fc3501a30483&#x27;.Are you sure you want to continue connecting (yes/no)? #首次连接，需要交互webA | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;yeswebB | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125; 4.3 ansible的简单使用方式ansible -i /etc/ansible/hosts 主机或主机组 -m 指定模块 -a 命令 不用-i指定配置文件默认为&#x2F;etc&#x2F;ansible&#x2F;hosts 4.4 使用ping模块用来查看服务器是否连接正常，ping模块不需要-a指定参数ansible all -m ping 主机组，主机，all代表所有 主机和主机组注意事项： 主机组范围 解释 all 代表所有主机 webA:webB 可以指定多台主机 all:\\!webA 指定all但不包含webB，注意”!”前需要加转义符号”&quot; 1234567891011121314151617181920212223242526[root@localhost ~]# ansible webA -m ping #测试webA | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;[root@localhost ~]# ansible all -m pingwebA | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;webB | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;[root@localhost ~]# ansible all:\\!webA -m pingwebB | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;[root@localhost ~]# ansible webA:webB -m command -a &#x27;uptime&#x27;webA | SUCCESS | rc=0 &gt;&gt; 06:06:05 up 1:09, 3 users, load average: 0.00, 0.00, 0.00webB | SUCCESS | rc=0 &gt;&gt; 06:06:07 up 1:08, 3 users, load average: 0.00, 0.00, 0.00 五，ansible的命令模块5.1 ansible模块command（不支持管道，不支持重定向）不建议使用 1234567#command支持直接回显命令的执行结果[root@localhost ~]# ansible all -m command -a &#x27;pwd&#x27;webB | SUCCESS | rc=0 &gt;&gt;/rootwebA | SUCCESS | rc=0 &gt;&gt;/root 5.2 ansible模块shell（支持管道，支持重定向）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@localhost ~]# ansible all -m shell -a &#x27;echo bb &gt;&gt; /tmp/zhaoshuo&#x27;webA | SUCCESS | rc=0 &gt;&gt;webB | SUCCESS | rc=0 &gt;&gt;[root@localhost ~]# ansible all -m shell -a &#x27;echo zhaoshuo | grep a&#x27;webB | SUCCESS | rc=0 &gt;&gt;zhaoshuowebA | SUCCESS | rc=0 &gt;&gt;zhaoshuo#如果遇到特殊符号需要加入\\转义，这样子ansible才能正常运行[root@localhost ~]# ansible all -m shell -a &quot;cat /etc/passwd | awk -F &quot;:&quot; &#x27;&#123;print \\$1&#125;&#x27;&quot;webB | SUCCESS | rc=0 &gt;&gt;rootbindaemonadmlpsyncshutdownhaltmailuucpoperatorgamesgopherftpnobodyvcsasaslauthpostfixsshdwebA | SUCCESS | rc=0 &gt;&gt;rootbindaemonadmlpsyncshutdownhaltmailuucpoperatorgamesgopherftpnobodyvcsasaslauthpostfixsshd 5.3 ansible模块raw，最原始的方式运行命令（不依赖python，仅通过ssh实现）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@localhost ~]# ansible all -m raw -a &quot;yum -y clean all&quot;webA | SUCCESS | rc=0 &gt;&gt;Loaded plugins: fastestmirrorCleaning repos: c6-mediaCleaning up EverythingCleaning up list of fastest mirrorswebB | SUCCESS | rc=0 &gt;&gt;Loaded plugins: fastestmirrorCleaning repos: c6-mediaCleaning up EverythingCleaning up list of fastest mirrors[root@localhost ~]# ansible all -m raw -a &quot;yum makecache&quot;webB | SUCCESS | rc=0 &gt;&gt;Loaded plugins: fastestmirrorDetermining fastest mirrors * c6-media: file:///media/CentOS/repodata/repomd.xml: [Errno 14] Could not open/read file:///media/CentOS/repodata/repomd.xmlTrying other mirror.file:///media/cdrecorder/repodata/repomd.xml: [Errno 14] Could not open/read file:///media/cdrecorder/repodata/repomd.xmlTrying other mirror.c6-media | 4.0 kB 00:00 ... c6-media/group_gz | 220 kB 00:00 ... c6-media/filelists_db | 5.8 MB 00:00 ... c6-media/primary_db | 4.4 MB 00:00 ... c6-media/other_db | 2.7 MB 00:00 ... Metadata Cache CreatedwebA | SUCCESS | rc=0 &gt;&gt;Loaded plugins: fastestmirrorDetermining fastest mirrors * c6-media: file:///media/CentOS/repodata/repomd.xml: [Errno 14] Could not open/read file:///media/CentOS/repodata/repomd.xmlTrying other mirror.file:///media/cdrecorder/repodata/repomd.xml: [Errno 14] Could not open/read file:///media/cdrecorder/repodata/repomd.xmlTrying other mirror.c6-media | 4.0 kB 00:00 ... c6-media/group_gz | 220 kB 00:00 ... c6-media/filelists_db | 5.8 MB 00:00 ... c6-media/primary_db | 4.4 MB 00:00 ... c6-media/other_db | 2.7 MB 00:00 ... Metadata Cache Created 5.4 yum模块 利用yum模块安装软件包，虽然能被shell模块替代但是用yum模块更显专业一些 软件包名： name：指定软件包的名字 state状态： present：安装软件包（默认就是这个） absent：卸载软件包 1234#安装nmap软件包[root@ansible ~]# ansible all -m yum -a &#x27;name=nmap&#x27;#卸载nmap软件包[root@ansible ~]# ansible all -m yum -a &#x27;name=nmap state=absent&#x27; 六，ansible的copy模块批量下发文件或文件夹6.1 copy模块概述 copy模块的参数,ansible 主机组 -m 模块 -a 命令 src：指定源文件或目录 dest：指定目标服务器的文件或目录 backup：是否要备份 owner：拷贝到目标服务器后，文件或目录的所属用户 group：拷贝到目标服务器后，文件或目录的所属群组 mode：文件或目录的权限 说明：若出现错误，说明节点未安装libselinux-python支持包。 准备工作: 12345[root@localhost ~]# mkdir -p /service/scripts[root@localhost ~]# echo &quot;aaa&quot; &gt; /service/scripts/test.txt[root@localhost ~]# echo &quot;bbb&quot; &gt; /service/scripts/test2.txt所有被管理端节点必须安装libselinux-python包yum -y install libselinux-python 6.2 copy模块拷贝文件 提示：如果目标路径不存在会自动创建 1234567891011121314151617181920212223242526272829303132[root@localhost ~]# ansible all -m copy -a &quot;src=/service/scripts/text.txt dest=/service/scripts/&quot;webA | SUCCESS =&gt; &#123; &quot;changed&quot;: true, &quot;checksum&quot;: &quot;972a1a11f19934401291cc99117ec614933374ce&quot;, &quot;dest&quot;: &quot;/service/scripts/text.txt&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;5c9597f3c8245907ea71a89d9d39d08e&quot;, &quot;mode&quot;: &quot;0644&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;secontext&quot;: &quot;system_u:object_r:svc_svc_t:s0&quot;, &quot;size&quot;: 4, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1535636459.4482992-11332265234244/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0&#125;webB | SUCCESS =&gt; &#123; &quot;changed&quot;: true, &quot;checksum&quot;: &quot;972a1a11f19934401291cc99117ec614933374ce&quot;, &quot;dest&quot;: &quot;/service/scripts/text.txt&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;5c9597f3c8245907ea71a89d9d39d08e&quot;, &quot;mode&quot;: &quot;0644&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;secontext&quot;: &quot;system_u:object_r:svc_svc_t:s0&quot;, &quot;size&quot;: 4, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1535636459.4617953-50994191944169/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0&#125; 6.3 copy模块拷贝文件夹123456789101112131415#拷贝/service/scripts/ 目录下所有内容到dest的路径下（注意两条命令的对比）[root@localhost ~]# ansible webA -m copy -a &quot;src=/service/scripts/ dest=/service/scripts/&quot;webA | SUCCESS =&gt; &#123; &quot;changed&quot;: true, &quot;dest&quot;: &quot;/service/scripts/&quot;, &quot;src&quot;: &quot;/service/scripts/&quot;&#125;#拷贝/service/scripts目录本身及其内部的所有内容到dest的路径下[root@localhost ~]# ansible webA -m copy -a &quot;src=/service/scripts dest=/service/scripts/&quot;webA | SUCCESS =&gt; &#123; &quot;changed&quot;: true, &quot;dest&quot;: &quot;/service/scripts/&quot;, &quot;src&quot;: &quot;/service/scripts&quot;&#125; 6.4 copy模块自动备份 说明：backup&#x3D;yes 意思是，如果目标路径下，有与我同名但不同内容的文件时，在覆盖前，对目标文件先进行备份。 1234567[root@localhost ~]# ansible webB -m copy -a &quot;src=/service/scripts/ dest=/service/scripts/ backup=yes&quot;webB | SUCCESS =&gt; &#123; &quot;changed&quot;: true, &quot;dest&quot;: &quot;/service/scripts/&quot;, &quot;src&quot;: &quot;/service/scripts/&quot;&#125; 6.5 copy模块指定用户和属主12345678910111213[root@localhost ~]# ansible webA -m copy -a &quot;src=/service/scripts/ dest=/service/scripts/ owner=nobody group=nobody mode=0600&quot;webA | SUCCESS =&gt; &#123; &quot;changed&quot;: true, &quot;dest&quot;: &quot;/service/scripts/&quot;, &quot;src&quot;: &quot;/service/scripts/&quot;&#125;webA服务器：[root@localhost ~]# ll /service/scripts/total 12-rw-------. 1 nobody nobody 4 Aug 30 21:43 text2.txt-rw-------. 1 nobody nobody 4 Aug 30 21:41 text.txt 七，ansible的script模块批量运行脚本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@localhost scripts]# pwd/service/scripts[root@localhost scripts]# vim auto_nginx.sh #nginx安装脚本#!/bin/bash#nginx install shell scriptstest -d /media/cdrom || mkdir -p /media/cdrommount /dev/sr0 /media/cdrom &amp;&gt;/dev/nullyum -y install gcc gcc-c++ make pcre pcre-devel zlib zlib-devel openssl openssl-devel &amp;&gt;/dev/nulltest -d /service/scripts || exit 3cd /service/scripts/tar xf nginx-1.10.2.tar.gz -C /usr/src/cd /usr/src/nginx-1.10.2/./configure --prefix=/usr/local/nginx --with-http_ssl_module --with-http_stub_status_module &amp;&gt;/dev/nullmake &amp;&gt;/dev/nullmake install &amp;&gt;/dev/nullexit 0[root@localhost scripts]# vim fenfa.sh #源码包和安装脚本的批量分发脚本#!/bin/sh#批量分发脚本Group=$1ansible $Group -m copy -a &quot;src=/service/scripts/ dest=/service/scripts/ mode=0755&quot;ansible $Group -m script -a &quot;/bin/bash /service/scripts/auto_nginx.sh&quot;[root@localhost scripts]# lsauto_nginx.sh fenfa.sh nginx-1.10.2.tar.gz[root@localhost scripts]# sh fenfa.sh all #激活脚本webA | SUCCESS =&gt; &#123; &quot;changed&quot;: true, &quot;dest&quot;: &quot;/service/scripts/&quot;, &quot;src&quot;: &quot;/service/scripts/&quot;&#125;webB | SUCCESS =&gt; &#123; &quot;changed&quot;: true, &quot;dest&quot;: &quot;/service/scripts/&quot;, &quot;src&quot;: &quot;/service/scripts/&quot;&#125;webA | SUCCESS =&gt; &#123; &quot;changed&quot;: true, &quot;rc&quot;: 0, &quot;stderr&quot;: &quot;&quot;, &quot;stderr_lines&quot;: [], &quot;stdout&quot;: &quot;&quot;, &quot;stdout_lines&quot;: []&#125;webB | SUCCESS =&gt; &#123; &quot;changed&quot;: true, &quot;rc&quot;: 0, &quot;stderr&quot;: &quot;&quot;, &quot;stderr_lines&quot;: [], &quot;stdout&quot;: &quot;&quot;, &quot;stdout_lines&quot;: []&#125; 八，ansible-playbook的初步使用 playbook的使用，playbook可以把ansible的模块进行组合 先做软链接ln -s /usr/local/python/bin/ansible-playbook /usr/local/bin/ 8.1 playbook的简单shell模块的使用123456789101112131415161718[root@localhost scripts]# vim test_shell.yaml #playbook的执行模板--- #开头三个小-开头- hosts: webB tasks: - name: test shell: echo &quot;welcome to yunjisuan&quot; &gt;&gt; /tmp/username - name: test2 shell: echo &quot;welcome to yunjisuan&quot; &gt;&gt; /tmp/username 模板说明：--- #开头必须有三个小-，顶格写- hosts： #正文配置代码的第一级，必须有两个空格（-占一个空格位）- host: webB #webB是host参数的值，值和hosts：之间要有一个空格 tasks: #tasks：表示接下来要执行的具体任务 - name: #相对于tasks再多缩进两个格（-占一个空格位），表示属于tasks的下一级 - name: test #test只是要执行的具体命令的名字可以随便写。name:后还是有一个空格要注意 shell: #表示调用shell模块执行命令相对于tasks仍旧要多缩进两个空格 shell: echo &quot;xxx&quot; &gt;&gt; xxx #shell:后边还是要有个空格，需要注意。 执行playbook配置文件 123456789101112131415[root@localhost scripts]# ansible-playbook test_shell.yaml #执行playbook配置文件PLAY [webB] ************************************************************************************************************TASK [Gathering Facts] *************************************************************************************************ok: [webB]TASK [test] ************************************************************************************************************changed: [webB]TASK [test2] ***********************************************************************************************************changed: [webB]PLAY RECAP *************************************************************************************************************webB : ok=3 changed=2 unreachable=0 failed=0 8.2 playbook的简单copy模块的使用123456789101112131415161718192021222324[root@localhost scripts]# echo &quot;welcome&quot; &gt;&gt; /tmp/test_copy[root@localhost scripts]# vim test_copy.yaml---- hosts: all tasks: - name: test copy copy: src=/tmp/test_copy dest=/tmp [root@localhost scripts]# ansible-playbook /service/scripts/test_copy.yaml #执行文件PLAY [all] *************************************************************************************************************TASK [Gathering Facts] *************************************************************************************************ok: [webB]ok: [webA]TASK [test copy] *******************************************************************************************************changed: [webA]changed: [webB]PLAY RECAP *************************************************************************************************************webA : ok=2 changed=1 unreachable=0 failed=0 webB : ok=2 changed=1 unreachable=0 failed=0 8.3 playbook使用register输出命令运行结果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[root@localhost scripts]# vim test_register.yaml---- hosts: all tasks: - name: test register shell: echo &quot;welcome to yunjisuan&quot; register: print_result #将之前命令的输出结果保存在变量print_result里（变量随便写） - debug: var=print_result #将变量的值作为debug输出出来[root@localhost scripts]# ansible-playbook test_register.yaml PLAY [all] *************************************************************************************************************TASK [Gathering Facts] *************************************************************************************************ok: [webB]ok: [webA]TASK [test register] ***************************************************************************************************changed: [webA]changed: [webB]TASK [debug] ***********************************************************************************************************ok: [webA] =&gt; &#123; #命令的执行结果有输出了 &quot;print_result&quot;: &#123; &quot;changed&quot;: true, &quot;cmd&quot;: &quot;echo \\&quot;welcome to yunjisuan\\&quot;&quot;, &quot;delta&quot;: &quot;0:00:00.002454&quot;, &quot;end&quot;: &quot;2018-08-31 00:30:03.681237&quot;, &quot;failed&quot;: false, &quot;rc&quot;: 0, &quot;start&quot;: &quot;2018-08-31 00:30:03.678783&quot;, &quot;stderr&quot;: &quot;&quot;, &quot;stderr_lines&quot;: [], &quot;stdout&quot;: &quot;welcome to yunjisuan&quot;, &quot;stdout_lines&quot;: [ &quot;welcome to yunjisuan&quot; ] &#125;&#125;ok: [webB] =&gt; &#123; &quot;print_result&quot;: &#123; &quot;changed&quot;: true, &quot;cmd&quot;: &quot;echo \\&quot;welcome to yunjisuan\\&quot;&quot;, &quot;delta&quot;: &quot;0:00:00.002346&quot;, &quot;end&quot;: &quot;2018-08-31 00:30:04.162016&quot;, &quot;failed&quot;: false, &quot;rc&quot;: 0, &quot;start&quot;: &quot;2018-08-31 00:30:04.159670&quot;, &quot;stderr&quot;: &quot;&quot;, &quot;stderr_lines&quot;: [], &quot;stdout&quot;: &quot;welcome to yunjisuan&quot;, &quot;stdout_lines&quot;: [ &quot;welcome to yunjisuan&quot; ] &#125;&#125;PLAY RECAP *************************************************************************************************************webA : ok=3 changed=1 unreachable=0 failed=0 webB : ok=3 changed=1 unreachable=0 failed=0 8.4 nginx配置下发并检测12345678910[root@localhost scripts]# cat test_nginx_conf.yaml ---- hosts: all tasks: - name: copy nginx.conf copy: src=/tmp/nginx.conf dest=/usr/local/nginx/conf/ backup=yes - name: shell: /usr/local/nginx/sbin/nginx -t register: nginx_result - debug: var=nginx_result 九，playbook的自定义变量和内置变量9.1 在Playbook中使用自定义变量123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[root@localhost scripts]# vim test_vars.yaml---- hosts: all vars: #定义变量 - Name: &quot;yunjisuan&quot; #第一个Name变量 age: &quot;3&quot; #第二个age变量 tasks: - name: &quot;&#123;&#123; Name &#125;&#125;&quot; #&#123;&#123;&#125;&#125;两对大括号引用变量，变量名两头空格 shell: echo &quot;myname &#123;&#123; Name &#125;&#125;,myage &#123;&#123; age &#125;&#125;&quot; register: var_result - debug: var=var_result特别提示：引用变量需要在双引号中引用。[root@localhost scripts]# ansible-playbook test_vars.yamlPLAY [all] *************************************************************************************************************TASK [Gathering Facts] *************************************************************************************************ok: [webB]ok: [webA]TASK [yunjisuan] *******************************************************************************************************changed: [webA]changed: [webB]TASK [debug] ***********************************************************************************************************ok: [webA] =&gt; &#123; &quot;var_result&quot;: &#123; &quot;changed&quot;: true, &quot;cmd&quot;: &quot;echo \\&quot;myname yunjisuan,myage 3\\&quot;&quot;, &quot;delta&quot;: &quot;0:00:00.002324&quot;, &quot;end&quot;: &quot;2018-08-31 01:10:51.436707&quot;, &quot;failed&quot;: false, &quot;rc&quot;: 0, &quot;start&quot;: &quot;2018-08-31 01:10:51.434383&quot;, &quot;stderr&quot;: &quot;&quot;, &quot;stderr_lines&quot;: [], &quot;stdout&quot;: &quot;myname yunjisuan,myage 3&quot;, &quot;stdout_lines&quot;: [ &quot;myname yunjisuan,myage 3&quot; ] &#125;&#125;ok: [webB] =&gt; &#123; &quot;var_result&quot;: &#123; &quot;changed&quot;: true, &quot;cmd&quot;: &quot;echo \\&quot;myname yunjisuan,myage 3\\&quot;&quot;, &quot;delta&quot;: &quot;0:00:00.003688&quot;, &quot;end&quot;: &quot;2018-08-31 01:10:51.908615&quot;, &quot;failed&quot;: false, &quot;rc&quot;: 0, &quot;start&quot;: &quot;2018-08-31 01:10:51.904927&quot;, &quot;stderr&quot;: &quot;&quot;, &quot;stderr_lines&quot;: [], &quot;stdout&quot;: &quot;myname yunjisuan,myage 3&quot;, &quot;stdout_lines&quot;: [ &quot;myname yunjisuan,myage 3&quot; ] &#125;&#125;PLAY RECAP *************************************************************************************************************webA : ok=3 changed=1 unreachable=0 failed=0 webB : ok=3 changed=1 unreachable=0 failed=0 9.2 在playbook中使用ansible内置变量 使用ansible all -m setup | less查看ansible内置变量 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[root@localhost scripts]# vim test_setupvars.yaml---- hosts: all gather_facts: True #使用ansible内置变量（开启内置变量） tasks: - name: setup var #定义任务名 shell: echo &quot;ip &#123;&#123; ansible_all_ipv4_addresses[0] &#125;&#125; cpu &#123;&#123; ansible_processor_count &#125;&#125;&quot; register: var_result - debug: var=var_result[root@localhost scripts]# ansible-playbook test_setupvars.yaml PLAY [all] *************************************************************************************************************TASK [Gathering Facts] *************************************************************************************************ok: [webB]ok: [webA]TASK [setup var] *******************************************************************************************************changed: [webA]changed: [webB]TASK [debug] ***********************************************************************************************************ok: [webA] =&gt; &#123; &quot;var_result&quot;: &#123; &quot;changed&quot;: true, &quot;cmd&quot;: &quot;echo \\&quot;ip 192.168.131.132 cpu 1\\&quot;&quot;, &quot;delta&quot;: &quot;0:00:00.006103&quot;, &quot;end&quot;: &quot;2018-08-31 01:15:24.088136&quot;, &quot;failed&quot;: false, &quot;rc&quot;: 0, &quot;start&quot;: &quot;2018-08-31 01:15:24.082033&quot;, &quot;stderr&quot;: &quot;&quot;, &quot;stderr_lines&quot;: [], &quot;stdout&quot;: &quot;ip 192.168.131.132 cpu 1&quot;, &quot;stdout_lines&quot;: [ &quot;ip 192.168.131.132 cpu 1&quot; ] &#125;&#125;ok: [webB] =&gt; &#123; &quot;var_result&quot;: &#123; &quot;changed&quot;: true, &quot;cmd&quot;: &quot;echo \\&quot;ip 192.168.131.110 cpu 1\\&quot;&quot;, &quot;delta&quot;: &quot;0:00:00.002989&quot;, &quot;end&quot;: &quot;2018-08-31 01:15:24.559372&quot;, &quot;failed&quot;: false, &quot;rc&quot;: 0, &quot;start&quot;: &quot;2018-08-31 01:15:24.556383&quot;, &quot;stderr&quot;: &quot;&quot;, &quot;stderr_lines&quot;: [], &quot;stdout&quot;: &quot;ip 192.168.131.110 cpu 1&quot;, &quot;stdout_lines&quot;: [ &quot;ip 192.168.131.110 cpu 1&quot; ] &#125;&#125;PLAY RECAP *************************************************************************************************************webA : ok=3 changed=1 unreachable=0 failed=0 webB : ok=3 changed=1 unreachable=0 failed=0 演示ansible内置变量的取用方法ansible all -m setup | less 1234567891011[root@localhost scripts]# cat test_setupvars.yaml---- hosts: all gather_facts: True tasks: - name: setup var shell: echo &quot;ip &#123;&#123; ansible_all_ipv4_addresses[0] &#125;&#125; cpu &#123;&#123; ansible_processor_count &#125;&#125;&quot; &gt;&gt; /tmp/test - name: setup var2 shell: echo &quot;time &#123;&#123; ansible_date_time[&quot;date&quot;] &#125;&#125;&quot; &gt;&gt; /tmp/test register: var_result - debug: var=var_result 十，Playbook下发可变配置文件10.1 利用template模块下发可变的配置文件1234567891011121314151617181920212223242526272829[root@localhost scripts]# vim /tmp/testmy name is &#123;&#123; myname &#125;&#125; #自定义变量my name is &#123;&#123; ansible_all_ipv4_addresses[0] &#125;&#125; #系统变量[root@localhost scripts]# vim test_filevars.yaml---- hosts: all gather_facts: True #开启系统变量 vars: - myname: &quot;yunjisuan&quot; #自定义变量 tasks: - name: template test template: src=/tmp/test dest=/root/test #使用template下发可变配置文件[root@localhost scripts]# ansible-playbook test_filevars.yamlPLAY [all] *************************************************************************************************************TASK [Gathering Facts] *************************************************************************************************ok: [webA]ok: [webB]TASK [template test] ***************************************************************************************************changed: [webA]changed: [webB]PLAY RECAP *************************************************************************************************************webA : ok=2 changed=1 unreachable=0 failed=0 webB : ok=2 changed=1 unreachable=0 failed=0 10.2 下发配置文件里面使用判断语法12345678910111213141516171819202122232425262728293031323334353637[root@localhost scripts]# vim /tmp/if.j2 &#123;% if PORT %&#125; #if PORT存在ip=0.0.0.0:&#123;&#123; PORT &#125;&#125;&#123;% else %&#125; #否则的话ip=0.0.0.0:80&#123;% endif %&#125; #结尾[root@localhost scripts]# vim test_ifvars.yaml ---- hosts: all gather_facts: True #开启系统内置变量 vars: - PORT: 90 #自定义变量 tasks: - name: jinja2 if test template: src=/tmp/if.j2 dest=/root/test[root@localhost scripts]# ansible-playbook test_ifvars.yaml PLAY [all] *************************************************************************************************************TASK [Gathering Facts] *************************************************************************************************ok: [webA]ok: [webB]TASK [jinja2 if test] **************************************************************************************************changed: [webB]changed: [webA]PLAY RECAP *************************************************************************************************************webA : ok=2 changed=1 unreachable=0 failed=0 webB : ok=2 changed=1 unreachable=0 failed=0 #webA服务器：[root@localhost ~]# cat test ip=0.0.0.0:90 将变量PORT值为空，结果为： 123456789101112131415161718192021222324252627282930[root@localhost scripts]# vim test_ifvars.yaml ---- hosts: all gather_facts: True vars: - PORT: #置空 tasks: - name: jinja2 if test template: src=/tmp/if.j2 dest=/root/test [root@localhost scripts]# ansible-playbook test_ifvars.yamlPLAY [all] *************************************************************************************************************TASK [Gathering Facts] *************************************************************************************************ok: [webA]ok: [webB]TASK [jinja2 if test] **************************************************************************************************changed: [webA]changed: [webB]PLAY RECAP *************************************************************************************************************webA : ok=2 changed=1 unreachable=0 failed=0 webB : ok=2 changed=1 unreachable=0 failed=0 #webA服务器:[root@localhost ~]# cat test ip=0.0.0.0:80 十一，Playbook的notify通知和下发nginx配置12345678910111213141516#实战下发可执行动作的可变的nginx配置文件[root@localhost scripts]# head -1 /tmp/nginx.j2 worker_processes &#123;&#123; ansible_processor_count &#125;&#125;; #可变的参数[root@localhost scripts]# cat test_nginxvars.yaml ---- hosts: all gather_facts: True #开启系统内置变量 tasks: - name: nginx conf template: src=/tmp/nginx.j2 dest=/usr/local/nginx/conf/nginx.conf notify: - reload nginx #下发通知给handlers模块执行名字叫做reload nginx的动作 handlers: #定义动作 - name: reload nginx #动作的名字 shell: /usr/local/nginx/sbin/nginx -s reload[root@localhost scripts]# ansible-playbook test_nginxvars.yaml","categories":[{"name":"架构基础","slug":"架构基础","permalink":"https://kkabuzs.github.io/categories/%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://kkabuzs.github.io/tags/Ansible/"}]},{"title":"Nginx反向代理","slug":"nginxfanxiangdaili","date":"2018-08-22T02:25:53.000Z","updated":"2018-08-22T02:25:53.000Z","comments":true,"path":"articles/2018/08/22/nginxfanxiangdaili/","permalink":"https://kkabuzs.github.io/articles/2018/08/22/nginxfanxiangdaili/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Nginx反向代理1.1 集群简介 简单地说，集群就是指一组（若干个）相互独立的计算机，利用高速通信网络组成的一个较大的计算机服务系统，每个集群节点（即集群中的每台计算机）都是运行各自服务的独立服务器。这些服务器之间可以彼此通信，协同向用户提供应用程序，系统资源和数据，并以单一系统的模式加以管理。当用户客户机请求集群系统时，集群给用户的感觉就是一个单一独立的服务器，而实际上用户请求的是一组集群服务器。 若要用一句话描述集群，即一堆服务器合作做同一件事，这些机器可能需要整个技术团队架构，设计和统一协调管理，这些机器可以分布在一个机房，也可以分布在全国全球各个地区的多个机房。 1.2 集群的好处 高性能,价格有效性,可伸缩性,高可用性,透明性,可管理性,可编程性 1.3 集群的常见分类1.3.1 集群的常见分类可分为： 负载均衡集群，简称LBC或者LB 高可用性集群，简称HAC 高性能计算集群，简称HPC 网格计算集群 1.3.2 不同种类的集群介绍（1）负载均衡集群作用： 分摊用户访问请求及数据流量（负载均衡） 保持业务连续性，即7*24小时服务（高可用性）。 应用于Web业务及数据库从库等服务器的业务 （2）高可用性集群作用： 当一台机器宕机时，另外一台机器接管宕机的机器的IP资源和服务资源，提供服务。 常用于不易实现负载均衡的应用，比如负载均衡器，主数据库，主存储对之间。 1.4 Nginx负载均衡集群介绍1.4.1 搭建负载均衡服务的需求负载均衡集群提供了一种廉价，有效，透明的方法，来扩展网络设备和服务器的负载，带宽和吞吐量，同时加强了网络数据处理能力，提高了网络的灵活性和可用性。 1.5 实践Nginx负载均衡环境准备1.5.1 环境准备 HOSTNAME IP 说明 nuinx反向代理 192.168.131.132 Nginx主负载均衡器 nginx web1 192.168.131.101 web1 服务器 nginx web2 192.168.131.102 web2 服务器 1.5.2 安装Nginx软件12345678[root@localhost ~]# yum install -y pcre-devel openssl-devel[root@localhost ~]# useradd -s /sbin/nologin -M nginx #创建程序用户[root@localhost ~]# tar xf nginx-1.10.2.tar.gz -C /usr/src/ #解压[root@localhost ~]# cd /usr/src/nginx-1.10.2/[root@localhost nginx-1.10.2]# ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module &amp;&amp; make &amp;&amp; make install #编译安装等待编译......[root@localhost nginx]# ln -s /usr/local/nginx/sbin/* /usr/local/sbin/ #做软连接，让环境变量能找到 1.5.3 配置用于测试的Web服务两台配置相同： 12345678910111213141516171819202122232425262728[root@localhost conf]# vim nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name www.zhaoshuo.com; location / &#123; root html/www; index index.html index.htm; &#125; &#125; server &#123; listen 80; server_name bbs.zhaoshuo.com; location / &#123; root html/bbs; index index.html index.htm; &#125; &#125;&#125; 填充测试文件数据 123456789101112131415[root@localhost nginx]# lsconf html logs sbin[root@localhost nginx]# cd html[root@localhost html]# ls50x.html index.html[root@localhost html]# mkdir www bbs[root@localhost html]# echo &quot;welcome www `hostname -I`&quot; &gt; www/index.html[root@localhost html]# echo &quot;welcome bbs `hostname -I`&quot; &gt; bbs/index.html#配置映射：[root@localhost html]# vim /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.131.101 bbs.zhaoshuo.com www.zhaoshuo.com 启动Nginx服务 12[root@localhost html]# /usr/local/nginx/sbin/nginx curl测试 web1: 1234567[root@localhost html]# curl bbs.zhaoshuo.comwelcome bbs 192.168.131.101 [root@localhost html]# curl www.zhaoshuo.comwelcome www 192.168.131.101 [root@localhost html]# curl 192.168.131.101welcome www 192.168.131.101 web2: 1234567[root@localhost html]# curl bbs.zhaoshuo.comwelcome bbs 192.168.131.101 [root@localhost html]# curl www.zhaoshuo.comwelcome www 192.168.131.101 [root@localhost html]# curl 192.168.131.102welcome www 192.168.131.102 1.5.4 实现一个负载均衡配置nginx负载均衡器配置文件： 123456789101112131415161718192021222324[root@localhost conf]# vim nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; upstream www_server &#123; #upstream是服务器池，www_server是服务器池的名字 server 192.168.131.101 weight=1; #server就是一个RS节点。weight是权重，负载均衡是按照权重比分配的 server 192.168.131.102 weight=1; &#125; server &#123; listen 80; server_name bbs.zhaoshuo.com; location / &#123; proxy_pass http://www_server; proxy_pass是反向代理推送请求，nginx自己发出，跟用户无关 &#125; &#125;&#125;[root@localhost html]# /usr/local/nginx/sbin/nginx #启动服务 添加映射： 12345[root@localhost conf]# vim /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.131.132 bbs.zhaoshuo.com 负载均衡测试 123456789[root@localhost conf]# curl bbs.zhaoshuo.comwelcome www 192.168.131.101 [root@localhost conf]# curl bbs.zhaoshuo.comwelcome www 192.168.131.102 [root@localhost conf]# curl bbs.zhaoshuo.comwelcome www 192.168.131.101 [root@localhost conf]# curl bbs.zhaoshuo.comwelcome www 192.168.131.102 1.6 Nginx负载均衡核心组件1.6.1 Nginx upstream模块upstream模块语法 123456upstream www_server &#123;server 192.168.131.101; #这行标签和下行是等价的server 192.168.131.102:80 weight=1 max_fails=1 fail_timeout=10s; #这行标签和上一行是等价的，此行多余的部分就是默认配置，不写也可以。server 192.168.131.103:80 weight=1 max_fails=2 fail_timeout=20s backup; #backup是备份，其他server坏了，才会启用此条server，max_fails=1是错误次数，upstream会检查所有server状态，会ping ip，只要有一次不通，就会认为此条server坏了，就不会向此条server推用户&#125; upstream模块相关说明 upstream模块调度算法 （1） rr轮询（默认调度算法，静态调度算法） 按客户端请求顺序把客户端的请求逐一分配到不同的后端节点服务器，这相当于LVS中的rr算法，如果后端节点服务器宕机（默认情况下Nginx只检测80端口），宕机的服务器会被自动从节点服务器池中剔除，以使客户端的用户访问不受影响。新的请求会分配给正常的服务器。 （2）wrr（权重轮询，静态调度算法） 在rr轮询算法的基础上加上权重，即为权重轮询算法，当使用该算法时，权重和用户访问成正比，权重值越大，被转发的请求也就越多。可以根据服务器的配置和性能指定权重值大小，有效解决新旧服务器性能不均带来的请求分配问题。 （3）ip_hash（静态调度算法）（会话保持） 每个请求按客户端IP的hash结果分配，当新的请求到达时，先将其客户端IP通过哈希算法哈希出一个值，在随后的客户端请求中，客户IP的哈希值只要相同，就会被分配至同一台服务器，该调度算法可以解决动态网页的session共享问题，但有时会导致请求分配不均，即无法保证1：1的负载均衡，因为在国内大多数公司都是NAT上网模式，多个客户端会对应一个外部IP，所以，这些客户端都会被分配到同一节点服务器，从而导致请求分配不均。LVS负载均衡的-p参数，Keepalived配置里的persistence_timeout 50参数都类似这个Nginx里的ip_hash参数，其功能都可以解决动态网页的session共享问题。 同一个ip地址的用户永远负载给同一台服务器。 1234567 upstream www_server &#123; ip_hash; server 192.168.131.101 weight=1; server 192.168.131.102 weight=1; &#125; #说明：当负载调度算法为ip_hash时，后端服务器在负载均衡调度中的状态不能有weight和backup，即使有也不会生效。 （4）fair（动态调度算法） 此算法会根据后端节点服务器的响应时间来分配请求，响应时间短的优先分配。这是更加智能的调度算法。此种算法可以根据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间来分配请求，响应时间短的优先分配。Nginx本身不支持fair调度算法，如果需要使用这种调度算法，必须下载Nginx相关模块upstream_fair。 12345upstream www_server &#123; server 192.168.131.101 weight=1; server 192.168.131.102 weight=1; fair; &#125; （5）least_conn least_conn算法会根据后端节点的连接数来决定分配情况，哪个机器连接数少就分发。 （6）url_hash算法(web缓存节点) 与ip_hash类似，这里是根据访问URL的hash结果来分配请求的，让每个URL定向到同一个后端服务器，后端服务器为缓存服务器时效果显著。在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method使用的是hash算法。url_hash按访问URL的hash结果来分配请求，使每个URL定向到同一个后端服务器，可以进一步提高后端缓存服务器的效率命令率。Nginx本身是不支持url_hash的，如果需要使用这种调度算法，必须安装Nginx的hash模块软件包。 123456upstream www_server &#123; server 192.168.131.101 weight=1; server 192.168.131.102 weight=1; hash $request_uri; hash_method crc32; &#125; （7）一致性hash算法 一致性hash算法一般用于代理后端业务为缓存服务（如Squid，Memcached）的场景，通过将用户请求的URI或者指定字符串进行计算，然后调度到后端的服务器上，此后任何用户查找同一个URI或者指定字符串都会被调度到这一台服务器上，因此后端的每个节点缓存的内容都是不同的，一致性hash算法可以解决后端某个或几个节点宕机后，缓存的数据动荡最小，一致性hash算法知识比较复杂 12345upstream www_server &#123; consistent_hash $request_uri; server 192.168.131.101 weight=1; server 192.168.131.102 weight=1; &#125; 1.6.2 http_proxy_module模块1.6.2.1 proxy_pass指令介绍 proxy_pass指令属于ngx_http_proxy_module模块，此模块可以将请求转发到另一台服务器，在实际的反向代理工作中，会通过location功能匹配指定的URI，然后把接收到的符合匹配URI的请求通过proxy_pass抛给定义好的upstream节点池。 使用方法示例： （1）将匹配URI为name的请求抛给http://127.0.0.1/remote/. 123location /name/ &#123;proxy_pass http://127.0.0.1/remote/;&#125; （2）将匹配URI为some&#x2F;path的请求抛给http://127.0.0.1 123location /some/path/ &#123;proxy_pass http://127.0.0.1;&#125; （3）将匹配URI为name的请求应用指定的rewrite规则，然后抛给http://127.0.0.1 1234location /name/ &#123;rewrite /name/( [^/]+ ) /username=$1 break;proxy_pass http://127.0.0.1;&#125; 1.6.2.2 http proxy模块参数 1.7 反向代理多虚拟主机节点服务器1.7.1 解决域名优先级和ip问题12proxy_set_header X-Forwarded-For $remote_addr;#这是反向代理时，节点服务器获取用户真实IP的必要功能配置 1.7.1.1 修改配置文件12345678910111213141516171819202122232425[root@localhost nginx]# vim conf/nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; upstream www_server &#123; server 192.168.131.101 weight=1; server 192.168.131.102 weight=1; &#125; server &#123; listen 80; server_name bbs.zhaoshuo.com; location / &#123; proxy_pass http://www_server; proxy_set_header host $host; #在代理向后端服务器发送的http请求头中加入host字段信息，用于当后端服务器配置有多个虚拟主机时，可以识别代理的是哪个虚拟主机。这是节点服务器多虚拟主机时的关键配置。 proxy_set_header X-Forwarded-For $remote_addr; #在代理向后端服务器发送的http请求头中加入X-Forwarded-For字段信息，用于后端服务器程序，日志等接收记录真实用户的IP，而不是代理服务器的IP &#125; &#125;&#125; 其它优化参数 123456789101112[root@localhost conf]# cat proxy.conf proxy_set_header host $host;proxy_set_header x-forwarded-for $remote_addr;proxy_connect_timeout 60;proxy_send_timeout 60;proxy_read_timeout 60;proxy_buffer_size 4k;proxy_buffers 4 32k;proxy_busy_buffers_size 64k;proxy_temp_file_write_size 64k;#说明：直接include进配置文件即可！ 1.7.1.2 重新加载Nginx服务，并用curl测试检查123456[root@localhost nginx]# nginx -s reload[root@localhost nginx]# curl bbs.zhaoshuo.comwelcome bbs 192.168.131.101 [root@localhost nginx]# curl bbs.zhaoshuo.comwelcome bbs 192.168.131.102 1.7.1.3客户端配置修改配置文件： 123456789101112131415161718192021222324252627282930313233[root@localhost ~]# vim /usr/local/nginx/conf/nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; log_format main &#x27;$remote_addr-$remote_user[$time_local]&quot;$request&quot;&#x27; #自定义日志格式 &#x27;$status $body_bytes_sent &quot;$http_referer&quot;&#x27; &#x27;&quot;$http_user_agent&quot;&quot;$http_x_forwarded_for&quot;&#x27;; server &#123; listen 80; server_name www.zhaoshuo.com; location / &#123; root html/www; index index.html index.htm; &#125; access_log logs/access_www.log main; #启用自定义日志 &#125; server &#123; listen 80; server_name bbs.zhaoshuo.com; location / &#123; root html/bbs; index index.html index.htm; &#125; access_log logs/access_bbs.log main; #启用自定义日志 &#125;&#125; 重启服务 12345[root@localhost ~]# /usr/local/nginx/sbin/nginx -tnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful[root@localhost ~]# /usr/local/nginx/sbin/nginx -s reload 1.7.1.4 测试访问1234567891011[root@bbs nginx]# curl bbs.zhaoshuo.comwelcome bbs 192.168.131.101 [root@bbs nginx]# curl bbs.zhaoshuo.comwelcome bbs 192.168.131.101 [root@bbs nginx]# curl bbs.zhaoshuo.comwelcome bbs 192.168.131.102 [root@bbs nginx]# curl bbs.zhaoshuo.comwelcome bbs 192.168.131.101 [root@bbs nginx]# curl bbs.zhaoshuo.comwelcome bbs 192.168.131.102 1.8 根据URL中的目录地址实现代理转发1.8.1 nginx反向代理机器配置123456789101112131415161718192021222324252627282930313233343536373839[root@bbs ~]# vim /usr/local/nginx/conf/nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; upstream www_server &#123; server 192.168.131.101 weight=1; &#125; upstream bbs_server &#123; server 192.168.131.102 weight=1; &#125; upstream zzz_server &#123; server 192.168.131.105 weight=1; &#125; server &#123; listen 80; server_name www.zhaoshuo.com; location / &#123; proxy_pass http://www_server; proxy_set_header host $host; proxy_set_header X-Forwarded-For $remote_addr; &#125; location /bbs/ &#123; proxy_pass http://bbs_server; proxy_set_header host $host; proxy_set_header X-Forwarded-For $remote_addr; &#125; location /zzz/ &#123; proxy_pass http://zzz_server; proxy_set_header host $host; proxy_set_header X-Forwarded-For $remote_addr; &#125; &#125;&#125; 1.8.2 web1服务端配置123456789101112131415161718192021222324[root@bbs ~]# vim /usr/local/nginx/conf/nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; log_format main &#x27;$remote_addr-$remote_user[$time_local]&quot;$request&quot;&#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot;&#x27; &#x27;&quot;$http_user_agent&quot;&quot;$http_x_forwarded_for&quot;&#x27;; server &#123; listen 80; server_name www.zhaoshuo.com; location / &#123; root html/www; index index.html index.htm; &#125; access_log logs/access_www.log main; &#125;&#125; 1.8.3 web2服务端配置123456789101112131415161718192021222324[root@localhost nginx]# vim /usr/local/nginx/conf/nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name bbs.zhaoshuo.com; location /bbs/ &#123; root html/bbs; index index.html index.htm; &#125; &#125;&#125;#注意路径的使用[root@localhost ~]# cat /usr/local/nginx/html/bbs/bbs/index.html welcome to bbs 1.8.4 web3服务端配置123456789101112131415161718192021222324[root@localhost nginx]# vim conf/nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name zzz.zhaoshuo.com; location /zzz/ &#123; root html/zzz; index index.html index.htm; &#125; &#125;&#125;#注意路径的使用[root@localhost ~]# cat /usr/local/nginx/html/zzz/zzz/index.html hahaha zzz 1.8.5 测试 1.9 根据客户端的设备（user_agent）转发实践需求1.9.1 根据客户端设备（user_agent）转发请求123456789101112131415location / &#123;if ($http_user_agent ~* &quot;MSIE&quot;)#如果请求的浏览器为微软IE浏览器（MSIE），则让请求由static_pools池处理&#123;proxy_pass http://static_pools;&#125;if ($http_user_agent ~* &quot;Chrome&quot;)#如果请求的浏览器为谷歌浏览器（Chrome），则让请求由upload_pools池处理&#123;proxy_pass http：//upload_pools;&#125;proxy_pass http://default_pools;#其他客户端，由default_pools处理include proxy.conf;&#125; 除了针对浏览器外，上述“$http_user_agent”变量也可针对移动端，比如安卓，苹果，Ipad设备进行匹配，去请求指定的服务器 123456789101112location / &#123;if ($http_user_agent ~* &quot;android&quot;)&#123;proxy_pass http://android_pools; #这里是android服务器池&#125;if ($http_user_agent ~* &quot;iphone&quot;)&#123;proxy_pass http://iphone_pools; #这里是iphone服务器池&#125;proxy_pass http://pc_pools; #这里是默认的pc服务器池include extra/proxy.conf;&#125; 2.0 根据文件扩展名实现代理转发2.0.1 相关server配置1234567891011121314#先看看location方法的匹配规则，如下：location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf|css|js)$ &#123;proxy_pass http://static_pools;include proxy.conf;&#125;#下面是if语句方法的匹配规则：if ($request_uri ~* &quot;.*\\.(php|php5)$&quot;)&#123;proxy_pass http://php_server_pools;&#125;if ($request_uri ~* &quot;.*\\.(jsp|jsp*|do|do*)$&quot;)&#123;proxy_pass http://java_server_pools;&#125; 2.0.2根据扩展名转发12345678location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf|css|js)$ &#123;proxy_pass http://static_pools;include proxy.conf;&#125;location ~ .*\\.(php|php3|php5)$ &#123;proxy_pass http://dynamic_pools;include proxy.conf&#125; 2.1 Nginx负载均衡检测节点状态 淘宝技术团队开发了一个Tengine（Nginx的分支）模块Nginx_upstream_check_module,用于提供主动式后端服务器健康检查。通过它可以检测后端realserver的健康状态，如果后端realserver不可用，则所有的请求就不会转发到该节点上。Tengine原生支持这个模块，而Nginx则需要通过打补丁的方式将该模块添加到Nginx中。补丁下载地址：https://github.com/yaoweibin/nginx_upstream_check_module。 2.1.1 安装nginx_upstream_check_module模块12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@localhost ~]# /usr/local/nginx/sbin/nginx -V #查看版本号和详细信息nginx version: nginx/1.10.2built by gcc 4.4.7 20120313 (Red Hat 4.4.7-4) (GCC) built with OpenSSL 1.0.1e-fips 11 Feb 2013TLS SNI support enabledconfigure arguments: --user=nginx --group=nginx --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module#下载补丁包[root@localhost ~]# wget https://codeload.github.com/yaoweibin/nginx_upstream_check_module/zip/master[root@localhost ~]# unzip master #解压 unzip命令需yum安装[root@localhost ~]# lsanaconda-ks.cfg install.log install.log.syslog master nginx-1.10.2.tar.gz nginx_upstream_check_module-master[root@localhost ~]# mv ~/nginx_upstream_check_module-master /usr/src/#因为是对源程序打补丁，所以还需要Nginx源程序[root@localhost ~]# cd /usr/src/nginx-1.10.2/[root@localhost nginx-1.10.2]# patch -p0 &lt; /usr/src/nginx_upstream_check_module-master/check_1.9.2+.patchpatching file src/http/modules/ngx_http_upstream_hash_module.cpatching file src/http/modules/ngx_http_upstream_ip_hash_module.cpatching file src/http/modules/ngx_http_upstream_least_conn_module.cpatching file src/http/ngx_http_upstream_round_robin.cpatching file src/http/ngx_http_upstream_round_robin.h#备份源安装程序[root@localhost nginx-1.10.2]# cd /usr/local/[root@localhost local]# lsbin etc games include lib lib64 libexec nginx sbin share src[root@localhost local]# mv nginx&#123;,.ori&#125;[root@localhost local]# lsbin etc games include lib lib64 libexec nginx.ori sbin share src#重新进行编译，编译的参数要和以前一致，最后加上 --add-module=/usr/src/nginx_upstream_check_module-master/[root@localhost local]# cd /usr/src/nginx-1.10.2/[root@localhost nginx-1.10.2]# ./configure --prefix=/usr/local/nginx --user=nginx --group=nginx --with-http_ssl_module --with-http_stub_status_module --add-module=/usr/src/nginx_upstream_check_module-master &amp;&amp; make &amp;&amp; make install[root@localhost nginx-1.10.2]# /usr/local/nginx/sbin/nginx -Vnginx version: nginx/1.10.2built by gcc 4.4.7 20120313 (Red Hat 4.4.7-4) (GCC) built with OpenSSL 1.0.1e-fips 11 Feb 2013TLS SNI support enabledconfigure arguments: --prefix=/usr/local/nginx --user=nginx --group=nginx --with-http_ssl_module --with-http_stub_status_module --add-module=/usr/src/nginx_upstream_check_module-master/#拷贝源配置文件到当前Nginx的安装目录下[root@localhost nginx-1.10.2]# cd /usr/local/[root@localhost local]# pwd/usr/local[root@localhost local]# cp bin/ games/ lib/ libexec/ nginx.ori/ share/ etc/ include/ lib64/ nginx/ sbin/ src/ [root@localhost local]# cp nginx.ori/conf/nginx.conf nginx/confcp: overwrite `nginx/conf/nginx.conf&#x27;? y[root@localhost local]# /usr/local/nginx/sbin/nginx -tnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful 2.1.2 配置Nginx健康检查12345678910111213141516171819202122232425262728293031323334353637383940414243444546[root@localhost ~]# vim /usr/local/nginx/conf/nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; upstream www_server &#123; server 192.168.131.101 weight=1; &#125; upstream bbs_server &#123; server 192.168.131.102 weight=1; check interval=3000 rise=2 fall=5 timeout=1000 type=http; #对bbs服务器池开启健康监测 &#125; upstream zzz_server &#123; server 192.168.131.105 weight=1; &#125; server &#123; listen 80; server_name www.zhaoshuo.com; location / &#123; proxy_pass http://www_server; proxy_set_header host $host; proxy_set_header X-Forwarded-For $remote_addr; &#125; location /bbs/ &#123; proxy_pass http://bbs_server; proxy_set_header host $host; proxy_set_header X-Forwarded-For $remote_addr; check_status; #启动健康检查模块 access_log off; #关闭此location的访问日志记录 &#125; location /zzz/ &#123; proxy_pass http://zzz_server; proxy_set_header host $host; proxy_set_header X-Forwarded-For $remote_addr; &#125; &#125;&#125;#说明：check interval=3000 rise=2 fall=5 timeout=1000 type=http; 上面配置的意思时，对static_pools这个负载均衡条目中的所有节点，每隔3秒检测一次，请求2次正常则标记realserver状态为up，如果检测5次都失败，则标记realserver的状态为down,超时时间为1秒，检查的协议是HTTP。 启动服务 12345[root@localhost ~]# /usr/local/nginx/sbin/nginx [root@localhost ~]# ss -antup | grep nginxtcp LISTEN 0 128 *:80 *:* users:((&quot;nginx&quot;,3673,6),(&quot;nginx&quot;,3674,6))#注意此处必须重启Nginx，不能重新加载 测试","categories":[{"name":"架构基础","slug":"架构基础","permalink":"https://kkabuzs.github.io/categories/%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://kkabuzs.github.io/tags/Nginx/"}]},{"title":"Nginx详细讲解","slug":"nginxxiangxijiangjie","date":"2018-08-16T10:52:14.000Z","updated":"2018-08-16T10:52:14.000Z","comments":true,"path":"articles/2018/08/16/nginxxiangxijiangjie/","permalink":"https://kkabuzs.github.io/articles/2018/08/16/nginxxiangxijiangjie/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 Nginx详细讲解一，Nginx是什么？ nginx是一个开源的，支持高性能，高并发的www服务和代理服务软件。 nginx因具有高并发（特别是静态资源），占用系统资源少等特性，且功能丰富而逐渐流行起来。 nginx不但是一个优秀Web服务软件，还具有反向代理负载均衡功能和缓存服务功能，与lvs负载均衡及Haproxy等专业代理软件相比，Nginx部署起来更为简单，方便；在缓存功能方面，它又类似于Squid等专业的缓存服务软件。 nginx还有一个分支，由淘宝开发，叫 Tengine 1.1 Nginx的重要特性 支持高并发：能支持几万并发连接（特别是静态小文件业务环境） 资源消耗少：在3万并发连接下，开启10个Nginx线程消耗的内存不到200MB 可以做HTTP反向代理及加速缓存，即负载均衡功能，内置对RS节点服务器健康检查功能，这相当于专业的Haproxy软件或LVS的功能 具备Squid等专业缓存软件等的缓存功能。 支持异步网络I／O事件模型epoll（linux2.6+）。 1.2 nginx的特点 作为HTTP服务软件的后起之秀，Nginx与它的老大哥Apache相比有很多改进之处，比如，在性能上，Nginx占用的系统资源更少，能支持更多的并发连接（特别是静态小文件场景下），达到更高的访问效率；在功能上，Nginx不但是一个优秀的Web服务软件，还可以作为反向代理负载均衡及（前端）缓存服务使用；在安装配置上，Nginx更为方便，简单，灵活，可以说，Nginx是一个极具发展潜力的Web服务软件。 1.3 Nginx软件的主要企业功能应用（1）作为Web服务软件 Nginx是一个支持高性能，高并发的Web服务软件，它具有很多优秀的特性，作为Web服务器，与Apache相比，Nginx能够支持更多的并发连接访问，但占用的资源更少，效率更高，在功能上也强大了很多，几乎不逊色于Apache。 （2）反向代理或负载均衡服务 在反向代理或负载均衡服务方面，Nginx可以作为Web服务，PHP等动态服务及Memcached缓存的代理服务器，它具有类似专业反向代理软件（如Haproxy）的功能，同时也是一个优秀的邮件代理服务软件，但是Nginx的代理功能还是相对简单了些，特别是支持TCP的代理（Nginx1.9.0版本已经开始支持TCP代理了） （3）前端业务数据缓存服务 在Web缓存服务方面，Nginx可通过自身的proxy_cache模块实现类Squid等专业缓存软件的功能。 1.4 为什么Nginx总体性能比Apache高？ Nginx使用最新的epoll（Linux2.6内核）和kqueue（freebsd）异步网络I／O模型，而Apache使用的是传统的select模型。目前Linux下能够承受高并发访问的Squid，Memcached软件采用的都是epoll模型。 处理大量连接的读写时，Apache所采用的select同步网络I／O模型比较低效。 如何正确选择Web服务器 静态业务：若是高并发场景，尽量采用Nginx或Lighttpd，二者首选Nginx。 动态业务：理论上采用Nginx和Apache均可，建议选择Nginx，为了避免相同业务的服务软件多样化，增加额外- 维护成本。动态业务可以由Nginx兼做前端代理，再根据页面元素的类型或目录，转发到后端相应的服务器进行处理。 既有静态业务又有动态业务：采用Nginx 二，nginx的编译安装部署2.1 编译安装和安装所需的支持包12345678[root@localhost ~]# yum install -y pcre-devel openssl-devel[root@localhost ~]# useradd -s /sbin/nologin -M nginx #创建程序用户[root@localhost ~]# tar xf nginx-1.10.2.tar.gz -C /usr/src/ #解压[root@localhost ~]# cd /usr/src/nginx-1.10.2/[root@localhost nginx-1.10.2]# ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module &amp;&amp; make &amp;&amp; make install #编译安装等待编译......[root@localhost nginx]# ln -s /usr/local/nginx/sbin/* /usr/local/sbin/ #做软连接，让环境变量能找到 web排错三部曲客户端排查的思路 第一步，在客户端上ping服务器端IP，命令如下： ping 10.0.0.8排除物理线路问题影响 第二步，在客户端上telnet服务器端IP，端口，命令如下： telnet 10.0.0.8 80排除防火墙等得影响 第三步，在客户端使用wget命令检测，如下： wget 10.0.0.8(curl -I 10.0.0.8)模拟用户访问，排除http服务自身问题，根据输出在排错 2.2 nginx主配置文件nginx.conf123456789101112131415161718192021222324252627282930313233343536[root@localhost conf]# pwd/usr/local/nginx/conf[root@localhost conf]# lsfastcgi.conf koi-utf nginx.conf uwsgi_paramsfastcgi.conf.default koi-win nginx.conf.default uwsgi_params.defaultfastcgi_params mime.types scgi_params win-utffastcgi_params.default mime.types.default scgi_params.default[root@localhost conf]# egrep -v &quot;#|^$&quot; nginx.conf.default &gt; nginx.conf #去掉包含#号和空行的内容[root@localhost conf]# vim nginx.conf #打开nginx的配置文件worker_processes 1; #nginx工作的进程个数（刚开始有几核就写几，不能超过核数的二倍）#events函数是事件函数events &#123; worker_connections 1024; #进程里的工作的线程个数（上限为65535，通常设置为20480）&#125;#http函数为web服务模块http &#123; include mime.types; #Nginx支持的媒体类型库文件包含（include引入的意思，相对于配置文件当前路径） default_type application/octet-stream; #默认媒体类型 sendfile on; #文件的高效传输，默认开启 keepalive_timeout 65; #连接保持持续65秒（连接超时） #server函数代表一个虚拟网站 server &#123; listen 80; #socket进程的监听端口 server_name localhost; #域名 location / &#123; root html; #站点的根目录（相对于nginx安装路径），root是根的意思 index index.html index.htm; #指定网站的首页是哪个网页名，多个用空格分开 &#125; error_page 500 502 503 504 /50x.html; #出现对应的http状态码时，使用50x.html回应客户 location = /50x.html &#123; #Location区块开始，访问50x.html root html; #指定对应的站点目录为html &#125; &#125; &#125; 2.3 启动进程12345[root@localhost conf]# nginx #没配置软件接则要绝对路径启动[root@localhost conf]# ss -antup | grep 80tcp LISTEN 0 128 *:80 *:* users:((&quot;nginx&quot;,3555,6),(&quot;nginx&quot;,3556,6))tcp ESTAB 0 0 192.168.131.132:22 192.168.131.1:60808 users:((&quot;sshd&quot;,945,3)) 2.4 测试网站搭建12345678910111213141516[root@localhost nginx]# lsclient_body_temp conf fastcgi_temp html logs proxy_temp sbin scgi_temp uwsgi_temp[root@localhost nginx]# cd html/[root@localhost html]# rm -rf *[root@localhost html]# echo &quot;you are welcome&quot; &gt; index.html[root@localhost html]# lsindex.html[root@localhost html]# echo &quot;`hostname -I` www.yunjisuan.com&quot; &gt;&gt; /etc/hosts[root@localhost html]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.131.132 www.yunjisuan.com模拟访问：[root@localhost html]# curl www.yunjisuan.comyou are welcome 修改配置文件，nginx服务要重启 123[root@localhost html]# /usr/local/nginx/sbin/nginx -s reload #平滑重启[root@localhost html]# /usr/local/nginx/sbin/nginx -s stop #关闭服务[root@localhost html]# /usr/local/nginx/sbin/nginx -t #验证配置文件是否正确 在windows上测试C:\\Windows\\System32\\drivers\\etc映射文件路径 测试成功 2.5 同时搭建多个网站123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[root@localhost conf]# vim /usr/local/nginx/conf/nginx.conf #打开配置文件worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name www.yunjisuan.com; location / &#123; root html; index index.html index.htm; &#125; &#125; server &#123; #多添加两个server模块 listen 80; server_name bbs.yunjisuan.com; location / &#123; root html/bbs; index index.html index.htm; &#125; &#125; server &#123; listen 80; server_name blog.yunjisuan.com; location / &#123; root html/blog; index index.html index.htm; &#125; &#125;&#125;[root@localhost conf]# /usr/local/nginx/sbin/nginx -s reload #平滑重启[root@localhost ~]# cd /usr/local/nginx/html/[root@localhost html]# mkdir bbs blog #创建网页目录[root@localhost html]# echo &quot;`hostname -I` hahahaha&quot; &gt; bbs/index.html #添加内容[root@localhost html]# echo &quot;`hostname -I` hello welcome&quot; &gt; blog/index.html[root@localhost html]# vim /etc/hosts #添加映射127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.131.132 www.yunjisuan.com bbs.yunjisuan.com blog.yunjisuan.com开始测试是否成功：[root@localhost html]# curl www.yunjisuan.comyou are welcome[root@localhost html]# curl bbs.yunjisuan.com192.168.131.132 hahahaha[root@localhost html]# curl blog.yunjisuan.com192.168.131.132 hello welcome[root@localhost html]# curl 192.168.131.132 #当系统分不清找谁时，优先级从上往下，默认找第一个server模块you are welcome curl -v www.yunjisuan.com查看报头 一个web服务器搭建多个网站有三种方法 1， 基于不同域名的虚拟网站2， 基于不同监听端口的虚拟网站3， 基于不同ip的虚拟网站 2.6 禁止非法访问操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@localhost html]# vim /usr/local/nginx/conf/nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name www.yunjisuan.com; location / &#123; root html; index index.html index.htm; &#125; &#125; server &#123; listen 80; server_name bbs.yunjisuan.com; location / &#123; root html/bbs; index index.html index.htm; &#125; &#125; server &#123; listen 80 default_server; #加个参数，实现默认优先级最高 server_name blog.yunjisuan.com; location / &#123; return 403; #用户访问到这会跳出 &#125; &#125;&#125;[root@localhost html]# /usr/local/nginx/sbin/nginx -s reload #平滑重启[root@localhost html]# curl 192.168.131.132 #ip访问就出现错误&lt;html&gt;&lt;head&gt;&lt;title&gt;403 Forbidden&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor=&quot;white&quot;&gt;&lt;center&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx/1.10.2&lt;/center&gt;&lt;/body&gt;&lt;/html&gt;#说明：如果用域名访问，DNS解析会追查用户的位置，判断是否为合法用户，报头会记录访问用户的信息，如：系统型号，访问时间。所以，黑客就不会用域名来访问网站，所以以上搭建一个防黑客操作。黑客用ip访问，修改默认优先级，使之强制跳转到default_server模块，报错403。 2.7 include用法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@localhost ~]# cd /usr/local/nginx/conf/[root@localhost conf]# lsfastcgi.conf fastcgi_params.default mime.types nginx.conf.default uwsgi_paramsfastcgi.conf.default koi-utf mime.types.default scgi_params uwsgi_params.defaultfastcgi_params koi-win nginx.conf scgi_params.default win-utf[root@localhost conf]# mkdir extra #创建一个目录[root@localhost conf]# vim extra/bbs.conf #单独创建bbs网站配置文件 server &#123; listen 80; server_name bbs.yunjisuan.com; location / &#123; root html/bbs; index index.html index.htm; &#125; &#125; [root@localhost conf]# vim extra/www.conf #单独创建一个www网站配置文件 server &#123; listen 80; server_name www.yunjisuan.com; location / &#123; root html; index index.html index.htm; &#125; &#125;[root@localhost ~]# vim /usr/local/nginx/conf/nginx.conf #打开原配置文件worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; include extra/bbs.conf; #使用include导入 include extra/www.conf; server &#123; listen 80 default_server; server_name blog.yunjisuan.com; location / &#123; return 403; &#125; &#125;&#125;[root@localhost conf]# curl www.yunjisuan.com #配置成功you are welcome[root@localhost conf]# curl bbs.yunjisuan.com192.168.131.132 hahahaha 2.8 Nginx状态信息功能检测2.8.1 确认编译时是否设定了此模块12345[root@localhost nginx-1.10.2]# ./configure --user=nginx --group=nginx --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module &amp;&amp; make &amp;&amp; make install #编译安装等待编译......##说明--with-http_stub_status_module模块就是状态信息模块 2.8.2 设定信息模块配置1234567891011121314151617181920212223242526272829303132333435363738394041[root@localhost conf]# vim extra/status.conf #创建状态信息配置文件server &#123; listen 80; server_name status.yunjisuan.com; location / &#123; stub_status on; access_log off; allow 192.168.131.132; deny all; &#125; &#125;[root@localhost ~]# vim /usr/local/nginx/conf/nginx.conf worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; include extra/bbs.conf; include extra/www.conf; include extra/status.conf; #在主配置文件引入 server &#123; listen 80 default_server; server_name blog.yunjisuan.com; location / &#123; return 403; &#125; &#125;&#125;[root@localhost conf]# /usr/local/nginx/sbin/nginx -s reload #平滑重启[root@localhost conf]# vim /etc/hosts #添加映射127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.131.132 www.yunjisuan.com bbs.yunjisuan.com blog.yunjisuan.com status.yunjisuan.com 测试访问 12345[root@localhost conf]# curl status.yunjisuan.comActive connections: 1 #表示Nginx正在处理的活动连接server accepts handled requests 10 10 10 Reading: 0 Writing: 1 Waiting: 0 第一个server表示Nginx启动到现在1共处理了10个连接第二个accepts表示Nginx启动到现在共成功创建了10次握手请求丢失数&#x3D;（握手数-连接数），可以看出，本次状态显示没有丢失请求。第三个handled requests,表示总共处理了10次请求。Reading为Nginx读取到客户端的Header信息数Writing为Nginx返回给客户端的Header信息数Waiting为Nginx已经处理完正在等候下一次请求指令的驻留连接。在开启keep-alive的情况下，这个值等于active - （reading+writing） 2.9 增加错误日志1234567891011121314worker_processes 1;error_log logs/error.log; #非常简单，一般增加此行即可events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; include extra/www.conf; include extra/mail.conf; include extra/status.conf;&#125; Nginx访问日志轮询切割脚本 1234567891011121314[root@localhost nginx]# cat /server/scripts/cut_nginx_log.sh #!/bin/bash#日志切割脚本可挂定时任务，每天00点整执行Dateformat=`date +%Y%m%d`Basedir=&quot;/usr/local/nginx&quot;Nginxlogdir=&quot;$Basedir/logs&quot;Logname=&quot;access&quot;[ -d $Nginxlogdir ] &amp;&amp; cd $Nginxlogdir || exit 1[ -f $&#123;Logname&#125;.log ] || exit 1/bin/mv $&#123;Logname&#125;.log $&#123;Dateformat&#125;_$&#123;Logname&#125;.log$Basedir/sbin/nginx -s reload[root@localhost nginx]# cat &gt;&gt;/var/spool/cron/root &lt;&lt; KOF#cut nginx access log by Mr.chen00 00 * * * /bin/bash /server/scripts/cut_nginx_log.sh &gt;/dev/null 2&gt;&amp;1 三，Nginx location3.1 location匹配示例1234567891011121314151617181920212223[root@localhost nginx]# cat /usr/local/nginx/conf/extra/www.conf server &#123; listen 80; server_name www.yunjisuan.com; location / &#123; return 401; &#125; location = / &#123; return 402; &#125; location = /images/ &#123; return 501; &#125; location /documents/ &#123; return 403; &#125; location ^~ /images/ &#123; return 404; &#125; location ~* \\.(gif|jpg|jpeg)$ &#123; return 500; &#125; &#125; 说明location是nginx非常重要的过滤函数，可以通过正则表达式来匹配到不同url里的uri部分。匹配到不同的uri进入到不同的location，也就可以进到不同的网页。 3.2 匹配的优先顺序如下 顺序 匹配标识的location 匹配说明 1 “ location &#x3D; &#x2F; { “ &#x3D;是精确匹配，没有代表模糊匹配 1 “ location &#x3D; &#x2F;images&#x2F; { “ 精确匹配 &#x2F;images&#x2F; ，缺少什么都不行 2 “ location ^~ &#x2F;images&#x2F; { “ 先进行字符串的前缀匹配，如果匹配到就不做正则匹配检查。^代表特殊正则 3 “ loction ~* .(gif | jpg | jpeg)$ { “ 正则匹配，*为不区分大小写，正则匹配只要含有就被匹配 4 “ location &#x2F;documents&#x2F; { “ 前缀型匹配，必须从根开始，才能被匹配。虽然是模糊匹配，但是必须在uri起始位置 5 “ location &#x2F; { “ 所有location都不能匹配后的默认匹配原则 提示：优先级一至五。正则匹配高于字符串匹配，低于特殊正则匹配。精确匹配（&#x3D;）&gt; 特殊正则（^）&gt; 普通正则（或~*）&gt;字符串前缀匹配 &gt; 默认匹配（&#x2F;） 四，Nginx rewrite4.1 什么是Nginx rewrite？ 和Apache等Web服务软件一样，Nginx rewrite的主要功能也是实现URL地址重写。 4.2 Nginx rewrite 语法（1）rewrite指令语法 指令语法：rewrite regex replacement 【flag】；默认值：none应用位置：server，location，ifrewrite是实现URL重写的关键指令，根据regex（正则表达式）部分的内容，重定向到replacement部分，结尾是flag标记。下面是一个简单的URL rewrite跳转例子： 1rewrite ^/(.*) http://www.baidu.com/$1 permanent; （2）regex常用正则表达式说明 （3）rewrite指令的最后一项参数flag标记的说明 flag标记符号 说明 last 本条规则匹配完成后，继续向下匹配新的location URI规则（必须写在location外面） break 本条规则匹配完成即终止，不在匹配后面的任何规则 redirect 返回302临时重定向，浏览器地址栏不会显示跳转后的URL地址 permanent 返回301永久重定向，浏览器地址栏会显示跳转后的URL地址 4.3 Nginx rewrite 的企业应用场景Nginx的rewrite功能在企业里应用非常广泛： 可以调整用户浏览的URL，使其看起来更规范，合乎开发及产品人员的需求。 为了让搜索引擎收录网站内容，并让用户体验更好,企业会将动态URL地址伪装成静态地址提供服务 网站换新域名后，让旧域名的访问跳转到新的域名上，例如：让京东的360buy换成了jd.com 根据特殊变量，目录，客户端的信息进行URL跳转等 4.4 Nginx rewrite 301 跳转 以往我们是通过别名方式实现yunjisuan.com和www.yunjisuan.com访问同一个地址的，事实上，除了这个方式外，还可以使用nginx rewrite 301 跳转的方式来实现。实现的配置如下： 12345678910111213141516171819202122232425262728293031[root@localhost nginx]# cat conf/extra/www.conf #www virtualhost by Mr.chen server &#123; listen 80; server_name www.yunjisuan.com; root /var/www/html/wwwcom; location / &#123; index index.html index.htm; &#125;# location = / &#123;# return 402;# &#125; location = /images/ &#123; return 501; &#125; location /documents/ &#123; return 403; &#125; location ^~ /images/ &#123; return 404; &#125; location ~* \\.(gif|jpg|jpeg)$ &#123; return 500; &#125; &#125; server&#123; listen 80; server_name yunjisuan.com; rewrite ^/(.*) http://www.yunjisuan.com/$1 permanent; #当用户访问yunjisuan.com及下面的任意内容时，都会通过这条rewrite跳转到www.yunjisuan.com对应的地址 &#125; 客户端访问测试结果 4.5 实现不同域名的URL跳转示例：实现访问http://mail.yunjisuan.com时跳转到http://www.yunjisuan.com/mail/yunjisuan.html 外部跳转时，使用这种方法可以让浏览器地址变为跳转后的地址，另外，要事先设置http://www.yunjisuan.com/mail/yunjisuan.html有结果输出，不然会出现401等权限错误。 （1）配置Nginx rewrite规则 12345678910111213[root@localhost nginx]# cat conf/extra/mail.conf server &#123; listen 80; server_name mail.yunjisuan.com; location / &#123; root /var/www/html/mailcom; index index.html index.htm; &#125; if ( $http_host ~* &quot;^(.*)\\.yunjisuan\\.com$&quot;) &#123; set $domain $1; rewrite ^(.*) http://www.yunjisuan.com/$domain/yunjisuan.html break; &#125;&#125; 说明：$1相当于$?，只能使用一次，值会变，所以用set，把$1的值赋给$domain，使之值的内容固定不便。 客户端访问测试 4.6 Nginx访问认证 使用账号密码才可以访问网站的功能主要应用在企业内部人员访问的地址上，例如：企业网站后台，MySQL客户端phpmyadmin，企业内部的CRM，WIKI网站平台。 4.6.1 创建密码文件我们可以借用apache的htpasswd软件，来创建加密的账号和密码 123456[root@localhost ~]# which htpasswd/usr/bin/htpasswd[root@localhost ~]# htpasswd -bc /usr/local/nginx/conf/htpasswd yunjisuan 123123Adding password for user yunjisuan[root@localhost ~]# cat /usr/local/nginx/conf/htpasswd yunjisuan:D3gxGm9PhHG/2 #账号密码是加密的（htpasswd是文件的名字） 4.6.2 在虚拟主机配置文件里加入两条配置信息123456789101112131415[root@localhost ~]# vim /usr/local/nginx/conf/extra/status.conf server &#123; listen 80; server_name status.yunjisuan.com; location / &#123; stub_status on; access_log off; auth_basic &quot;welcome to 161&quot;; #加入这条配置 auth_basic_user_file /usr/local/nginx/conf/htpasswd; #加入这条配置 &#125; &#125; #配置解释：auth_basic :验证的基本信息选项（后边跟着的双引号里就是验证窗口的名字）auth_basic_user_file ：验证的用户文件（后边根账号密码文件的绝对路径） 4.6.3 网页登陆验证 登陆成功 4.7 访问Nginx时出现状态码“403 forbidden”的原因（1）Nginx配置文件里没有配置默认首页参数，或者首页文件在站点目录下没有如下内容： index index.php index.html index.htm; （2）站点目录或内部的程序文件没有Nginx用户访问权限 （3）Nginx配置文件中设置了allow，deny等权限控制，导致客户端没有访问权限。 开启权限方法 1234567891011[root@localhost ~]# vim /usr/local/nginx/conf/extra/www.conf server &#123; listen 80; server_name www.yunjisuan.com; autoindex on; #开启权限的参数 location / &#123; root html; index index.html index.htm; &#125; &#125; 说明：添加开启权限的参数之后，当用户访问的界面找不到时，就会从根下给用户看网页，并可以下载，会引发安全问题。nginx默认关闭，apache默认开启。 演示： 先删除网页文件 1234567[root@localhost ~]# cd /usr/local/nginx/html/[root@localhost html]# lsbbs blog index.html[root@localhost html]# rm -rf index.html [root@localhost html]# ls bbs blog 访问网页：显示报错403而添加了uri之后就会报错404 开启权限参数之后再访问： 说明：会把网站从根开始所有的网页都给用户看，可以下载。所以403报错是被安全机制拒绝。404报错是找不到页面。程序用户对index.html没有权限，也会报错403。存放网页的目录没有执行权限也会报错403。 实现最简单的网页跳转： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@localhost nginx]# vim conf/nginx.conf worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name www.zhaoshuo.com; location / &#123; root html/www; index index.html index.htm; &#125; &#125; server &#123; listen 80; server_name mail.zhaoshuo.com; location / &#123; rewrite ^/(.*) http://www.zhaoshuo.com/mail/$1 permanent; &#125; &#125; server &#123; listen 80; server_name bbs.zhaoshuo.com; location / &#123; rewrite ^/(.*) http://www.zhaoshuo.com/bbs/$1 permanent; &#125; &#125; server &#123; listen 80; server_name org.zhaoshuo.com; location / &#123; rewrite ^/(.*) http://www.zhaoshuo.com/org/$1 permanent; &#125; &#125;&#125;[root@localhost nginx]# ls html/www/bbs index.html mail org[root@localhost nginx]# ls html/www/mail/index.html[root@localhost nginx]# ls html/www/org/index.html[root@localhost nginx]# ls html/www/bbs/index.html","categories":[{"name":"架构基础","slug":"架构基础","permalink":"https://kkabuzs.github.io/categories/%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://kkabuzs.github.io/tags/Nginx/"}]},{"title":"http协议原理","slug":"httpxieyiyuanli","date":"2018-08-14T02:46:43.000Z","updated":"2018-08-14T02:46:43.000Z","comments":true,"path":"articles/2018/08/14/httpxieyiyuanli/","permalink":"https://kkabuzs.github.io/articles/2018/08/14/httpxieyiyuanli/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 http协议原理一，Web服务基础1.1http服务重要基础1.1.1 用户访问网站基本流程 第一步：用户在浏览器里输入网址 第二步：域名解析出ip地址（DNS） 第三步：浏览器发起一个http协议的请求 第四步：web服务器响应用户的请求（域名看的是首页） 第五步：用户的浏览器解析web服务器相应的数据包 第六步：用户就从浏览器里看到了内容 1.1.2DNS解析原理 DNS就是区域域名服务。 DNS的递归查询过程（前6步） 第一步：当我们输入了一个域名，先去内存里看有没有映射（映射的格式就是IP+域名） 第二步：如果内存里没有，就会找本地映射文件hosts 第三步：如果hosts文件里没有，就会找本地DNS去要（LDNS，L是local的意思） 第四步：DNS也在内存里找，如果找到，就会记录在内存里 第五步：如果内存里找不到，DNS就会找映射文件 第六步：映射文件里没有，DNS就会去域名记录本文件里查找（域名记录本文件就是文件里记录了哪个域名对应哪个IP） 第七步：还找不到，就要请外援。（这步开始就叫迭代查询） DNS迭代查询过程 全世界公开的最大的DNS服务器有13个，这种服务器叫做“点”服务器。DNS解析域名的顺序是从右往左，先辨别.com&#x2F;.cn&#x2F;.xxx之类的后缀。 第一步：LDNS会从DNS系统的（.）根开始请求www.baidu.com域名的解析，根服务器下面是没有www.baidu.com域名解析记录的，但是根下面有www.baidu.com对应的顶级域.org的解析记录，因此，根会把.org对应的DNS服务器地址返回给LDNS。 第二步：LDNS获取到.org对应的DNS服务器地址后，就会去.org服务器请求www.baidu.com域名的解析，而.org服务器下面也没有www.baidu.com域名对应的解析记录,但是有baidu.com域名的解析记录，因此，.org服务器会把baidu.com对应的DNS服务器地址返回给LDNS 第三步：同理，LDNS获取到baidu.com对应的DNS服务器地址后，就会去baidu.org服务器请求www.baidu.com域名的解析，baidu.com域名对应的DNS服务器是该域名的授权DNS服务器，这个DNS服务器正是企业购买域名时管理解析所在的服务器（也可能是自建的授权DNS服务器），这个服务器会有www.baidu.com对应的IP解析记录，如果此时没有，就表示企业的域名人员没有为www.baidu.com域名做解析，即网站还没架设好。 第四步：baidu.com域名DNS服务器会把www.baidu.com对应的IP解析记录发给LDNS 第五步：用户就访问到了www.baidu.com网站。 1.2http协议 HTTP协议，为超文本传输协议，是互联网中最常用的一种网络协议。HTTP的重要应用之一是WWW服务。 HTTP协议是互联网上常用的通信协议之一。 WWW，全称World Wide Web，常称为Web，中文译为“万维网”。它是目前互联网上最受用户欢迎的信息服务形式。HTTP协议的WWW服务应用的默认端口为80（端口的概念），另外的一个加密的WWW服务应用https的默认端口为443，主要用于网银，支付等和钱相关的业务。 1.2.2 常见的HTTP请求方法 HTTP方法 作用描述 GET（读） 客户端请求指定资源信息，服务器返回指定资源 HEAD 只请求响应报文中的HTTP首部（单纯的看http数据包的头部） POST（写） 将客户端的数据提交到服务器，例：注册表单（提交请求） PUT 从客户端向服务器传送的数据取代指定的文档内容 DELETE 请求服务器删除Request-URI所标识的资源 MOVE 请求服务器将制定的页面移至另一个网络地址 http数据包由头部和主体构成，头部用来装http状态码，主体就是网页。 1.2.3 HTTP状态码 HTTP状态码是用来表示Web服务器响应http请求状态的数字代码。每当Web客户端向Web服务器发送一个HTTP请求时，Web服务器都会返回一个状态响应代码。这个状态码是一个三位数字代码，作用是告知Web客户端此次的请求是否成功，或者是否要采取其他的动作方式。 HTTP协议1.1版本中的状态吗可以分为五大类，如下表： 状态码范围 作用描述 100-199 用于指定客户端相应的某些动作 200-299 用于表示请求成功 300-399 用于已经移动的文件并且常被包含在定位头信息中指定新的地址信息 400-499 用于指出客户端的错误 500-599 用于指出服务器端的错误 生产场景常见的状态吗及其对应的作用 状态代码 详细描述说明 200～OK 服务器成功返回网页，这是成功的http请求，返回的标准状态码 301-Moved Permanently 永久跳转，所有请求的网页将永久跳转到被设定的新的位置，例如：从baidu.com跳转到www.baidu.com 403-Forbidden 禁止访问，这个请求是合法的，但是服务器端因为匹配了预先设置的规则而拒绝响应客户端的请求，此类问题一般为服务器或服务权限配置不当所致。 404-Not Found 服务器找不到客户端请求的指定页面，可能是客户端请求了服务器上不存在的资源导致 500-Internal Server Error 内部服务器错误，服务器遇到了意料不到的情况，不能完成客户的请求。这是一个较为笼统地报错，一般为服务器的设置或者内部程序问题导致。例如SElinux开启，而又没有为http设置规则许可，客户端访问就是500 502-Bad Gateway 坏的网关，一般是代理服务器请求后端服务时，后端服务不可用或没有完成响应网关服务器。一般为反向代理服务器下面的节点出问题导致。 503-Service Unavailable 服务当前不可用，可能因为服务器超载或停机维护导致，或者是反向代理服务器后面没有可以提供服务的节点 504-Gateway Timeout 网关超时，一般是网关代理服务器请求后端服务时，后端服务没有在特定的时间内完成处理请求，一般是服务器过载导致没有在指定的时间内返回数据给前端代理服务器。 1.2.4 HTTP响应报文介绍HTTP响应报文由起始行，响应头部，空行和响应报文主体几个部分组成，和HTTP请求报文格式类似。报文如下表： 报文格式 报文信息 起始行 协议及版本号 数字状态码 状态信息 响应头部 字段1:值1 字段2：值2 空行 空白内容 响应报文主体 就是html的网页 下面对响应报文的每个部分逐一阐述： （1）起始行 响应报文的起始行，也叫状态行，用来说明服务器响应客户端请求的状况。一般为协议及版本号，数字状态码，状态情况。例如：HTTP&#x2F;1.1 200 OK （2）响应头部 和请求报文类似，起始行的后面一般有若干个头部字段。每个头部字段都包含一个名字和一个值，两者之间用冒号分隔。头部结尾也是以一个空行结束。 （3）空行 最后一个响应头部信息之后是一个空行，发送回车符和换行符，通知客户端空行下文无头部信息。 （4）响应报文主体 响应报文主体中装载了要返回给客户端的数据。这些数据可以是文本，也可以是二进制的（图片，视频）。 1.2.5 HTTP协议原理及重点分析 HTTP协议属于OSI模型中的第七层应用层协议，HTTP协议的重要应用就是WWW服务应用，下面就以WWW服务应用为例介绍HTTP协议的通信原理了，HTTP协议进行通信时，需要有客户端（即终端用户）和服务端（即Web服务器），在Web客户端向Web服务器发送请求报文之前，先要通过TCP&#x2F;IP协议在Web客户端和服务器之间建立一个TCP&#x2F;IP连接。 过程： 用户访问网站的流程 DNS解析流程细节 建立TCP连接过程（TCP&#x2F;IP三次握手原理知识）（11种状态） 发送HTTP报文及HTTP请求报文内容细节。 Web服务器响应客户端请求处理细节（网站集群架构细节） 响应HTTP报文及HTTP响应报文的细节 关闭TCP连接，涉及TCP&#x2F;IP协议四次挥手原理 重要知识点汇总 http协议位于OSI模型中第7层应用层 http协议的重要应用是www服务。 用户上网流程，DNS解析原理流程 DNS解析获取的IP后，建立TCP连接，然后发送http请求细节和服务器响应细节。 HTTP请求报文与HTTP响应报文知识 到达HTTP服务后请求后端集群节点流程为Nginix–&gt;fastcgi–&gt;PHP–&gt;(数据库，存储等) TCP&#x2F;IP协议三次握手和四次挥手原理。 1.3 http资源1.3.1 媒体类型 互联网上的数据有很多不同的数据类型，Web服务器会把通过Web传输的每个对象都打上名为MIME类型（MIME Type）的数据格式标签。 当Web服务器响应HTTP请求时，会为每一个HTTP对象数据加一个MIME类型，当Web浏览器获取到服务器返回的对象时，会去查看相关的MIME类型，进行相应处理。 MIME类型存在于HTTP响应报文的响应头部信息里，它是一种文本标记，表示一种主要的对象类型和一个特定的子类型，中间由一条斜杠来分隔。 MIME类型 文件类型 text&#x2F;html html htm shtml文本类型 text&#x2F;css css文本类型 text&#x2F;xml xml文本类型 image&#x2F;gif gif图像类型 image&#x2F;jpeg jpeg jpg图像类型 application&#x2F;javascript js文本类型 text&#x2F;plain txt文本类型 application&#x2F;json json文本类型 text.plain txt文本类型 application&#x2F;json json文本类型 video&#x2F;mp4 mp4视频类型 video&#x2F;quicktime mov视频类型 video&#x2F;x-flv flv视频类型 video&#x2F;x-ms-wmv wmv视频类型 video&#x2F;x-msvideo avi视频类型 说明PC电脑浏览器上显示的网站，是web前端做的。用户直接接触的客户端就是前端。1，前端包括：web开发，安卓开发，ios开发，VR开发。 web开发用到了什么？(1和2写的网页是静态网页，3写的是动态网页)（1），html语言：决定了你的浏览器上显示的具体内容（2），CSS语言：决定了内容的显示格式，例如图片是方的还是圆的。（3），JavaScript语言：不是java。2，后端语言包括：PHP，java，python（前后端都有），c++，c#（前后端都有），Golang 1.3.2 URL介绍 URL，全称Uniform Resource Location，中文翻译为统一资源定位符，就叫（用户的）请求，也被称为网页地址（网址）。如同在网络上的门牌，它是因特网上标准的资源唯一地址。 URL的组成 URL是由域名+后边的部分(页面的存储路径，这个部分成为URi）（URL&#x3D;域名+URi） 1.3.4 静态网页资源静态网页资源介绍 在网站设计中，纯粹HTML格式的网页（可以包含图片，视频，JS（前端功能实现），CSS（样式）等）通常被称为“静态网页”，早期的网站大多都是静态的。静态网页是相对于动态网页而言的，是指没有后台数据库，不含程序（如php，jsp，asp(C#)）和可交互的网页。 静态网页资源特点 静态网页资源的特点是，开发者编写的是什么，它显示的就是什么，一旦编写完成，就不会有任何改变。静态网页的维护和更新相对比较麻烦，每个不同的网页都需要单独编集更新，静态网页一般适用于更新较少的宣传展示型网站（例如：酒，家具，猪饲料等的宣传网站），是早期很多中小网站展示的形式。 静态网页资源的对应程序及资源文件的常见扩展名为：纯文本类程序或文件，如htm，html，xml，shtml，js，css等图片类文件或数据文档，如jpg，gif，png，bmp，txt，doc，ppt等视频类流媒体文件，如mp4，swf，avi，wmv，flv等 静态网页资源有几个重要的特征： 每个网页都有一个固定的URL地址，且URL一般以.html,.html,shtml等常见形式为后缀，而且地址中不含邮问号“？”或“&amp;”等特殊符号。 网页内容一经发布到网站服务器上，无论是否有用户访问，每个网页的内容都是保存在网站服务器文件系统上的，也就是说，静态网页是实实在在保存在服务器上的文件实体，每个网页都是一个独立的文件。 网页内容是固定不变的，因此，容易被搜索引擎收录（容易被用户找到）（优点） 网页没有数据库支持，在网站制作和维护方面工作量较大，因此当网站信息量很大时完全依靠静态网页制作的方式比较困难（缺点） 网页的交互性较差，在程序功能实现方面有较大的限制（缺点） 网页程序在用户浏览器端解析，如IE浏览器，程序解析效率很高，由于服务端不进行解析，并且不需要读取数据库，因此服务器端可以接受更多的并发访问。当客户端向服务器请求数据时，服务器直接把数据从磁盘文件系统上返回（不做任何解析），待客户端拿到数据后，在浏览器端解析展现出来（优点) 静态网页的核心特点，如下：1）程序在客户浏览器端解析，不读取后端数据库，因此性能和效率很高。2）因为后端没有数据库支持，所以和用户的交互性较差，功能实现也很少。 有关静态网页的架构思想 在高并发，高访问量的场景下做架构优化，涉及的关键环节就是把动态网页转成静态网页，而不直接请求数据库和动态服务器，并且可以把静态内容推送到前端缓存（或CDN）中提供服务，这样就可以提升用户体验，节约服务器和维护成本。 1.3.5 动态网页资源动态网页资源介绍 所谓的动态网页是与静态网页相对而言的，也就是说，动态网页的URL后缀不是.htm,.html,.shtml,.xml,.js,.css等静态网页的常见后缀扩展名形式，而是以.asp,.aspx,.php,.js,.do,.cgi等形式作为后缀的，并且一般在动态网页网址中会有标志性的符号–“？，&amp;”，此外，在大多数情况下后端都需要有数据库支持等。 动态网页资源特点 1）网页扩展名后缀常见为：.asp,.aspx,.php,.jsp,.do,cgi等2）网页一般以数据库技术为基础，大大降低了网站维护的工作量3）采用动态网页技术的网站可以实现更多的功能，如用户注册，用户登录，在线调查，投票，用户管理，订单管理，发博文等等4）动态网页并不是独立存在于服务器上的网页文件，当用户请求服务器上的动态程序时，服务器解析这些程序并可能读取数据库返回一个完整的网页内容。5）动态网页中的“？”在搜索引擎的收录方面存在一定问题，搜索引擎一般不会从一个网站的数据库中访问全部网页，或者出于技术等方面的考虑，搜索蜘蛛一般不会去抓去网址中“？”后面的内容，因此在企业通过搜索引擎进行推广时，需要针对采用动态网页的网站做一定的技术处理（伪静态技术），以便适应搜索引擎的抓取要求。6）程序在服务器端解析，这相当于顾客点餐，饭店厨师做饭做菜，耗时长，效率低。由于程序在服务端解析，因此，会消耗大量的CPU和内存，I&#x2F;O等资源，并且多数还要读取数据库等服务，因此，其访问效率远不如静态网页，在服务端解析动态程序的服务常见的有PHP引擎，Java容器（tomcat,resin,jboss,weblogic） 有关动态网页的架构思想 一般来说，静态网页的性能效率是动态网页的10～30倍。且动态网站效率很差，并发能力也很低，在高并发场景中，应尽可能转换成静态网页提供服务。动态转静态几乎是所有高并发网站必备的架构方案思路，也是高级架构师的职责所在。 此外，动态转静态也要根据业务需求设计，例如，对于更新频繁的网站如果设计不好就可能会产生数据不一致的情况，即用户看到的数据不是网站最新的内容，而是静态的内容。 1.3.6 生产Web架构优化实战方案由于静态网页程序在客户端解析，大大降低了服务器端的访问压力，因此解析效率更高，在实际高并发网站架构中，可以考虑把用户请求的数据解析后存成静态文件放于磁盘中或放于内存中，来降低动态服务器的压力，节约企业成本，提升用户体验。 （1）门户新闻业务 新闻网站的特点是一旦发布完成，几乎不会再改动网页内容。因此，对于新闻业务内容的静态化相对比较简单。 程序&#x3D;&#x3D;&#x3D;&gt;生成静态页面 第一步：程序要支持发布动态内容转成静态功能 第二步：运营编辑人员发布新闻网页后，后台程序立刻将动态网页生成静态文件。 第三步：运维人员通过发布或事件触发把运营编辑生成的静态网页发布到事先搭建好的公司缓存集群服务器上，或者把静态内容同步到购买的全国所有CDN服务器节点上，然后，再提供给用户访问浏览。 （2）视频网站业务 视频网站和新闻网站类似，特点都是一旦发布完成，几乎不会再改动网页内容。因此，实现视频业务网站高效访问也很简单。 以优酷视频网为例，用户在上传视频时，需要经历转码–&gt;审核的过程（大概1小时），然后一些热点视频也可能会被提前推送同步到CDN的核心节点或全国所有CDN服务器节点，用户访问时才会更快。 （3）Blog&#x2F;BBS&#x2F;SNS&#x2F;微博社区业务&#x2F;电商（如淘宝，京东） 这几类业务的动态转静态是比较困难的，因为，用户发布完成内容，可能会随时更新并查看，这种情况一般会通过异步方式，例如消息中间件技术加上NoSQL集群技术实现转换，当然也会改进产品细节，例如：在访问的环节设置延时，异步加载等手段。 1.4 网站流量度量术语1.4.1 IP（internet Protocol） IP(独立IP)即Internet Protocol，这里指独立IP数，独立IP数是指不同IP地址的计算机访问网站时被计算的总次数。独立IP数是衡量网站流量的一个重要指标。一般一天内（00:00- 24:00）相同IP地址的客户端访问网站页面只被计算为一次，记录独立IP的时间可为一天或一个月，目前通用的标准为“一天”。 1.4.2 PV（Page View） PV（访问量）即Page View，中文翻译为页面浏览，即页面浏览器或点击量，不管客户端是不是相同，也不管IP是不是相同，用户每次访问一个网站页面都会被计算一个PV。 PV的具体度量方法就是从客户浏览器发出一个对Web服务器的请求（Request），Web服务器接到这个请求后，将该请求对应的一个网页（Page）发送给浏览器，就产生一个PV。这里有一个问题，就是只要这个请求发送给了浏览器，无论这个页面是否完全打开（或下载完成），那么都是会被计数为1个PV（服务器日志），一般为了防止用户快速刷PV，很多网站把PV的统计程序放在页面的最下面。 用PV衡量网站时，PV数反映的是浏览某网站的页面数量，每刷新一次页面也算一次。因此，可以说PV数与来访用户的数量成正比，但PV数并不是真正的页面来访者数量，而是网站被访问的页面数量，因为一个来访者可能产生多个PV。 说明PV（Page View）是网站被访问的页面数量的一个指标，但不能直接知道有多少人访问了这个网站。一个来访者访问网站，可能产生若干PV数，但是独立IP数就只有1个，因此，如果对比一个网站的独立IP数和PV数，不难看出，PV数一定会大于等于独立IP数的 1.4.3 UV(Unique Visitor) UV(独立访客)即Unique Visitor，同一台客户端（PC或移动端）访问网站被计算为一个访客。一天（00:00-24:00）内相同的客户端访问同一个网站只计算一次UV。UV一般是以客户端Cookie等技术作为统计依据的，实际统计会有误差。 考虑到一台客户端电脑可能会有多人使用的情况，因此，UV（独立访客）实际上并不一定是独立的自然人访问。1.http请求报文：浏览器版本，OS2.http响应报文：cookie（id） 1.4.4 企业网站对IP，PV，UV的度量 先来看对IP的度量： 分析所有Web服务器的访问日志信息，对IP地址段去重后计数，这是IT人员的基本计算手段 在网站的每一个（所有）页面结尾，嵌入JS等统计程序代码，待用户加载网页后，IP即传给统计IP的服务器，这种方法一般被第三方统计公司或企业内部开发日志分析程序时使用 用第三方大家比较信任的统计工具例如：谷歌的统计（GA）。 IP的统计方法简单，易用，因此，成为了多数网站衡量网站流量的重要指标之一。 对PV的度量如下： 分析Web服务的访问日志（需要排除js，css及各种图片的日志信息），只计算HTML，PHP等页面数量。 在网站的每一个页面结尾，嵌入JS等统计程序代码，待用户加载网页后，访问数量即传给统计PV的服务器，这种方法一般被第三方统计公司或在企业内部开发日志分析程序时使用。 用第三方大家比较信任的统计工具例如：谷歌的统计（GA） PV的统计方法也很简单，易用，因此，也是多数网站衡量网站流量的重要指标之一。 对于UV的度量如下： 通过客户端HTTP请求报文分析 一个客户端会多次请求网站服务器，每次HTTP请求都会携带客户端自身的大量信息，比如：IP地址，请求发出时间，浏览器版本，操作系统版本等等。网站服务器对这些请求进行分析，如果这些请求满足一些共同特征，比如来自同一个IP地址，且浏览器版本和操作系统版本相同，请求时间又相近等，那么就可以认为这些是来自于同一个客户端，那么多个页面访问也只算一个UV。共同特征的定义是由服务器方决定的。通常，用IP地址+其他特征共同来定义的情况较多。但此种度量方法无法解决以下问题，例如：多个人的电脑软硬件经常雷同，并且是一个公司或者学校的人；多个人共用一个电脑的情况。 通过cookie鉴别 当客户端第一次访问某个网站服务器的时候，网站服务器会给这个客户端的电脑发出一个cookie，通常放在这个客户端电脑的C盘当中。在这个cookie中会分配一个独一无二的编号，这其中会记录一些访问服务器的信息，如访问时间，访问了哪些页面，等等。当你下次再访问这个服务器的时候，服务器就可以直接从你的电脑中找到上一次放进去的Cookie文件，并且对其进行一些更新，但那个独一无二的编号是不会变的。如果在一定时间内，服务器发现2个来访者对应的是一个编号，那么自然可以认为它来源于同一个来访者了，于是就计算1个UV。 使用Cookie的方法要比分析客户端HTTP请求头部信息分析更精确些。但也存在一些问题，比如：有的客户端为保证更高级别的安全，关闭了Cookie的功能；或者是有些客户端设置了在退出页面时自动删除Cookie，亦或你经常自己去手动删除Cookie，那么这个方法就不那么精确了。 1.4.5 IP,PV,UV的区别 一个网站的独立IP数量要比网站实际访问的PV数量小的多。通常情况下（国内互联网环境），网站的UV数也会大于独立IP数。 PV数高说明访问的页面数多，但是不一定就代表来访者多：但PV数一定与来访者的数量成正比，不过，PV并不直接决定页面的真实来访者数量。比如在访问某网站时，一个人也可通过不断的刷新页面，制造出非常高的PV数。PV数多，用户访问网站页面的总数量多，通常服务器的压力会大一些。 1.4.6 并发连接统计并发连接方法(ss命令等于netstat命令)：ss -an | grep ESTAB | wc -l 网站并发连接 那么到底什么事并发？怎么理解并发呢？A种理解：网站服务器每秒能够接收的最大用户请求数B种理解：网站服务器每秒能够响应的最大用户请求数。C种理解：网站服务器在单位时间内能够处理的最大连接数。虽然A，B的理解占IT人员中的大多数，但是，C理解更为准确一些。 如何测试磁盘的存储性能？ 1.连续的读写向磁盘中写入大的文件dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;tmp&#x2F;test01.bin bs&#x3D;1K count&#x3D;10000 1.5 www服务软件介绍1.5.1 当前互联网主流Web服务说明常用来提供静态Web服务的软件： Apache：这是中小型Web服务的主流，Web服务器中的老大哥。 Nginx：大型网站Web服务主流，曾经Web服务器中的初生牛犊，现已长大。Nginx的分支tengine（http://tengine.taobao.org/）目前也在飞速发展。 常用来提供动态服务的软件 PHP（fastcgi）：大中小型网站都会使用，动态网页语言PHP程序的解析容器。它可配合Apache解析动态程序，不过，这里的PHP不是Fastcgi守护进程模式，而是mod_php5.so(module).也可配合Nginx解析动态程序，此时的PHP常用Fastcgi守护进程模式提供服务。 tomcat:中小企业动态Web服务主流，互联网Java容器主流（如jsp，do） resin：大型动态Web服务主流，互联网Java容器主流（如jsp，do） IIS（internet information services）：微软Windows下的Web服务软件（asp，.aspx） 1.6重点 用户访问网站基本流程 DNS系统解析原理 HTTP协议通信原理，包括HTTP协议，请求报文，响应报文，状态码等相关知识 动态，静态概念特点以及伪静态技术； 动态转静态Web优化方案 IP，PV，UV的概念和区别 并发的概念理解 了解常用www服务软件特点，如Apache，Nginx，PHP（Fastcgi），tomcat，Resin等","categories":[{"name":"架构基础","slug":"架构基础","permalink":"https://kkabuzs.github.io/categories/%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://kkabuzs.github.io/tags/HTTP/"}]},{"title":"Rsync数据同步工具","slug":"rsync","date":"2018-07-25T08:33:44.000Z","updated":"2018-07-25T08:33:44.000Z","comments":true,"path":"articles/2018/07/25/rsync/","permalink":"https://kkabuzs.github.io/articles/2018/07/25/rsync/","excerpt":"","text":"无论你跑多少距离，放弃很容易，停下来，肉体得到暂时的舒服，但你最终会一无所得。坚持或许很难，但你最后一定会有所收获，跑过的路，永远不会欺骗自己。 一，Rsync介绍1.1什么是Rsync？ Rsync是一款开源的，快速的，多功能的，可实现全量及变量的本地或远程数据同步备份的优秀工具。 1.2Rsync简介 Rsync具有可是本地和远程两台主机之间的数据快速复制同步镜像，远程备份的功能。这个功能类似ssh带的scp命令，但又优于scp命令的功能。scp每次都是全量拷贝，而Rsync可以增量拷贝。当然，Rsync还可以在本地主机的不同分区的不同目录之间全量及增量的复制数据。这有类似cp命令，但同样也优于cp命令，cp每次都是全量拷贝，而Rsync可以增量拷贝。 小提示：利用Rsync还可以实现删除文件和目录的功能，这又相当于rm命令。 1.3Rsync的特性 支持拷贝特殊文件、设备等。 可以有排除指定文件或目录同步的功能，相当于打包命令tar的排除功能。 可以做到保持原文件的目录的权限、时间、软硬连接、属主、属组等属性均不改变。 可以实现增量同步，即只同步发生变化的数据，因此数据传输效率很高。 可以使用rcp，rsh，ssh等方式来配合传输文件。（rsync本身不对数据加密） 可以通过socket（进程方式）传输文件和数据。（服务端和客户端） 支持匿名的或认证（无需系统用户）的进程模式传输，可实现方便安全的进行数据备份及镜像。 1.4Rsync的企业工作场景说明 两台服务器之间数据同步（定时任务+备份数据） 即：crond + rsync 二，Rsync的工作方式 本地间类似cp命令的复制方式 网络间两太不同服务器间数据传输（类似scp命令的功能），也叫借助rcp，ssh等通道来传输数据。 以守护进程（socket）的方式进行数据传输（这是Rsync自身的重要功能）。也叫以socket进程监听的方式启动rsync server端。 三，Rsync的应用方式第一种，本地间数据传输服务 检查有无rsync软件包12345[root@localhost ~]# mount /dev/sr0 /media/cdrom/mount: block device /dev/sr0 is write-protected, mounting read-only[root@localhost ~]# rpm -qa rsync[root@localhost ~]# yum -y install rsync #没有则安装 Rsync本地数据同步就相当于cp命令rsync /etc/hosts /tmp 12345678910[root@localhost ~]# rsync /etc/hosts /tmp[root@localhost ~]# ls /tmp/hosts[root@localhost ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6[root@localhost ~]# cat /tmp/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 增量复制文件内容1234567[root@localhost ~]# echo &quot;`hostname -I` www.yunjisuan.com&quot; &gt;&gt; /etc/hosts[root@localhost ~]# tail -1 /etc/hosts192.168.131.129 www.yunjisuan.com[root@localhost ~]# rsync /etc/hosts /tmp[root@localhost ~]# tail -1 /tmp/hosts192.168.131.129 www.yunjisuan.com 备注只复制没有的那一部分数据，cp命令是全量复制，所以会提示是否覆盖。 复制目录rsync -avz /root/zhaoshuo /tmp/ 12345678910111213141516171819202122232425262728293031323334353637383940[root@localhost ~]# ls zhaoshuoaaa stu_102999_14.jpg stu_102999_19.jpg stu_102999_4.jpg stu_102999_9.jpgstu_102999_10.jpg stu_102999_15.jpg stu_102999_1.jpg stu_102999_5.jpg yunjisuan.txtstu_102999_11.jpg stu_102999_16.jpg stu_102999_20.jpg stu_102999_6.jpgstu_102999_12.jpg stu_102999_17.jpg stu_102999_2.jpg stu_102999_7.jpgstu_102999_13.jpg stu_102999_18.jpg stu_102999_3.jpg stu_102999_8.jpg[root@localhost ~]# rsync -avz /root/zhaoshuo /tmp/ #/root/zhaoshuo/*就是复制zhaoshuo下所有文件sending incremental file list zhaoshuo/ zhaoshuo/stu_102999_1.jpg zhaoshuo/stu_102999_10.jpgzhaoshuo/stu_102999_11.jpgzhaoshuo/stu_102999_12.jpgzhaoshuo/stu_102999_13.jpgzhaoshuo/stu_102999_14.jpgzhaoshuo/stu_102999_15.jpgzhaoshuo/stu_102999_16.jpgzhaoshuo/stu_102999_17.jpgzhaoshuo/stu_102999_18.jpgzhaoshuo/stu_102999_19.jpgzhaoshuo/stu_102999_2.jpgzhaoshuo/stu_102999_20.jpgzhaoshuo/stu_102999_3.jpgzhaoshuo/stu_102999_4.jpgzhaoshuo/stu_102999_5.jpgzhaoshuo/stu_102999_6.jpgzhaoshuo/stu_102999_7.jpgzhaoshuo/stu_102999_8.jpgzhaoshuo/stu_102999_9.jpgzhaoshuo/yunjisuan.txtzhaoshuo/aaa/sent 2057 bytes received 419 bytes 4952.00 bytes/sectotal size is 944 speedup is 0.38[root@localhost ~]# ls /tmp/zhaoshuo/aaa stu_102999_14.jpg stu_102999_19.jpg stu_102999_4.jpg stu_102999_9.jpgstu_102999_10.jpg stu_102999_15.jpg stu_102999_1.jpg stu_102999_5.jpg yunjisuan.txtstu_102999_11.jpg stu_102999_16.jpg stu_102999_20.jpg stu_102999_6.jpgstu_102999_12.jpg stu_102999_17.jpg stu_102999_2.jpg stu_102999_7.jpgstu_102999_13.jpg stu_102999_18.jpg stu_102999_3.jpg stu_102999_8.jp 参数介绍-a：归档模式，表示以递归方式传输文件，并保持文件的所有属性-v：显示详细信息-z：传输时压缩以提高传输效率 增量同步rsync -avz /root/www/ /tmp/ 1234567891011[root@localhost ~]# touch www/25[root@localhost ~]# ls /root/www1 10 15 2 20 25 3 4 5 6 7 8 9[root@localhost ~]# rsync -avz /root/www/ /tmp/sending incremental file list25sent 158 bytes received 31 bytes 378.00 bytes/sectotal size is 0 speedup is 0.00[root@localhost ~]# ls /tmp/1 10 15 2 20 25 3 4 5 6 7 8 9 特别提示：文中同步命令是rsync -avz &#x2F;root&#x2F;www&#x2F;* &#x2F;tmp的意思 实现源目录和目标保持一致rsync -avz /root/www/ /tmp --delete 12345678910111213141516171819202122232425262728293031323334[root@localhost ~]# touch /tmp/&#123;41..45&#125;[root@localhost ~]# [root@localhost ~]# touch /tmp/&#123;41..45&#125;[root@localhost ~]# ls /tmp/41 42 43 44 45[root@localhost ~]# rsync -avz /root/www/ /tmp --deletesending incremental file list./deleting 45 #源没有，目标有，删除deleting 44deleting 43deleting 42deleting 411 #源有，目标没有，添加10152202533132333435456789sent 842 bytes received 357 bytes 2398.00 bytes/sectotal size is 0 speedup is 0.00 –delete：实现删除的参数，也有同步的意思 第二种，两台服务器之间进行数据同步 演示 注意实验开始前，两台机器都要有rsync软件包和openssh-clients软件包 rsync -avz /root/www root@192.168.131.128:/tmp 123456789101112131415161718192021222324252627[root@localhost ~]# yum -y install openssh-clients #先装ssh远程连接支持包，也是scp命令的包[root@localhost ~]# rsync -avz /root/www root@192.168.131.128:/tmproot@192.168.131.128&#x27;s password: sending incremental file listwww/www/1www/10www/15www/2www/20www/25www/3www/31www/32www/33www/34www/35www/4www/5www/6www/7www/8www/9sent 869 bytes received 358 bytes 350.57 bytes/sectotal size is 0 speedup is 0.00 小贴士root@ 代表对方登陆的账号192.168.200.103 代表对方服务器的ip：&#x2F;tmp 代表对方服务器的目录路径 scp命令远程复制的应用方法scp -r /root/zhaoshuo 192.168.131.128:/tmp-r:递归复制不写root@，就默认以root用户登陆 1234567891011121314151617181920212223[root@localhost ~]# scp -r /root/zhaoshuo 192.168.131.128:/tmproot@192.168.131.128&#x27;s password: 10 100% 0 0.0KB/s 00:00 7 100% 0 0.0KB/s 00:00 11 100% 0 0.0KB/s 00:00 9 100% 0 0.0KB/s 00:00 2 100% 0 0.0KB/s 00:00 6 100% 0 0.0KB/s 00:00 19 100% 0 0.0KB/s 00:00 12 100% 0 0.0KB/s 00:00 3 100% 0 0.0KB/s 00:00 1 100% 0 0.0KB/s 00:00 13 100% 0 0.0KB/s 00:00 17 100% 0 0.0KB/s 00:00 4 100% 0 0.0KB/s 00:00 5 100% 0 0.0KB/s 00:00 &#123;1.20&#125; 100% 0 0.0KB/s 00:00 8 100% 0 0.0KB/s 00:00 18 100% 0 0.0KB/s 00:00 14 100% 0 0.0KB/s 00:00 16 100% 0 0.0KB/s 00:0020 100% 0 0.0KB/s 00:00 15 100% 0 0.0KB/s 00:00 还可以指定端口 12345678[root@localhost ~]# rsync -avz -e &#x27;ssh -p 22&#x27; /root/xxx root@192.168.131.128:/tmproot@192.168.131.128&#x27;s password: sending incremental file listxxxsent 235 bytes received 31 bytes 48.36 bytes/sectotal size is 388 speedup is 1.46 -e ‘ssh -p 22’ 代表指定22端口 修改默认指定端口的方法 123456789101112131415161718[root@localhost ~]# vim /etc/ssh/sshd_config #打开ssh远程连接的配置文件 10 # possible, but leave them commented. Uncommented options change a 11 # default value. 12 13 #Port 22 14 port 8888 #保存退出就等于把端口改为8888了。如果不指定端口号，默认为22 15 #AddressFamily any 16 #ListenAddress 0.0.0.0 17 #ListenAddress :: 18 19 # Disable legacy (protocol version 1) support in the server for new 20 # installations. In future the default will change to require explicit 21 # activation of protocol 1 22 Protocol 2 23 24 # HostKey for protocol version 1 25 #HostKey /etc/ssh/ssh_host_key 小贴士 修改端口，端口号不能用1000以下的，因为默认机制，1000以下的端口都被占用了(注意关闭防火墙服务)。 参数 含义 -P 显示同步过程及传输时的进度等信息 -e 指定端口的参数 -t 保持文件时间信息 -o 保持文件属主信息 -g 保持文件属组信息 -p 保持文件权限 -l 保留软链接 案例某DBA做数据同步，宽带占满，导致用户无法访问网站rsync -avz dbfile 10.0.0.41：/backup 没有给带宽做限制rsync -avz --bwlimit 100 dbfile 10.0.0.41：/backup 限定了带宽 第三种，多组服务器之间，以socket进程的方式启动Rsync进程进行监听 第一步，设置部署环境1.主机网络参数 主机名 网卡eth0 用途 代号 zhaoshuo 192.168.131.129 Rsync服务端 D-server zhaoshuo1 192.168.131.128 Rsync客户端 A-server 2.具体要求 要求在D-server上以rsync守护进程的方式部署rsync服务。使得所有rsync节点客户端主机，可以把本地数据通过rsync的方式备份到数据备份服务器D-server上，客户端为A-server。 第二步，开始部署rsync服务——&gt;Rsync服务端D-server操作过程1.配置rsyncd.conf1）首先确认是否安装rsync软件 123[root@zhaoshuo ~]# rpm -qa rsyncrsync-3.0.6-9.el6_4.1.x86_64 2)创建rsync配置文件（默认不存在） 12345678910111213141516171819[root@zhaoshuo ~]# vim /etc/rsyncd.confuid = rsync #用户gid = rsync #组use chroot = no #程序安全设置（no代表不开启）max connections = 200 #客户端最大连接数timeout = 300 #超时时间pid file = /var/run/rsyncd.pid #进程号文件位置lock file = /var/run/rsyncd.pid #进程锁log file = /var/log/rsyncd.log #日志文件位置[backup] #模块名path = /backup #使用目录（path代表路径）ignore errors #有错误时忽略（意思是有错误也传）read only = false #可读可写（ture或false）list = false #阻止远程列表（禁止客户端通过远程方式看服务端都有啥）hosts allow = 192.168.131.0/24 #允许iphosts deny = 0.0.0.0/32 #禁止ip（32代表谁都不拒绝）auth users = rsync_backup #虚拟用户secrets file = /etc/rsync.password #存放用户和密码的文件 客户端和服务端的连接原理： 客户端要告诉server端两点1,告诉server，我要推送给配置文件的哪个模块。（模块可以有很多个）2,需要验证推送到那个模块下的用户名和密码。 2.创建共享目录及添加rsync程序用户12345[root@zhaoshuo ~]# useradd -s /sbin/nologin -M rsync[root@zhaoshuo ~]# mkdir /backup[root@zhaoshuo ~]# chown -R rsync.rsync /backup #-R：递归修改，将目录和目录下所有文件一起修改[root@zhaoshuo ~]# ll -d /backupdrwxr-xr-x. 2 rsync rsync 4096 Jul 26 22:27 /backup 3.启动服务1234[root@zhaoshuo ~]# rsync --daemon[root@zhaoshuo ~]# netstat -antup | grep rsynctcp 0 0 0.0.0.0:873 0.0.0.0:* LISTEN 1653/rsync tcp 0 0 :::873 :::* LISTEN 1653/rsync --daemon 是守护进程的意思 4.创建rsync虚拟账户名和密码12[root@zhaoshuo ~]# vim /etc/rsync.passwordrsync_backup:123456 5.将账户密码的文件权限改为600（必须，否则失败）123[root@zhaoshuo ~]# chmod 600 /etc/rsync.password [root@zhaoshuo ~]# ll -d /etc/rsync.password -rw-------. 1 root root 20 Jul 26 22:43 /etc/rsync.password 6.加入开机启动1[root@zhaoshuo ~]# echo &quot;rsync --daemon&quot; &gt;&gt; /etc/rc.d/rc.local 小提示:如何重启rsync服务 pkill rsync 关闭rsync服务 rsync –daemon 开启rsync服务 第三步，开始部署rsync服务——&gt;Rsync客户端A-server1.只需要创建密码文件12345[root@zhaoshuo1 ~]# rpm -qa rsyncrsync-3.0.6-9.el6_4.1.x86_64[root@zhaoshuo1 ~]# echo &quot;123456&quot; &gt;&gt; /etc/rsync.password[root@zhaoshuo1 ~]# cat /etc/rsync.password 123456 2.将密码文件的权限设置为600（必须，否则失败）123[root@zhaoshuo1 ~]# chmod 600 /etc/rsync.password [root@zhaoshuo1 ~]# ll -d /etc/rsync.password -rw------- 1 root root 7 Jul 26 22:44 /etc/rsync.password 四，Rsync同步测试推送测试1：将客户端指定目录推送到服务端rsync指定目录下rsync -avz /root/sss rsync_backup@192.168.131.129::backup --password-file=/etc/rsync.password 123456789101112131415161718192021222324252627[root@zhaoshuo1 ~]# rsync -avz /root/sss rsync_backup@192.168.131.129::backup --password-file=/etc/rsync.password #在sss后面加上/就代表推送sss里的文件，不包括目录，不加/就代表推送sss目录sending incremental file listsss/sss/1sss/10sss/11sss/12sss/13sss/14sss/15sss/16sss/17sss/18sss/19sss/2sss/20sss/3sss/4sss/5sss/6sss/7sss/8sss/9sent 906 bytes received 392 bytes 2596.00 bytes/sectotal size is 0 speedup is 0.00 说明-avz：保持文件各项属性不变。-v：显示同步信息&#x2F;root&#x2F;www：要推送的内容所在目录rsync_backup：服务器端rsync服务同步的用户名（虚拟用户的用户名）192.168.131.129：rsync服务端的ip地址backup：rsync服务器配置文件里的模块名–password-file&#x3D;&#x2F;etc&#x2F;rsync.password：免密码的操作，指定密码文件。若不指定，需要手动输入密码 拉取测试1：将rsync服务端指定的目录内容同步到客户端rsync -avz rsync_backup@192.168.131.129::backup /root --password-file=/etc/rsync.password 123456789101112131415161718192021222324252627[root@zhaoshuo1 ~]# rsync -avz rsync_backup@192.168.131.129::backup /root --password-file=/etc/rsync.password receiving incremental file list./sss/sss/1sss/10sss/11sss/12sss/13sss/14sss/15sss/16sss/17sss/18sss/19sss/2sss/20sss/3sss/4sss/5sss/6sss/7sss/8sss/9sent 831 bytes received 1867 bytes 5396.00 bytes/sectotal size is 0 speedup is 0.00 说明：和推送相比，两个目录换了位置 拉取测试2：将rsync服务器端指定目录下的全部内容排除某目录或文件后，同步到客户端12345678910111213141516171819202122232425[root@zhaoshuo1 ~]# rsync -avz --exclude=sss/3 --exclude=4 rsync_backup@192.168.131.129::backup /root --password-file=/etc/rsync.password #拉取时排除了sss目录下3和4文件receiving incremental file list./sss/sss/1sss/10sss/11sss/12sss/13sss/14sss/15sss/16sss/17sss/18sss/19sss/2sss/20sss/5sss/6sss/7sss/8sss/9sent 431 bytes received 919 bytes 2700.00 bytes/sectotal size is 0 speedup is 0.00 排除列表的方法排除 12345678910111213141516171819202122232425262728[root@zhaoshuo1 ~]# vim /etc/exclude.txt13579[root@zhaoshuo1 ~]# rsync -avz --exclude-from=/etc/exclude.txt rsync_backup@192.168.131.129::backup /root --password-file=/etc/rsync.password receiving incremental file list #排除了/etc/exclude.txt里包含的文件名的文件./sss/sss/10sss/11sss/12sss/13sss/14sss/15sss/16sss/17sss/18sss/19sss/2sss/20sss/4sss/6sss/8sent 391 bytes received 790 bytes 787.33 bytes/sectotal size is 0 speedup is 0.00 Rsync同步拉取测试:让rsync客户端指定目录内容始终和rsync服务端共享目录内容保持一致（反之，两路径调换位置）1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@zhaoshuo1 ~]# rsync -avz --delete rsync_backup@192.168.131.129::backup /root --password-file=/etc/rsync.password receiving incremental file listdeleting 9deleting 8deleting 7deleting 6deleting 5deleting 4deleting 3deleting 20deleting 2deleting 19deleting 18deleting 17deleting 16deleting 15deleting 14deleting 13deleting 123deleting 12deleting 11deleting 10deleting 1sss/sss/1sss/10sss/11sss/12sss/13sss/14sss/15sss/16sss/17sss/18sss/19sss/20sss/3sss/5sss/7sss/9sent 372 bytes received 859 bytes 820.67 bytes/sectotal size is 0 speedup is 0.00 排除某文件，再同步 1[root@zhaoshuo1 ~]# rsync -avz --delete --exclude-from=/etc/exclude.txt rsync_backup@192.168.131.129::backup /root --password-file=/etc/rsync.password 特别说明执行–delete参数，从rsync服务端往rsync客户端拉取数据时，一定要小心，最好不用。客户端带–delete参数往服务端推送进删除服务端模块下的数据。反之则有能力删除rsync客户端本地的所有数据，包括根下的所有目录。 五，Rsync优缺点1.5.1Rsync优点 1，增量备份，支持socket（daemon），集中备份（支持推拉，都是以客户端为参照物）。2，远程SHELL通道模式还可以加密（SSH）传输，socket（daemon）需要加密传输，可以利用vpn服务或ipsec服务 1.5.2Rsync缺点 1，大量小文件时候同步的时候，比对时间较长，有的时候，同步过程中，rsync进程可能会停止，僵死了。2，同步大文件，10G这样的大文件有时也会出问题，中断。未完整同步前，是隐藏文件，可以通过续传（–partial）等参数实现传输3，一次性远程拷贝可以用scp，大量小文件要打成一个包再拷贝。 附录：rsyncd.conf配置文件常用参数说明： rsyncd.conf参数 参数说明 uid&#x3D;rsync rsync使用的用户。 gid&#x3D;rsync rsync使用的用户组（用户所在的组） use chroot&#x3D;no 如果为true，daemon会在客户端传输文件前“chroot to the path”。这是一种安全配置，因为我们大多数都在内网，所以不配也没关系 max connections&#x3D;200 设置最大连接数，默认0，意思无限制，负值为关闭这个模块 timeout&#x3D;400 默认为0，表示no timeout，建议300-600（5-10分钟） pid file rsync daemon启动后将其进程pid写入此文件。如果这个文件存在，rsync不会覆盖该文件，而是会终止 lock file 指定lock文件用来支持“max connections”参数，使得总连接数不会超过限制 log file 不设或者设置错误，rsync会使用rsyslog输出相关日志信息 ignore errors 忽略I&#x2F;O错误 read only&#x3D;false 指定客户端是否可以上传文件，默认对所有模块为true list&#x3D;false 是否允许客户端可以查看可用模块列表，默认为可以 hosts allow 指定可以联系的客户端主机名或和ip地址或地址段，默认情况没有此参数，即都可以连接 hosts deny 指定不可以联系的客户端主机名或ip地址或地址段，默认情况没有此参数，即都可以连接 auth users 指定以空格或逗号分隔的用户可以使用哪些模块，用户不需要在本地系统中存在。默认为所有用户无密码访问 secrets file 指定用户名和密码存放的文件，格式；用户名；密码，密码不超过8位 [backup] 这里就是模块名称，需用中括号扩起来，起名称没有特殊要求，但最好是有意义的名称，便于以后维护 path 这个模块中，daemon使用的文件系统或目录，目录的权限要注意和配置文件中的权限一致，否则会遇到读写的问题","categories":[{"name":"服务基础","slug":"服务基础","permalink":"https://kkabuzs.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Rsync","slug":"Rsync","permalink":"https://kkabuzs.github.io/tags/Rsync/"}]}],"categories":[{"name":"架构基础","slug":"架构基础","permalink":"https://kkabuzs.github.io/categories/%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/"},{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/categories/Python/"},{"name":"容器自动化","slug":"容器自动化","permalink":"https://kkabuzs.github.io/categories/%E5%AE%B9%E5%99%A8%E8%87%AA%E5%8A%A8%E5%8C%96/"},{"name":"工作随笔，问题排查","slug":"工作随笔，问题排查","permalink":"https://kkabuzs.github.io/categories/%E5%B7%A5%E4%BD%9C%E9%9A%8F%E7%AC%94%EF%BC%8C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"name":"大数据","slug":"大数据","permalink":"https://kkabuzs.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"服务基础","slug":"服务基础","permalink":"https://kkabuzs.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://kkabuzs.github.io/tags/MongoDB/"},{"name":"Python","slug":"Python","permalink":"https://kkabuzs.github.io/tags/Python/"},{"name":"Kuberbetes","slug":"Kuberbetes","permalink":"https://kkabuzs.github.io/tags/Kuberbetes/"},{"name":"Docker","slug":"Docker","permalink":"https://kkabuzs.github.io/tags/Docker/"},{"name":"Nginx","slug":"Nginx","permalink":"https://kkabuzs.github.io/tags/Nginx/"},{"name":"Linux","slug":"Linux","permalink":"https://kkabuzs.github.io/tags/Linux/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://kkabuzs.github.io/tags/Hadoop/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://kkabuzs.github.io/tags/kubernetes/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://kkabuzs.github.io/tags/Prometheus/"},{"name":"Grafana","slug":"Grafana","permalink":"https://kkabuzs.github.io/tags/Grafana/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kkabuzs.github.io/tags/Kubernetes/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://kkabuzs.github.io/tags/Zookeeper/"},{"name":"Devops","slug":"Devops","permalink":"https://kkabuzs.github.io/tags/Devops/"},{"name":"Ftp","slug":"Ftp","permalink":"https://kkabuzs.github.io/tags/Ftp/"},{"name":"Tomcat","slug":"Tomcat","permalink":"https://kkabuzs.github.io/tags/Tomcat/"},{"name":"Keepalived","slug":"Keepalived","permalink":"https://kkabuzs.github.io/tags/Keepalived/"},{"name":"Ansible","slug":"Ansible","permalink":"https://kkabuzs.github.io/tags/Ansible/"},{"name":"HTTP","slug":"HTTP","permalink":"https://kkabuzs.github.io/tags/HTTP/"},{"name":"Rsync","slug":"Rsync","permalink":"https://kkabuzs.github.io/tags/Rsync/"}]}